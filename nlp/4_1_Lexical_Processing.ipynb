{"cells":[{"cell_type":"markdown","metadata":{"id":"3oTYPg_XcPQf"},"source":["![](images/EscUpmPolit_p.gif \"UPM\")"]},{"cell_type":"markdown","metadata":{"id":"kiV5cfDucPQi"},"source":["# Course Notes for Learning Intelligent Systems"]},{"cell_type":"markdown","metadata":{"id":"cNiX9km1cPQi"},"source":["Department of Telematic Engineering Systems, Universidad Politécnica de Madrid, © Carlos A. Iglesias"]},{"cell_type":"markdown","metadata":{"id":"nHzIWQKVcPQi"},"source":["# Lexical Processing"]},{"cell_type":"markdown","metadata":{"id":"7i9aRfQjcPQj"},"source":["# Table of Contents\n","* [Objectives](#Objectives)\n","* [Tools](#Tools)\n","* [Cleansing](#Cleansing)\n","* [Tokenization](#Tokenization)\n","* [Sentence Splitter](#Sentence-Splitter)\n","* [Word Splitter](#Word-Splitter)\n","* [Stemming and Lemmatization](#Stemming-and-Lemmatization)\n","* [Stop word removal](#Stop-word-removal)\n","* [Punctuation removal](#Punctuation-removal)\n","* [Rare words and spelling](#Rare-words-and-spelling)"]},{"cell_type":"markdown","metadata":{"id":"wGo81ngccPQj"},"source":["# Objectives"]},{"cell_type":"markdown","metadata":{"id":"BmbJU88UcPQk"},"source":["In this session we are going to learn how to preprocess texts, also known as *text wrangling*.  This task involves data munging, text cleansing, specific preprocessing, tokenization, stemming or lemmatization and stop word removal.\n","\n","The main objectives of this session are:\n","* Learn how to preprocess text sources\n","* Learn to use some of the most popular NLP libraries\n","\n","We are going to use as an example part of a computer review included in [Liu's Product Review of IJCA 2015](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets) and a tweet from the [Semeval 2013 Task 2 dataset](https://www.cs.york.ac.uk/semeval-2013/task2/data/uploads/datasets/readme.txt), slightly modified for learning purposes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Xh0LAJzcPQl"},"outputs":[],"source":["review = \"\"\"I purchased this monitor because of budgetary concerns. This item was the most inexpensive 17 inch monitor \n","available to me at the time I made the purchase. My overall experience with this monitor was very poor. When the \n","screen  wasn't contracting or glitching the overall picture quality was poor to fair. I've viewed numerous different \n","monitor models since I 'm a college student and this particular monitor had as poor of picture quality as \n","any I 've seen.\"\"\"\n","\n","tweet = \"\"\"@concert Lady Gaga is actually at the Britney Spears Femme Fatale Concert tonight!!! She still listens to \n","        her music!!!! WOW!!! #ladygaga #britney\"\"\""]},{"cell_type":"markdown","metadata":{"id":"dlwaDzyscPQn"},"source":["# Tools"]},{"cell_type":"markdown","metadata":{"id":"S2T4vF0ucPQo"},"source":["In this session we are going to use several libraries, which provide complementary features:\n","* [NLTK](nltk.org/book_1ed/) - provides functionalities for sentence splitting, tokenization, lemmatization, NER, collocations ... and access to many lexical resources (WordNet, Corpora, ...)\n","* [Gensim](https://radimrehurek.com/gensim/) - provides functionalities for corpora management, LDA and LSI, among others.\n","* [TextBlob](http://textblob.readthedocs.io/) - provides a simple way to access to many of the NLP functions. It is simpler than NLTK and integrates additional functionatities, such as language detection, spelling or even sentiment analysis.\n","* [CLiPS](http://www.clips.ua.ac.be/pages/pattern-en#parser) -- contains a fast part-of-speech tagger for English (identifies nouns, adjectives, verbs, etc. in a sentence), sentiment and mood analysis, tools for English verb conjugation and noun singularization & pluralization, and a WordNet interface. Unfortunately, it does not support Python 3 yet.\n","\n","\n","In order to use nltk, we should download first the lexical resources we are going to use. We can updated them later. For this, you need:\n","* install nltk. Execute 'conda install nltk'\n","* import nltk\n","* Run *nltk.download()* (the first time we use it). A window will appear. You should select just the corpus 'book' and press download.\n","If you inspect the window, you can get an overview of available lexical resources (corpora, lexicons and grammars). For example, you can find some relevant sentiment lexicons in corpora (SentiWordNet, Sentence Polarity Dataset, Vader, Opinion Lexicon or VADER Sentiment Lexicon). Don't forget to close the window once the data has been downloaded."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRyolJcAcPQp","executionInfo":{"status":"ok","timestamp":1652016725918,"user_tz":-120,"elapsed":10010,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"159f6b8f-4ccc-4265-bc1c-3fa6a0cb4118"},"outputs":[{"name":"stdout","output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> d\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> book\n","    Downloading collection 'book'\n","       | \n","       | Downloading package abc to /root/nltk_data...\n","       |   Package abc is already up-to-date!\n","       | Downloading package brown to /root/nltk_data...\n","       |   Package brown is already up-to-date!\n","       | Downloading package chat80 to /root/nltk_data...\n","       |   Package chat80 is already up-to-date!\n","       | Downloading package cmudict to /root/nltk_data...\n","       |   Package cmudict is already up-to-date!\n","       | Downloading package conll2000 to /root/nltk_data...\n","       |   Package conll2000 is already up-to-date!\n","       | Downloading package conll2002 to /root/nltk_data...\n","       |   Package conll2002 is already up-to-date!\n","       | Downloading package dependency_treebank to /root/nltk_data...\n","       |   Package dependency_treebank is already up-to-date!\n","       | Downloading package genesis to /root/nltk_data...\n","       |   Package genesis is already up-to-date!\n","       | Downloading package gutenberg to /root/nltk_data...\n","       |   Package gutenberg is already up-to-date!\n","       | Downloading package ieer to /root/nltk_data...\n","       |   Package ieer is already up-to-date!\n","       | Downloading package inaugural to /root/nltk_data...\n","       |   Package inaugural is already up-to-date!\n","       | Downloading package movie_reviews to /root/nltk_data...\n","       |   Package movie_reviews is already up-to-date!\n","       | Downloading package nps_chat to /root/nltk_data...\n","       |   Package nps_chat is already up-to-date!\n","       | Downloading package names to /root/nltk_data...\n","       |   Package names is already up-to-date!\n","       | Downloading package ppattach to /root/nltk_data...\n","       |   Package ppattach is already up-to-date!\n","       | Downloading package reuters to /root/nltk_data...\n","       |   Package reuters is already up-to-date!\n","       | Downloading package senseval to /root/nltk_data...\n","       |   Package senseval is already up-to-date!\n","       | Downloading package state_union to /root/nltk_data...\n","       |   Package state_union is already up-to-date!\n","       | Downloading package stopwords to /root/nltk_data...\n","       |   Package stopwords is already up-to-date!\n","       | Downloading package swadesh to /root/nltk_data...\n","       |   Package swadesh is already up-to-date!\n","       | Downloading package timit to /root/nltk_data...\n","       |   Package timit is already up-to-date!\n","       | Downloading package treebank to /root/nltk_data...\n","       |   Package treebank is already up-to-date!\n","       | Downloading package toolbox to /root/nltk_data...\n","       |   Package toolbox is already up-to-date!\n","       | Downloading package udhr to /root/nltk_data...\n","       |   Package udhr is already up-to-date!\n","       | Downloading package udhr2 to /root/nltk_data...\n","       |   Package udhr2 is already up-to-date!\n","       | Downloading package unicode_samples to /root/nltk_data...\n","       |   Package unicode_samples is already up-to-date!\n","       | Downloading package webtext to /root/nltk_data...\n","       |   Package webtext is already up-to-date!\n","       | Downloading package wordnet to /root/nltk_data...\n","       |   Package wordnet is already up-to-date!\n","       | Downloading package wordnet_ic to /root/nltk_data...\n","       |   Package wordnet_ic is already up-to-date!\n","       | Downloading package words to /root/nltk_data...\n","       |   Package words is already up-to-date!\n","       | Downloading package maxent_treebank_pos_tagger to\n","       |     /root/nltk_data...\n","       |   Package maxent_treebank_pos_tagger is already up-to-date!\n","       | Downloading package maxent_ne_chunker to /root/nltk_data...\n","       |   Package maxent_ne_chunker is already up-to-date!\n","       | Downloading package universal_tagset to /root/nltk_data...\n","       |   Package universal_tagset is already up-to-date!\n","       | Downloading package punkt to /root/nltk_data...\n","       |   Package punkt is already up-to-date!\n","       | Downloading package book_grammars to /root/nltk_data...\n","       |   Package book_grammars is already up-to-date!\n","       | Downloading package city_database to /root/nltk_data...\n","       |   Package city_database is already up-to-date!\n","       | Downloading package tagsets to /root/nltk_data...\n","       |   Package tagsets is already up-to-date!\n","       | Downloading package panlex_swadesh to /root/nltk_data...\n","       |   Package panlex_swadesh is already up-to-date!\n","       | Downloading package averaged_perceptron_tagger to\n","       |     /root/nltk_data...\n","       |   Package averaged_perceptron_tagger is already up-to-date!\n","       | \n","     Done downloading collection book\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> q\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}],"source":["import nltk\n","nltk.download()"]},{"cell_type":"markdown","metadata":{"id":"FVos_cdocPQp"},"source":["# Cleansing"]},{"cell_type":"markdown","metadata":{"id":"A3r2r62ZcPQp"},"source":["In this case we will use raw text. In case you need to clean the documents (eliminate HTML markup, etc.), you can use libraries such as [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)."]},{"cell_type":"markdown","metadata":{"id":"mWIm6l_McPQq"},"source":["# Tokenization"]},{"cell_type":"markdown","metadata":{"id":"JFPMhaZMcPQq"},"source":["Tokenization is the process of transforming a text into tokens. Depending on the input, we can want to split the text into sentences or words. Moreover, some input such as Twitter can require taking into account processing special tokens, such as hashtags.\n","\n","NLTK provides good support for [tokenization](http://www.nltk.org/api/nltk.tokenize.html).\n","\n","Next we are going to practice several of these features."]},{"cell_type":"markdown","metadata":{"id":"d1oOuYHjcPQq"},"source":["# Sentence Splitter"]},{"cell_type":"markdown","metadata":{"id":"9HhQ-GClcPQr"},"source":["We can use a standard sentence splitter (*sent_tokenize* which uses *PunkTonenizer*), or train a sentence splitter, using the class [*PunktSentenceTokenizer*](http://www.nltk.org/api/nltk.tokenize.html).\n","\n","If the text is multilingual, we can install [*textblob*](http://textblob.readthedocs.io/) or  [*langdetect*](https://pypi.python.org/pypi/langdetect?) to detect the text language and select the most suitable sentence splitter. NLTK comes with 17 trained languages for sentence splitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GmsxY0QcPQr","executionInfo":{"status":"ok","timestamp":1652016417712,"user_tz":-120,"elapsed":326,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ccd8e085-a0d7-434e-b75c-c5408d1e721e"},"outputs":[{"output_type":"stream","name":"stdout","text":["['I purchased this monitor because of budgetary concerns.', 'This item was the most inexpensive 17 inch monitor \\navailable to me at the time I made the purchase.', 'My overall experience with this monitor was very poor.', \"When the \\nscreen  wasn't contracting or glitching the overall picture quality was poor to fair.\", \"I've viewed numerous different \\nmonitor models since I 'm a college student and this particular monitor had as poor of picture quality as \\nany I 've seen.\"]\n"]}],"source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","sentences = sent_tokenize(review, language='english')\n","print(sentences)"]},{"cell_type":"markdown","metadata":{"id":"Mk6X9iPqcPQr"},"source":["# Word Splitter"]},{"cell_type":"markdown","metadata":{"id":"V4MzQXvEcPQs"},"source":["Next stem is dividing every sentence (or the full step) into words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXg8fHVGcPQs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652016424846,"user_tz":-120,"elapsed":316,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"e888efbb-950e-457a-b2b9-249954d8dad3"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['I', 'purchased', 'this', 'monitor', 'because', 'of', 'budgetary', 'concerns', '.'], ['This', 'item', 'was', 'the', 'most', 'inexpensive', '17', 'inch', 'monitor', 'available', 'to', 'me', 'at', 'the', 'time', 'I', 'made', 'the', 'purchase', '.'], ['My', 'overall', 'experience', 'with', 'this', 'monitor', 'was', 'very', 'poor', '.'], ['When', 'the', 'screen', 'was', \"n't\", 'contracting', 'or', 'glitching', 'the', 'overall', 'picture', 'quality', 'was', 'poor', 'to', 'fair', '.'], ['I', \"'ve\", 'viewed', 'numerous', 'different', 'monitor', 'models', 'since', 'I', \"'m\", 'a', 'college', 'student', 'and', 'this', 'particular', 'monitor', 'had', 'as', 'poor', 'of', 'picture', 'quality', 'as', 'any', 'I', \"'ve\", 'seen', '.']]\n"]}],"source":["words = [word_tokenize(t) for t in sent_tokenize(review)]\n","print(words)"]},{"cell_type":"markdown","metadata":{"id":"UUzMPTbPcPQs"},"source":["In our case, we are not interested in processing every sentence, we have split into sentence just for learning purposes. So, we are going to get the word tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGehOGU9cPQs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652016427506,"user_tz":-120,"elapsed":325,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"f3441b70-3214-42c1-d053-e0978ebfea83"},"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'purchased', 'this', 'monitor', 'because', 'of', 'budgetary', 'concerns', '.', 'This', 'item', 'was', 'the', 'most', 'inexpensive', '17', 'inch', 'monitor', 'available', 'to', 'me', 'at', 'the', 'time', 'I', 'made', 'the', 'purchase', '.', 'My', 'overall', 'experience', 'with', 'this', 'monitor', 'was', 'very', 'poor', '.', 'When', 'the', 'screen', 'was', \"n't\", 'contracting', 'or', 'glitching', 'the', 'overall', 'picture', 'quality', 'was', 'poor', 'to', 'fair', '.', 'I', \"'ve\", 'viewed', 'numerous', 'different', 'monitor', 'models', 'since', 'I', \"'m\", 'a', 'college', 'student', 'and', 'this', 'particular', 'monitor', 'had', 'as', 'poor', 'of', 'picture', 'quality', 'as', 'any', 'I', \"'ve\", 'seen', '.']\n"]}],"source":["words = word_tokenize(review)\n","print(words)"]},{"cell_type":"markdown","metadata":{"id":"qLFh-XSCcPQt"},"source":["We can define our own word tokenizer using regular expressions (and using the class [*RegexpTokenizer*](http://www.nltk.org/api/nltk.tokenize.html).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DE5MjLXJcPQt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652016430095,"user_tz":-120,"elapsed":299,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"63a28dc3-87a0-4036-a55a-18a4a8add971"},"outputs":[{"output_type":"stream","name":"stdout","text":["With TweetTokenizer Lady Gaga is actually at the Britney Spears Femme Fatale Concert tonight ! ! ! She still listens to her music ! ! ! WOW ! ! ! #ladygaga #britney\n","With word_tokenizer @ concert Lady Gaga is actually at the Britney Spears Femme Fatale Concert tonight ! ! ! She still listens to her music ! ! ! ! WOW ! ! ! # ladygaga # britney\n"]}],"source":["from nltk.tokenize import TweetTokenizer\n","tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n","tweet_tokens = tknzr.tokenize(tweet)\n","print (\"With TweetTokenizer \" + \" \".join(tweet_tokens))\n","print(\"With word_tokenizer \" + \" \".join(word_tokenize(tweet)))"]},{"cell_type":"markdown","metadata":{"id":"p0e_MW_VcPQt"},"source":["# Stemming and Lemmatization"]},{"cell_type":"markdown","metadata":{"id":"be-StJancPQt"},"source":["NLTK provides support for stemming in the package [*stem*](http://www.nltk.org/api/nltk.stem.html). There are several available stemmers:PorterStemmer, lancaster or WordNetLemmatizer. Check the API for more details. Here we are going the output of different stemmers (and the execution time). You can observe that some of them use rules and do not behave properly always (e.g. Porter algorithm)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1oHtPrycPQu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652016437342,"user_tz":-120,"elapsed":2017,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"df874e50-1f8f-44df-c349-b3720bd39a36"},"outputs":[{"output_type":"stream","name":"stdout","text":["Porter: boy children are have is ha madrid\n","Execution time: 0.0022547245025634766\n","Lancaster: boy childr ar hav is has madrid\n","Execution time: 0.0020613670349121094\n","WordNet: boy child are have is ha Madrid\n","Execution time: 1.6970922946929932\n","SnowBall: boy children are have is has madrid\n","Execution time: 0.0007143020629882812\n"]}],"source":["from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n","from nltk.stem.snowball import EnglishStemmer\n","import time\n","\n","porter = PorterStemmer()\n","lancaster = LancasterStemmer()\n","wordnet = WordNetLemmatizer()\n","snowball = EnglishStemmer()\n","\n","words = \"boys children are have is has Madrid\"\n","\n","start = time.time()\n","print(\"Porter: \" + \" \".join([porter.stem(w) for w in word_tokenize(words)]))\n","end = time.time()\n","print(\"Execution time: \"  + str(end - start))\n","start = time.time()\n","print(\"Lancaster: \" + \" \".join([lancaster.stem(w) for w in word_tokenize(words)]))\n","end = time.time()\n","print(\"Execution time: \"  + str(end - start))\n","start = time.time()\n","print(\"WordNet: \" + \" \".join([wordnet.lemmatize(w) for w in word_tokenize(words)]))\n","end = time.time()\n","print(\"Execution time: \"  + str(end - start))\n","start = time.time()\n","print(\"SnowBall: \" + \" \".join([snowball.stem(w) for w in word_tokenize(words)]))\n","end = time.time()\n","print(\"Execution time: \"  + str(end - start))"]},{"cell_type":"markdown","metadata":{"id":"hL_S5LvzcPQu"},"source":["As we can see, we get the forms *are* and *is* instead of *be*. This is because we have not introduce the Part-Of-Speech (POS), and the default POS is 'n' (name).\n","\n","The main difference between stemmers and lemmatizers is that stemmers operate in isolated words, while lemmatizers take into account the context (e.g. POS). However, stemmers are quicker and require fewer resources.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjMYFOTWcPQu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652016440316,"user_tz":-120,"elapsed":326,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"ce0515a8-cd10-4a53-c5ef-6f68bf39531d"},"outputs":[{"output_type":"stream","name":"stdout","text":["WordNet: be cry be have have\n"]}],"source":["verbs = \"are crying is have has\"\n","print(\"WordNet: \" + \" \".join([wordnet.lemmatize(w, pos='v') for w in word_tokenize(verbs)]))\n"]},{"cell_type":"markdown","metadata":{"id":"C64C1OY-cPQu"},"source":["Depending of the application, we can select stemmers or lemmatizers. \n","\n","Regarding Twitter, we could use specialised software for managing tweets, such as [*TweetNLP*](http://www.cs.cmu.edu/~ark/TweetNLP/).\n","\n","Now we go back to our example and we apply stemming."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5T_eNjfocPQv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652016445144,"user_tz":-120,"elapsed":306,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"9c766e75-c0b5-4e9c-c94f-ba52f344dadd"},"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'purchas', 'thi', 'monitor', 'becaus', 'of', 'budgetari', 'concern', '.', 'thi', 'item', 'wa', 'the', 'most', 'inexpens', '17', 'inch', 'monitor', 'avail', 'to', 'me', 'at', 'the', 'time', 'i', 'made', 'the', 'purchas', '.', 'my', 'overal', 'experi', 'with', 'thi', 'monitor', 'wa', 'veri', 'poor', '.', 'when', 'the', 'screen', 'wa', \"n't\", 'contract', 'or', 'glitch', 'the', 'overal', 'pictur', 'qualiti', 'wa', 'poor', 'to', 'fair', '.', 'i', \"'ve\", 'view', 'numer', 'differ', 'monitor', 'model', 'sinc', 'i', \"'m\", 'a', 'colleg', 'student', 'and', 'thi', 'particular', 'monitor', 'had', 'as', 'poor', 'of', 'pictur', 'qualiti', 'as', 'ani', 'i', \"'ve\", 'seen', '.']\n","['ladi', 'gaga', 'is', 'actual', 'at', 'the', 'britney', 'spear', 'femm', 'fatal', 'concert', 'tonight', '!', '!', '!', 'she', 'still', 'listen', 'to', 'her', 'music', '!', '!', '!', 'wow', '!', '!', '!', '#ladygaga', '#britney']\n"]}],"source":["def preprocess(words, type='doc'):\n","    if (type == 'tweet'):\n","        tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n","        tokens = tknzr.tokenize(words)\n","    else:\n","        tokens = nltk.word_tokenize(words.lower())\n","    porter = nltk.PorterStemmer()\n","    lemmas = [porter.stem(t) for t in tokens]\n","    return lemmas\n","print(preprocess(review))\n","print(preprocess(tweet, type='tweet'))"]},{"cell_type":"markdown","metadata":{"id":"KB4m5NiEcPQv"},"source":["# Stop word removal"]},{"cell_type":"markdown","metadata":{"id":"y3P7dT7ScPQv"},"source":["Next step is removing stop words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbED7itzcPQv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652016454828,"user_tz":-120,"elapsed":315,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"d8e89cb4-a50d-476b-81e6-f07ea9cc3432"},"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}],"source":["from nltk.corpus import stopwords\n","\n","stoplist = stopwords.words('english')\n","print(stoplist)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukv_GwgjcPQw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652016467124,"user_tz":-120,"elapsed":306,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"61befb27-964f-4747-e0ef-4d3b684908fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["['purchas', 'thi', 'monitor', 'becaus', 'budgetari', 'concern', '.', 'thi', 'item', 'wa', 'inexpens', '17', 'inch', 'monitor', 'avail', 'time', 'made', 'purchas', '.', 'overal', 'experi', 'thi', 'monitor', 'wa', 'veri', 'poor', '.', 'screen', 'wa', \"n't\", 'contract', 'glitch', 'overal', 'pictur', 'qualiti', 'wa', 'poor', 'fair', '.', \"'ve\", 'view', 'numer', 'differ', 'monitor', 'model', 'sinc', \"'m\", 'colleg', 'student', 'thi', 'particular', 'monitor', 'poor', 'pictur', 'qualiti', 'ani', \"'ve\", 'seen', '.']\n","['ladi', 'gaga', 'actual', 'britney', 'spear', 'femm', 'fatal', 'concert', 'tonight', '!', '!', '!', 'still', 'listen', 'music', '!', '!', '!', 'wow', '!', '!', '!', '#ladygaga', '#britney']\n"]}],"source":["def preprocess(words, type='doc'):\n","    if (type == 'tweet'):\n","        tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n","        tokens = tknzr.tokenize(tweet)\n","    else:\n","        tokens = nltk.word_tokenize(words.lower())\n","    porter = nltk.PorterStemmer()\n","    lemmas = [porter.stem(t) for t in tokens]\n","    stoplist = stopwords.words('english')\n","    lemmas_clean = [w for w in lemmas if w not in stoplist]\n","    return lemmas_clean\n","\n","print(preprocess(review))\n","print(preprocess(tweet, type='tweet'))"]},{"cell_type":"markdown","metadata":{"id":"4_XKt1focPQw"},"source":["# Punctuation removal"]},{"cell_type":"markdown","metadata":{"id":"os38dAm9cPQw"},"source":["Punctuation is useful for sentence splitting and POS tagging. Once we have used it, we can remove it easily."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSQCG5AdcPQw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652016469141,"user_tz":-120,"elapsed":315,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"4bc38e64-b22c-4e59-99f5-f4f53bda9ad1"},"outputs":[{"output_type":"stream","name":"stdout","text":["['purchas', 'thi', 'monitor', 'becaus', 'budgetari', 'concern', 'thi', 'item', 'wa', 'inexpens', '17', 'inch', 'monitor', 'avail', 'time', 'made', 'purchas', 'overal', 'experi', 'thi', 'monitor', 'wa', 'veri', 'poor', 'screen', 'wa', \"n't\", 'contract', 'glitch', 'overal', 'pictur', 'qualiti', 'wa', 'poor', 'fair', \"'ve\", 'view', 'numer', 'differ', 'monitor', 'model', 'sinc', \"'m\", 'colleg', 'student', 'thi', 'particular', 'monitor', 'poor', 'pictur', 'qualiti', 'ani', \"'ve\", 'seen']\n","['ladi', 'gaga', 'actual', 'britney', 'spear', 'femm', 'fatal', 'concert', 'tonight', 'still', 'listen', 'music', 'wow', '#ladygaga', '#britney']\n"]}],"source":["import string\n","\n","def preprocess(words, type='doc'):\n","    if (type == 'tweet'):\n","        tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n","        tokens = tknzr.tokenize(tweet)\n","    else:\n","        tokens = nltk.word_tokenize(words.lower())\n","    porter = nltk.PorterStemmer()\n","    lemmas = [porter.stem(t) for t in tokens]\n","    stoplist = stopwords.words('english')\n","    lemmas_clean = [w for w in lemmas if w not in stoplist]\n","    punctuation = set(string.punctuation)\n","    words = [w for w in lemmas_clean if  w not in punctuation]\n","    return words\n","\n","print(preprocess(review))\n","print(preprocess(tweet, type='tweet'))"]},{"cell_type":"markdown","metadata":{"id":"qo9yJXtxcPQx"},"source":["# Rare words and spelling"]},{"cell_type":"markdown","metadata":{"id":"-mu9s54RcPQx"},"source":["In large corpus, we may want to clean rare words (probably they are typos) and correct spelling. \n","\n","For the first task, you can exclude the least frequent words (or compare with their frequency in a corpus). NLTK provides facilities for calculating frequencies.\n","\n","For the second task, you can use spell-checker packages such as [*textblob*](http://textblob.readthedocs.io/) or [*autocorrect*](https://pypi.python.org/pypi/autocorrect/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LO-KzPDcPQy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652016474723,"user_tz":-120,"elapsed":337,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"0a7b4f01-7df3-4c74-9f65-8686e10b8103"},"outputs":[{"output_type":"stream","name":"stdout","text":["Most frequent\n","[('I', 5), ('monitor', 5), ('.', 5), ('the', 5), ('was', 4), ('this', 3), ('poor', 3), ('of', 2), ('to', 2), ('overall', 2)]\n","Least frequent\n","[\"'m\", 'a', 'college', 'student', 'and', 'particular', 'had', 'as', 'any', 'seen']\n"]}],"source":["frec = nltk.FreqDist(nltk.word_tokenize(review))\n","print(\"Most frequent\")\n","print(frec.most_common(10))\n","print(\"Least frequent\")\n","print(list(frec.keys())[-10:])"]},{"cell_type":"markdown","metadata":{"id":"UjaZiQYFcPQy"},"source":["## References\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3v1qJ7cNcPQz"},"source":["* [NLTK Book. Natural Language Processing with Python. Steven Bird, Ewan Klein, and Edward Loper. O'Reilly Media, 2009 ](http://www.nltk.org/book_1ed/)\n","* [NLTK Essentials, Nitin Hardeniya, Packt Publishing, 2015](http://proquest.safaribooksonline.com/search?q=NLTK%20Essentials)"]},{"cell_type":"markdown","metadata":{"id":"0VLxQIhzcPQz"},"source":["## Licence"]},{"cell_type":"markdown","metadata":{"id":"thYxju1xcPQz"},"source":["The notebook is freely licensed under under the [Creative Commons Attribution Share-Alike license](https://creativecommons.org/licenses/by/2.0/).  \n","\n","© Carlos A. Iglesias, Universidad Politécnica de Madrid."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"latex_envs":{"LaTeX_envs_menu_present":true,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"colab":{"name":"4_1_Lexical_Processing.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}