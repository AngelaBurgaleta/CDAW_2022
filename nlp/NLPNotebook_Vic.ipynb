{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLPNotebook_Vic.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x6wDo2Sp7J_h"},"source":["**Víctor Álvarez Provencio**\n","Notebook ABID NLP"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUsuduV_7Tmv","outputId":"97ee27d6-a391-402c-d3e9-606faaf2246d"},"source":["# Librerias\n","import nltk\n","from bs4 import BeautifulSoup\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.tokenize import TweetTokenizer\n","from nltk.corpus import stopwords\n","from nltk import pos_tag, word_tokenize\n","from nltk import ne_chunk, pos_tag, word_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","from scipy.spatial.distance import cosine\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.utils.extmath import density\n","import numpy as np\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn import metrics\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.datasets import fetch_20newsgroups\n","from gensim.models.ldamodel import LdaModel\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import cross_val_score, KFold\n","from sklearn.metrics import classification_report\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.decomposition import NMF, LatentDirichletAllocation\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from nltk import pos_tag\n","from collections import Counter \n","\n","\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from nltk.stem import PorterStemmer\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","import string\n","\n","\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('universal_tagset')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('tagsets')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package tagsets to /root/nltk_data...\n","[nltk_data]   Package tagsets is already up-to-date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"OFso1DOu7TvI"},"source":["# Variables con diferentes textos y formatos\n","review = \"\"\"I purchased this monitor because of budgetary concerns. This item was the most inexpensive 17 inch monitor \n","available to me at the time I made the purchase. My overall experience with this monitor was very poor. When the \n","screen  wasn't contracting or glitching the overall picture quality was poor to fair. I've viewed numerous different \n","monitor models since I 'm a college student and this particular monitor had as poor of picture quality as \n","any I 've seen.\"\"\"\n","\n","tweet = \"\"\"@concert Lady Gaga is actually at the Britney Spears Femme Fatale Concert tonight!!! She still listens to \n","        her music!!!! WOW!!! #ladygaga #britney\"\"\"\n","\n","#HTML\n","html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n","<body>\n","<p class=\"title\"><b>The Dormouse's story</b></p>\n","\n","<p class=\"story\">Once upon a time there were three little sisters; and their names were\n","<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n","<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n","<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n","and they lived at the bottom of a well.</p>\n","\n","<p class=\"story\">...</p>\n","\"\"\"\n","\n","# Para la parte de vector representation\n","doc1 = 'Summer is coming but Summer is short'\n","doc2 = 'I like the Summer and I like the Winter'\n","doc3 = 'I like sandwiches and I like the Winter'\n","documents = [doc1, doc2, doc3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pEu0p4Po7T0b","outputId":"01e4e1f4-e37a-4c0f-b634-f7aa781a1d2e"},"source":["# Para limpiar los textos utilizando BeautifulSoup\n","soup = BeautifulSoup(html_doc, 'html.parser')\n","# Y puede dividir el texto en funcion de las etiquetas HTML\n","soup.title\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<title>The Dormouse's story</title>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"x25JCxip7T27","outputId":"4734b3b0-20a2-4f24-89d4-44d1923f9251"},"source":["soup.title.string"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"The Dormouse's story\""]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"kri30fZ08v8I"},"source":["**Tokenización**\n"]},{"cell_type":"markdown","metadata":{"id":"tw_mAqyl85kE"},"source":["Se va a tokenizar el texto utilizando solamente uno de los ejemplos que aparece en el notebook. Se va a utilizar Word Splitter"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3lf3nz-C7T45","outputId":"dd73b49a-e1e7-4b0d-b802-6347b3c88657"},"source":["\n","sentences = sent_tokenize(review, language='english')\n","print(sentences)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","['I purchased this monitor because of budgetary concerns.', 'This item was the most inexpensive 17 inch monitor \\navailable to me at the time I made the purchase.', 'My overall experience with this monitor was very poor.', \"When the \\nscreen  wasn't contracting or glitching the overall picture quality was poor to fair.\", \"I've viewed numerous different \\nmonitor models since I 'm a college student and this particular monitor had as poor of picture quality as \\nany I 've seen.\"]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w98juRgF7T6_","outputId":"b5c655c3-553e-4281-baa0-f23cb8533ea8"},"source":["words = [word_tokenize(t) for t in sent_tokenize(review)]\n","print('separado por palabras es:')\n","print(words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["separado por palabras es:\n","[['I', 'purchased', 'this', 'monitor', 'because', 'of', 'budgetary', 'concerns', '.'], ['This', 'item', 'was', 'the', 'most', 'inexpensive', '17', 'inch', 'monitor', 'available', 'to', 'me', 'at', 'the', 'time', 'I', 'made', 'the', 'purchase', '.'], ['My', 'overall', 'experience', 'with', 'this', 'monitor', 'was', 'very', 'poor', '.'], ['When', 'the', 'screen', 'was', \"n't\", 'contracting', 'or', 'glitching', 'the', 'overall', 'picture', 'quality', 'was', 'poor', 'to', 'fair', '.'], ['I', \"'ve\", 'viewed', 'numerous', 'different', 'monitor', 'models', 'since', 'I', \"'m\", 'a', 'college', 'student', 'and', 'this', 'particular', 'monitor', 'had', 'as', 'poor', 'of', 'picture', 'quality', 'as', 'any', 'I', \"'ve\", 'seen', '.']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oOQB-6Nm-D5M"},"source":["**Stopwords**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KyLC1urt7T9S","outputId":"b99ec06e-d9e7-44d5-bcf8-25dd33077d73"},"source":["\n","stoplist = stopwords.words('english')\n","print(stoplist)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oU0hs1Of7T_o","outputId":"ca8c6642-ca52-415d-ddbb-cc6e67a27332"},"source":["# Se trata de eliminar las stopwords\n","def preprocess(words, type='doc'):\n","    if (type == 'tweet'):\n","        tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n","        tokens = tknzr.tokenize(tweet)\n","    else:\n","        tokens = nltk.word_tokenize(words.lower())\n","    porter = nltk.PorterStemmer()\n","    lemmas = [porter.stem(t) for t in tokens]\n","    stoplist = stopwords.words('english')\n","    lemmas_clean = [w for w in lemmas if w not in stoplist]\n","    return lemmas_clean\n","\n","print(preprocess(review))\n","print(preprocess(tweet, type='tweet'))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['purchas', 'thi', 'monitor', 'becaus', 'budgetari', 'concern', '.', 'thi', 'item', 'wa', 'inexpens', '17', 'inch', 'monitor', 'avail', 'time', 'made', 'purchas', '.', 'overal', 'experi', 'thi', 'monitor', 'wa', 'veri', 'poor', '.', 'screen', 'wa', \"n't\", 'contract', 'glitch', 'overal', 'pictur', 'qualiti', 'wa', 'poor', 'fair', '.', \"'ve\", 'view', 'numer', 'differ', 'monitor', 'model', 'sinc', \"'m\", 'colleg', 'student', 'thi', 'particular', 'monitor', 'poor', 'pictur', 'qualiti', 'ani', \"'ve\", 'seen', '.']\n","['ladi', 'gaga', 'actual', 'britney', 'spear', 'femm', 'fatal', 'concert', 'tonight', '!', '!', '!', 'still', 'listen', 'music', '!', '!', '!', 'wow', '!', '!', '!', '#ladygaga', '#britney']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HdrlX7dS-cmr"},"source":["**Rare words and spelling**\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJ1xalIc7UCA","outputId":"fe53ae18-39fa-4e9e-ae47-1b4d14cc27bf"},"source":["# Para analisis de textos, es muy importante calcular la frecuencia. Datos como las palabras que mas aparecen (y menos)\n","# y cuantas veces aparecen.\n","frec = nltk.FreqDist(nltk.word_tokenize(review))\n","print(\"Most frequent\")\n","print(frec.most_common(10))\n","print(\"Least frequent\")\n","print(list(frec.keys())[-10:])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Most frequent\n","[('I', 5), ('monitor', 5), ('.', 5), ('the', 5), ('was', 4), ('this', 3), ('poor', 3), ('of', 2), ('to', 2), ('overall', 2)]\n","Least frequent\n","[\"'m\", 'a', 'college', 'student', 'and', 'particular', 'had', 'as', 'any', 'seen']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YtOLlHxT_ING"},"source":["**Syntactic Processing**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w5cTjBHF_KYx","outputId":"2dc71961-ee47-444a-c9f4-d85825489f58"},"source":["# Parte para entender POS (Part of Speech) y NER (Named Entity Recognition)\n","\n","\n","# POS\n","\n","print (pos_tag(word_tokenize(review), tagset='universal'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","[('I', 'PRON'), ('purchased', 'VERB'), ('this', 'DET'), ('monitor', 'NOUN'), ('because', 'ADP'), ('of', 'ADP'), ('budgetary', 'ADJ'), ('concerns', 'NOUN'), ('.', '.'), ('This', 'DET'), ('item', 'NOUN'), ('was', 'VERB'), ('the', 'DET'), ('most', 'ADV'), ('inexpensive', 'ADJ'), ('17', 'NUM'), ('inch', 'NOUN'), ('monitor', 'NOUN'), ('available', 'ADJ'), ('to', 'PRT'), ('me', 'PRON'), ('at', 'ADP'), ('the', 'DET'), ('time', 'NOUN'), ('I', 'PRON'), ('made', 'VERB'), ('the', 'DET'), ('purchase', 'NOUN'), ('.', '.'), ('My', 'PRON'), ('overall', 'ADJ'), ('experience', 'NOUN'), ('with', 'ADP'), ('this', 'DET'), ('monitor', 'NOUN'), ('was', 'VERB'), ('very', 'ADV'), ('poor', 'ADJ'), ('.', '.'), ('When', 'ADV'), ('the', 'DET'), ('screen', 'NOUN'), ('was', 'VERB'), (\"n't\", 'ADV'), ('contracting', 'VERB'), ('or', 'CONJ'), ('glitching', 'VERB'), ('the', 'DET'), ('overall', 'ADJ'), ('picture', 'NOUN'), ('quality', 'NOUN'), ('was', 'VERB'), ('poor', 'ADJ'), ('to', 'PRT'), ('fair', 'VERB'), ('.', '.'), ('I', 'PRON'), (\"'ve\", 'VERB'), ('viewed', 'VERB'), ('numerous', 'ADJ'), ('different', 'ADJ'), ('monitor', 'NOUN'), ('models', 'NOUN'), ('since', 'ADP'), ('I', 'PRON'), (\"'m\", 'VERB'), ('a', 'DET'), ('college', 'NOUN'), ('student', 'NOUN'), ('and', 'CONJ'), ('this', 'DET'), ('particular', 'ADJ'), ('monitor', 'NOUN'), ('had', 'VERB'), ('as', 'ADP'), ('poor', 'ADJ'), ('of', 'ADP'), ('picture', 'NOUN'), ('quality', 'NOUN'), ('as', 'ADP'), ('any', 'DET'), ('I', 'PRON'), (\"'ve\", 'VERB'), ('seen', 'VERB'), ('.', '.')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1pFRXJX_Kam","outputId":"5ef8ea88-08a8-4a4d-be14-c6a3cb7e3759"},"source":["print (pos_tag(word_tokenize(review)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('I', 'PRP'), ('purchased', 'VBD'), ('this', 'DT'), ('monitor', 'NN'), ('because', 'IN'), ('of', 'IN'), ('budgetary', 'JJ'), ('concerns', 'NNS'), ('.', '.'), ('This', 'DT'), ('item', 'NN'), ('was', 'VBD'), ('the', 'DT'), ('most', 'RBS'), ('inexpensive', 'JJ'), ('17', 'CD'), ('inch', 'NN'), ('monitor', 'NN'), ('available', 'JJ'), ('to', 'TO'), ('me', 'PRP'), ('at', 'IN'), ('the', 'DT'), ('time', 'NN'), ('I', 'PRP'), ('made', 'VBD'), ('the', 'DT'), ('purchase', 'NN'), ('.', '.'), ('My', 'PRP$'), ('overall', 'JJ'), ('experience', 'NN'), ('with', 'IN'), ('this', 'DT'), ('monitor', 'NN'), ('was', 'VBD'), ('very', 'RB'), ('poor', 'JJ'), ('.', '.'), ('When', 'WRB'), ('the', 'DT'), ('screen', 'NN'), ('was', 'VBD'), (\"n't\", 'RB'), ('contracting', 'VBG'), ('or', 'CC'), ('glitching', 'VBG'), ('the', 'DT'), ('overall', 'JJ'), ('picture', 'NN'), ('quality', 'NN'), ('was', 'VBD'), ('poor', 'JJ'), ('to', 'TO'), ('fair', 'VB'), ('.', '.'), ('I', 'PRP'), (\"'ve\", 'VBP'), ('viewed', 'VBN'), ('numerous', 'JJ'), ('different', 'JJ'), ('monitor', 'NN'), ('models', 'NNS'), ('since', 'IN'), ('I', 'PRP'), (\"'m\", 'VBP'), ('a', 'DT'), ('college', 'NN'), ('student', 'NN'), ('and', 'CC'), ('this', 'DT'), ('particular', 'JJ'), ('monitor', 'NN'), ('had', 'VBD'), ('as', 'IN'), ('poor', 'JJ'), ('of', 'IN'), ('picture', 'NN'), ('quality', 'NN'), ('as', 'IN'), ('any', 'DT'), ('I', 'PRP'), (\"'ve\", 'VBP'), ('seen', 'VBN'), ('.', '.')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H480EDvT_Kde","outputId":"578f18ea-41b0-4007-a72e-aa98db3b19f3"},"source":["# NER\n","\n","ne_tagged = ne_chunk(pos_tag(word_tokenize(review)), binary=False)\n","print(ne_tagged)\n","\n","# Donde te asocia cada una de las palabras, con el tipo que son"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","(S\n","  I/PRP\n","  purchased/VBD\n","  this/DT\n","  monitor/NN\n","  because/IN\n","  of/IN\n","  budgetary/JJ\n","  concerns/NNS\n","  ./.\n","  This/DT\n","  item/NN\n","  was/VBD\n","  the/DT\n","  most/RBS\n","  inexpensive/JJ\n","  17/CD\n","  inch/NN\n","  monitor/NN\n","  available/JJ\n","  to/TO\n","  me/PRP\n","  at/IN\n","  the/DT\n","  time/NN\n","  I/PRP\n","  made/VBD\n","  the/DT\n","  purchase/NN\n","  ./.\n","  My/PRP$\n","  overall/JJ\n","  experience/NN\n","  with/IN\n","  this/DT\n","  monitor/NN\n","  was/VBD\n","  very/RB\n","  poor/JJ\n","  ./.\n","  When/WRB\n","  the/DT\n","  screen/NN\n","  was/VBD\n","  n't/RB\n","  contracting/VBG\n","  or/CC\n","  glitching/VBG\n","  the/DT\n","  overall/JJ\n","  picture/NN\n","  quality/NN\n","  was/VBD\n","  poor/JJ\n","  to/TO\n","  fair/VB\n","  ./.\n","  I/PRP\n","  've/VBP\n","  viewed/VBN\n","  numerous/JJ\n","  different/JJ\n","  monitor/NN\n","  models/NNS\n","  since/IN\n","  I/PRP\n","  'm/VBP\n","  a/DT\n","  college/NN\n","  student/NN\n","  and/CC\n","  this/DT\n","  particular/JJ\n","  monitor/NN\n","  had/VBD\n","  as/IN\n","  poor/JJ\n","  of/IN\n","  picture/NN\n","  quality/NN\n","  as/IN\n","  any/DT\n","  I/PRP\n","  've/VBP\n","  seen/VBN\n","  ./.)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u5TptCDKBAoa"},"source":["**Vector Representation**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acvU954u_Kh2","outputId":"1f6e9a53-59da-47f1-eeb3-67c32eddf4c6"},"source":["vectorizer = CountVectorizer(analyzer = \"word\", max_features = 5000) \n","vectorizer"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n","                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n","                lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n","                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n","                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                tokenizer=None, vocabulary=None)"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezh_f06T_KkO","outputId":"cd3e938f-a47f-474d-b5b4-426194f1319c"},"source":["vectors = vectorizer.fit_transform(documents)\n","vectors"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<3x10 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 15 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4KRPDZNi_Kmn","outputId":"17c36048-80a4-45ea-b44b-7d1ae8c6b56c"},"source":["print(vectors.toarray())\n","print(vectorizer.get_feature_names())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0 1 1 2 0 0 1 2 0 0]\n"," [1 0 0 0 2 0 0 1 2 1]\n"," [1 0 0 0 2 1 0 0 1 1]]\n","['and', 'but', 'coming', 'is', 'like', 'sandwiches', 'short', 'summer', 'the', 'winter']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMGwi4hg_Ko5","outputId":"df12d74f-7119-420e-9d42-660f51fb2c80"},"source":["# Y tambien calcular las distancias entre los vectores\n","\n","f_array = vectors.toarray()\n","d12 = cosine(f_array[0], f_array[1])\n","d13 = cosine(f_array[0], f_array[2])\n","d23 = cosine(f_array[1], f_array[2])\n","print(d12, d13, d23)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8181818181818181 1.0 0.14719713457755823\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hbrS48cyB78C"},"source":["\n","**Tf-idf vector representation**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dc2bQdyb_KtR","outputId":"157440ff-6772-453f-e172-807a8c52b8ed"},"source":["vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words='english')\n","vectors = vectorizer.fit_transform(documents)\n","vectorizer.get_feature_names()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['coming', 'like', 'sandwiches', 'short', 'summer', 'winter']"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J8o0sEeA_KvX","outputId":"5cfa1a61-5773-4878-d221-3a9b9e77dffd"},"source":["vectors.toarray()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.48148213, 0.        , 0.        , 0.48148213, 0.73235914,\n","        0.        ],\n","       [0.        , 0.81649658, 0.        , 0.        , 0.40824829,\n","        0.40824829],\n","       [0.        , 0.77100584, 0.50689001, 0.        , 0.        ,\n","        0.38550292]])"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nQYsS1sCFZE","outputId":"a11cc120-a951-44b1-f722-dc82268be3bc"},"source":["train = [doc1, doc2, doc3]\n","vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words='english')\n","\n","# We learn the vocabulary (fit) and tranform the docs into vectors\n","vectors = vectorizer.fit_transform(train)\n","vectorizer.get_feature_names()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['coming', 'like', 'sandwiches', 'short', 'summer', 'winter']"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_91R77UCFfx","outputId":"660784b4-9e9e-4c00-c2e3-2b7e2fe69a0c"},"source":["query = ['winter short']\n","\n","# We transform the query into a vector of the learnt vocabulary\n","vector_query = vectorizer.transform(query)\n","\n","# Here we calculate the distance of the query to the docs\n","cosine_similarity(vector_query, vectors)\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.38324078, 0.24713249, 0.23336362]])"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"x_Csu1luCa6h"},"source":["**Clasificación**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fm_LOEz9CFiB","outputId":"c134bdbc-a266-4c48-9e03-6fb6d574a009"},"source":["# We remove metadata to avoid bias in the classification\n","newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n","newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n","\n","# print categories\n","print(list(newsgroups_train.target_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading 20news dataset. This may take a few minutes.\n","Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"],"name":"stderr"},{"output_type":"stream","text":["['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EXPrqolCFkZ","outputId":"52bc1df4-82f9-4e90-8544-f0e970663c4c"},"source":["# Show a document\n","docid = 1\n","doc = newsgroups_train.data[docid]\n","cat = newsgroups_train.target[docid]\n","\n","print(\"Category id \" +  str(cat) + \" \" + newsgroups_train.target_names[cat])\n","print(\"Doc \" + doc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Category id 4 comp.sys.mac.hardware\n","Doc A fair number of brave souls who upgraded their SI clock oscillator have\n","shared their experiences for this poll. Please send a brief message detailing\n","your experiences with the procedure. Top speed attained, CPU rated speed,\n","add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n","functionality with 800 and 1.4 m floppies are especially requested.\n","\n","I will be summarizing in the next two days, so please add to the network\n","knowledge base if you have done the clock upgrade and haven't answered this\n","poll. Thanks.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZnUy9gwICFmh","outputId":"1633fc11-b414-4856-8f19-a9d7f5fdd234"},"source":["newsgroups_train.filenames.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11314,)"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkOhoBjtDBjL","outputId":"a096c25f-8da8-4b51-ab00-1d612eb6c5f5"},"source":["# Obtain a vector\n","\n","vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n","\n","vectors_train = vectorizer.fit_transform(newsgroups_train.data)\n","vectors_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11314, 101322)"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAZ8n0BCDBld","outputId":"7a001b81-16f4-4108-9c81-2ca82df9063f"},"source":["\n","# We learn the vocabulary (fit) with the train dataset and transform into vectors (fit_transform)\n","# Nevertheless, we only transform the test dataset into vectors  (transform, not fit_transform)\n","\n","model = MultinomialNB(alpha=.01)\n","model.fit(vectors_train, newsgroups_train.target)\n","\n","vectors_test = vectorizer.transform(newsgroups_test.data)\n","pred = model.predict(vectors_test)\n","\n","metrics.f1_score(newsgroups_test.target, pred, average='weighted')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.695453607190013"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRzlu3OpDBoz","outputId":"3f6f54bd-c56e-4e9b-cb2d-1c9092c963ed"},"source":["print(\"dimensionality: %d\" % model.coef_.shape[1])\n","print(\"density: %f\" % density(model.coef_))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dimensionality: 101322\n","density: 1.000000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jMBDnMTZDxsD"},"source":["Vemos como se pueden clasificar los textos en diferentes temáticas"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lq08_rxKDBq6","outputId":"f7e2cac8-ed3d-4310-faf0-947ec892620f"},"source":["# We can review the top features per topic in Bayes (attribute coef_)\n","def show_top10(classifier, vectorizer, categories):\n","    feature_names = np.asarray(vectorizer.get_feature_names())\n","    for i, category in enumerate(categories):\n","        top10 = np.argsort(classifier.coef_[i])[-10:]\n","        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n","\n","        \n","show_top10(model, vectorizer, newsgroups_train.target_names)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["alt.atheism: islam atheists say just religion atheism think don people god\n","comp.graphics: looking format 3d know program file files thanks image graphics\n","comp.os.ms-windows.misc: card problem thanks driver drivers use files dos file windows\n","comp.sys.ibm.pc.hardware: monitor disk thanks pc ide controller bus card scsi drive\n","comp.sys.mac.hardware: know monitor does quadra simms thanks problem drive apple mac\n","comp.windows.x: using windows x11r5 use application thanks widget server motif window\n","misc.forsale: asking email sell price condition new shipping offer 00 sale\n","rec.autos: don ford new good dealer just engine like cars car\n","rec.motorcycles: don just helmet riding like motorcycle ride bikes dod bike\n","rec.sport.baseball: braves players pitching hit runs games game baseball team year\n","rec.sport.hockey: league year nhl games season players play hockey team game\n","sci.crypt: people use escrow nsa keys government chip clipper encryption key\n","sci.electronics: don thanks voltage used know does like circuit power use\n","sci.med: skepticism cadre dsl banks chastity n3jxp pitt gordon geb msg\n","sci.space: just lunar earth shuttle like moon launch orbit nasa space\n","soc.religion.christian: believe faith christian christ bible people christians church jesus god\n","talk.politics.guns: just law firearms government fbi don weapons people guns gun\n","talk.politics.mideast: said arabs arab turkish people armenians armenian jews israeli israel\n","talk.politics.misc: know state clinton president just think tax don government people\n","talk.religion.misc: think don koresh objective christians bible people christian jesus god\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-x7bxs0VDBtN","outputId":"a58989e4-4ba2-4a00-a79f-e19df1f10164"},"source":["# We try the classifier in two new docs\n","\n","new_docs = ['This is a survey of PC computers', 'God is love']\n","new_vectors = vectorizer.transform(new_docs)\n","\n","pred_docs = model.predict(new_vectors)\n","print(pred_docs)\n","print([newsgroups_train.target_names[i] for i in pred_docs])\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 2 15]\n","['comp.os.ms-windows.misc', 'soc.religion.christian']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WNY4JnsUD5Fb"},"source":["**Modelos semánticos**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdQxkMssDBvS","outputId":"7920030e-680e-4902-8ec2-338c3109a76c"},"source":["# We filter only some categories, otherwise we have 20 categories\n","\n","# Eligiendo categorias para agrupar.\n","\n","categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n","# We remove metadata to avoid bias in the classification\n","newsgroups_train = fetch_20newsgroups(subset='train', \n","                                      remove=('headers', 'footers', 'quotes'), \n","                                      categories=categories)\n","newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),\n","                                    categories=categories)\n","\n","\n","# Obtain a vector\n","\n","\n","vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', min_df=10)\n","\n","vectors_train = vectorizer.fit_transform(newsgroups_train.data)\n","vectors_train.shape\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2034, 2807)"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"dWjvTL5yDBxW"},"source":["# Otra forma de modelar semánticamente es mediante LDA (Latent Dirichlet Allocation)\n","\n","# LDA. No ejecutado porque tarda bastante tiempo. \n","\n","lda = LdaModel(corpus_tfidf, num_topics=4, passes=20, id2word=dictionary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZGN_eSoxDBzS"},"source":["# import the gensim.corpora module to generate dictionary\n","from gensim import corpora\n","\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk import RegexpTokenizer\n","\n","import string\n","\n","def preprocess(words):\n","    tokenizer = RegexpTokenizer('[A-Z]\\w+')\n","    tokens = [w.lower() for w in tokenizer.tokenize(words)]\n","    stoplist = stopwords.words('english')\n","    tokens_stop = [w for w in tokens if w not in stoplist]\n","    punctuation = set(string.punctuation)\n","    tokens_clean = [w for w in tokens_stop if  w not in punctuation]\n","    return tokens_clean\n","\n","#words = preprocess(newsgroups_train.data)\n","#dictionary = corpora.Dictionary(newsgroups_train.data)\n","\n","texts = [preprocess(document) for document in newsgroups_train.data]\n","\n","dictionary = corpora.Dictionary(texts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OaGWZZC2EnUM"},"source":["**Combinar features**"]},{"cell_type":"code","metadata":{"id":"7swQVIW8DB3i"},"source":["import pandas as pd\n","\n","\n","# Hay que instalar el dataset desde Kaggle antes de ejecutar este trozo de codigo.\n","\n","df_orig = pd.read_csv(\"data-essays/training_set_rel3.tsv\", encoding='ISO-8859-1', delimiter=\"\\t\", header=0)\n","df_orig[0:4]\n","df_orig.shape\n","\n","\n","# We filter the data of the essay_set number 1, and we keep only two columns for this \n","# example\n","\n","df = df_orig[df_orig['essay_set'] == 1][['essay_id', 'essay', 'domain1_score']].copy()\n","df.shape\n","\n","df[0:5]\n","\n","\n","\n","# Define X and Y\n","X = df['essay'].values\n","y = df['domain1_score'].values\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"azUvL_cTDB5x"},"source":["# Generic Transformer \n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class GenericTransformer(BaseEstimator, TransformerMixin):\n","\n","    def transform(self, X, y=None):\n","        return do_something_to(X, self.vars)  # where the actual feature extraction happens\n","\n","    def fit(self, X, y=None):\n","        return self  # used if the feature requires training, for example, clustering"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWZXu3o6DB8D"},"source":["# Sample of statistics using nltk\n","# Another option is defining a function and pass it as a parameter to FunctionTransformer\n","\n","\n","class LexicalStats (BaseEstimator, TransformerMixin):\n","    \"\"\"Extract lexical features from each document\"\"\"\n","    \n","    def number_sentences(self, doc):\n","        sentences = sent_tokenize(doc, language='english')\n","        return len(sentences)\n","\n","    def fit(self, x, y=None):\n","        return self\n","\n","    def transform(self, docs):\n","        return [{'length': len(doc),\n","                 'num_sentences': self.number_sentences(doc)}\n","                for doc in docs]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkwtlgPHFIUe"},"source":["\n","def custom_tokenizer(words):\n","    \"\"\"Preprocessing tokens as seen in the lexical notebook\"\"\"\n","    tokens = word_tokenize(words.lower())\n","    porter = PorterStemmer()\n","    lemmas = [porter.stem(t) for t in tokens]\n","    stoplist = stopwords.words('english')\n","    lemmas_clean = [w for w in lemmas if w not in stoplist]\n","    punctuation = set(string.punctuation)\n","    lemmas_punct = [w for w in lemmas_clean if  w not in punctuation]\n","    return lemmas_punct\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLV9d6RKFIWt"},"source":["\n","class PosStats(BaseEstimator, TransformerMixin):\n","    \"\"\"Obtain number of tokens with POS categories\"\"\"\n","\n","    def stats(self, doc):\n","        tokens = custom_tokenizer(doc)\n","        tagged = pos_tag(tokens, tagset='universal')\n","        counts = Counter(tag for word,tag in tagged)\n","        total = sum(counts.values())\n","        #copy tags so that we return always the same number of features\n","        pos_features = {'NOUN': 0, 'ADJ': 0, 'VERB': 0, 'ADV': 0, 'CONJ': 0, \n","                        'ADP': 0, 'PRON':0, 'NUM': 0}\n","        \n","        pos_dic = dict((tag, float(count)/total) for tag,count in counts.items())\n","        for k in pos_dic:\n","            if k in pos_features:\n","                pos_features[k] = pos_dic[k]\n","        return pos_features\n","    \n","    def transform(self, docs, y=None):\n","        return [self.stats(doc) for doc in docs]\n","    \n","    def fit(self, docs, y=None):\n","        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n","        return self"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pjkUCpwxFIY9"},"source":["\n","ngrams_featurizer = Pipeline([\n","  ('count_vectorizer',  CountVectorizer(ngram_range = (1, 3), encoding = 'ISO-8859-1', \n","                                        tokenizer=custom_tokenizer)),\n","  ('tfidf_transformer', TfidfTransformer())\n","])\n","## All the steps of the Pipeline should end with a sparse vector as the input data\n","\n","pipeline = Pipeline([\n","       ('features', FeatureUnion([\n","                    ('lexical_stats', Pipeline([\n","                                ('stats', LexicalStats()),\n","                                ('vectors', DictVectorizer())\n","                            ])),\n","                    ('words', TfidfVectorizer(tokenizer=custom_tokenizer)),\n","                    ('ngrams', ngrams_featurizer),\n","                    ('pos_stats', Pipeline([\n","                                ('pos_stats', PosStats()),\n","                                ('vectors', DictVectorizer())\n","                            ])),\n","                    ('lda', Pipeline([ \n","                                ('count', CountVectorizer(tokenizer=custom_tokenizer)),\n","                                ('lda',  LatentDirichletAllocation(n_components=4, max_iter=5,\n","                                                       learning_method='online', \n","                                                       learning_offset=50.,\n","                                                       random_state=0))\n","                            ])),\n","                ])),\n","       \n","        ('clf', MultinomialNB(alpha=.01))  # classifier\n","    ])\n","\n","# Using KFold validation\n","\n","cv = KFold(2, shuffle=True, random_state=33)\n","scores = cross_val_score(pipeline, X, y, cv=cv)\n","print(\"Scores in every iteration\", scores)\n","print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7hwW5UPFpZl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GptB-2rTFIbV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKD5uGxEFIdc"},"source":[""],"execution_count":null,"outputs":[]}]}