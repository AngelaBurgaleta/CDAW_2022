{"cells":[{"cell_type":"markdown","metadata":{"id":"KUkWFNEC7atn"},"source":["![](images/EscUpmPolit_p.gif \"UPM\")"]},{"cell_type":"markdown","metadata":{"id":"jMvi76lU7atr"},"source":["# Course Notes for Learning Intelligent Systems"]},{"cell_type":"markdown","metadata":{"id":"0tzudAIm7ats"},"source":["Department of Telematic Engineering Systems, Universidad Politécnica de Madrid, © Carlos A. Iglesias"]},{"cell_type":"markdown","metadata":{"id":"f37gifd_7ats"},"source":["# Combining Features"]},{"cell_type":"markdown","metadata":{"id":"JX-ufgXK7att"},"source":["# Table of Contents\n","* [Objectives](#Objectives)\n","* [Dataset](#Dataset)\n","* [Loading the dataset](#Loading-the-dataset)\n","* [Transformers](#Transformers)\n","* [Lexical features](#Lexical-features)\n","* [Syntactic features](#Syntactic-features)\n","* [Feature Extraction Pipelines](#Feature-Extraction-Pipelines)\n","* [Feature Union Pipeline](#Feature-Union-Pipeline)"]},{"cell_type":"markdown","metadata":{"id":"tx23jITa7att"},"source":["# Objectives"]},{"cell_type":"markdown","metadata":{"id":"6VMRo3A_7att"},"source":["In the previous section we have seen how to analyse lexical, syntactic and semantic features. All these features can help in machine learning techniques.\n","\n","In this notebook we are going to learn how to combine them. \n","\n","There are several approaches for combining features, at character, lexical, syntactical, semantic or behavioural levels. \n","\n","Some authors obtain the different featuras as lists and then join these lists, a good example is shown [here](http://www.aicbt.com/authorship-attribution/) for authorship attribution. Other authors use *FeatureUnion* to join the different sparse matrices, as shown [here](http://es.slideshare.net/PyData/authorship-attribution-forensic-linguistics-with-python-scikit-learn-pandas-kostas-perifanos) and [here](http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html). Finally, other authors use FeatureUnions with weights, as shown in [scikit-learn documentation](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html).\n","\n","A *FeatureUnion* is built using a list of (key, value) pairs, where the key is the name you want to give to a given transformation (an arbitrary string; it only serves as an identifier) and value is an estimator object.\n","\n","In this chapter we are going to follow the combination of Pipelines and FeatureUnions, as described in scikit-learn, [Zac Stewart](http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html), his [Kaggle submission](https://github.com/zacstewart/kaggle_seeclickfix/blob/master/estimator.py), and [Michelle Fullwood](https://michelleful.github.io/code-blog/2015/06/20/pipelines/), since it provides a simple and structured approach."]},{"cell_type":"markdown","metadata":{"id":"ZEQgTtSN7atu"},"source":["# Dataset"]},{"cell_type":"markdown","metadata":{"id":"XFbDgVZk7atv"},"source":["We are going to use one [dataset from Kaggle](https://www.kaggle.com/c/asap-aes/) for automatic essay scoring, a very interesting area for teachers.\n","\n","The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.For this competition, there are eight essay sets. Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities.\n","\n","Each of these files contains 28 columns:\n","\n","* **essay_id**: A unique identifier for each individual student essay\n","* **essay_set**: 1-8, an id for each set of essays\n","* **essay**: The ascii text of a student's response\n","* **rater1_domain1**: Rater 1's domain 1 score; all essays have this\n","* **rater2_domain1**: Rater 2's domain 1 score; all essays have this\n","* **rater3_domain1**: Rater 3's domain 1 score; only some essays in set 8 have this.\n","* **domain1_score**: Resolved score between the raters; all essays have this\n","* **rater1_domain2**: Rater 1's domain 2 score; only essays in set 2 have this\n","* **rater2_domain2**: Rater 2's domain 2 score; only essays in set 2 have this\n","* **domain2_score**: Resolved score between the raters; only essays in set 2 have this\n","* **rater1_trait1 score - rater3_trait6 score**: trait scores for sets 7-8\n","\n","The dataset is provided in the folder *data-kaggle/training_set_rel3.tsv*.\n","\n","There are cases in the training set that contain ???, \"illegible\", or \"not legible\" on some words. You may choose to discard them if you wish, and essays with illegible words will not be present in the validation or test sets.\n","\n","The dataset has been anonymized  to remove personally identifying information from the essays using the Named Entity Recognizer (NER) from the Stanford Natural Language Processing group and a variety of other approaches. The relevant entities are identified in the text and then replaced with a string such as \"@PERSON1.\"\n","\n","The entities identified by NER are: \"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"DATE\", \"TIME\", \"MONEY\", \"PERCENT\"\n","\n","Other replacements made: \"MONTH\" (any month name not tagged as a date by the NER), \"EMAIL\" (anything that looks like an e-mail address), \"NUM\" (word containing digits or non-alphanumeric symbols), and \"CAPS\" (any capitalized word that doesn't begin a sentence, except in essays where more than 20% of the characters are capitalized letters), \"DR\" (any word following \"Dr.\" with or without the period, with any capitalization, that doesn't fall into any of the above), \"CITY\" and \"STATE\" (various cities and states)."]},{"cell_type":"markdown","metadata":{"id":"itTgyEap7atw"},"source":["# Loading the dataset"]},{"cell_type":"markdown","metadata":{"id":"0IpuWEch7atw"},"source":["We will use Pandas to load the dataset. We will not go deeper in analysing the dataset, using the techniques already seen previously."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":598},"id":"suoNOGtp7atx","executionInfo":{"status":"ok","timestamp":1652017974547,"user_tz":-120,"elapsed":946,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"1e098199-68c4-40fb-88c9-971f44b52c2a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   essay_id  essay_set                                              essay  \\\n","0         1          1  Dear local newspaper, I think effects computer...   \n","1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n","2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n","3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n","\n","   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n","0               4               4             NaN              8   \n","1               5               4             NaN              9   \n","2               4               3             NaN              7   \n","3               5               5             NaN             10   \n","\n","   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n","0             NaN             NaN            NaN  ...            NaN   \n","1             NaN             NaN            NaN  ...            NaN   \n","2             NaN             NaN            NaN  ...            NaN   \n","3             NaN             NaN            NaN  ...            NaN   \n","\n","   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n","0            NaN            NaN            NaN            NaN            NaN   \n","1            NaN            NaN            NaN            NaN            NaN   \n","2            NaN            NaN            NaN            NaN            NaN   \n","3            NaN            NaN            NaN            NaN            NaN   \n","\n","   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n","0            NaN            NaN            NaN            NaN  \n","1            NaN            NaN            NaN            NaN  \n","2            NaN            NaN            NaN            NaN  \n","3            NaN            NaN            NaN            NaN  \n","\n","[4 rows x 28 columns]"],"text/html":["\n","  <div id=\"df-44292c19-2fa5-49c4-bba4-079dbdfa7141\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>essay_id</th>\n","      <th>essay_set</th>\n","      <th>essay</th>\n","      <th>rater1_domain1</th>\n","      <th>rater2_domain1</th>\n","      <th>rater3_domain1</th>\n","      <th>domain1_score</th>\n","      <th>rater1_domain2</th>\n","      <th>rater2_domain2</th>\n","      <th>domain2_score</th>\n","      <th>...</th>\n","      <th>rater2_trait3</th>\n","      <th>rater2_trait4</th>\n","      <th>rater2_trait5</th>\n","      <th>rater2_trait6</th>\n","      <th>rater3_trait1</th>\n","      <th>rater3_trait2</th>\n","      <th>rater3_trait3</th>\n","      <th>rater3_trait4</th>\n","      <th>rater3_trait5</th>\n","      <th>rater3_trait6</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Dear local newspaper, I think effects computer...</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>8</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>9</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows × 28 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44292c19-2fa5-49c4-bba4-079dbdfa7141')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-44292c19-2fa5-49c4-bba4-079dbdfa7141 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-44292c19-2fa5-49c4-bba4-079dbdfa7141');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}],"source":["import pandas as pd\n","\n","# The files are coded in ISO-8859-1\n","\n","df_orig = pd.read_csv(\"training_set_rel3.tsv\", encoding='ISO-8859-1', delimiter=\"\\t\", header=0)\n","df_orig[0:4]"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEIgs1NW7aty","executionInfo":{"status":"ok","timestamp":1652017978918,"user_tz":-120,"elapsed":189,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"cc409bb0-1742-4143-a65c-e7c792dd9379"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(12976, 28)"]},"metadata":{},"execution_count":6}],"source":["df_orig.shape"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SNiWyf_7aty","executionInfo":{"status":"ok","timestamp":1652017980658,"user_tz":-120,"elapsed":182,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"1cc8bb86-1d3d-4dad-a2ae-397bf1df488e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1783, 3)"]},"metadata":{},"execution_count":7}],"source":["# We filter the data of the essay_set number 1, and we keep only two columns for this \n","# example\n","\n","df = df_orig[df_orig['essay_set'] == 1][['essay_id', 'essay', 'domain1_score']].copy()\n","df.shape"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"0pFBaqUj7atz","executionInfo":{"status":"ok","timestamp":1652017981597,"user_tz":-120,"elapsed":4,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"739c60fd-e73f-438c-b409-4d8291860add"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   essay_id                                              essay  domain1_score\n","0         1  Dear local newspaper, I think effects computer...              8\n","1         2  Dear @CAPS1 @CAPS2, I believe that using compu...              9\n","2         3  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...              7\n","3         4  Dear Local Newspaper, @CAPS1 I have found that...             10\n","4         5  Dear @LOCATION1, I know having computers has a...              8"],"text/html":["\n","  <div id=\"df-19156c18-6d40-406c-aa9a-154c93671892\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>essay_id</th>\n","      <th>essay</th>\n","      <th>domain1_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Dear local newspaper, I think effects computer...</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Dear @LOCATION1, I know having computers has a...</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19156c18-6d40-406c-aa9a-154c93671892')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-19156c18-6d40-406c-aa9a-154c93671892 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-19156c18-6d40-406c-aa9a-154c93671892');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"source":["df[0:5]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"inl1BMRE7atz","executionInfo":{"status":"ok","timestamp":1652017983224,"user_tz":-120,"elapsed":206,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}}},"outputs":[],"source":["# Define X and Y\n","X = df['essay'].values\n","y = df['domain1_score'].values"]},{"cell_type":"markdown","metadata":{"id":"hOdqYF6n7atz"},"source":["# Transformers"]},{"cell_type":"markdown","metadata":{"id":"crQSgSLl7at0"},"source":["Every feature extractor should be implemented as a custom Transformer. A transformer can be seen as an object that receives data, applies some changes, and returns the data, usually with the same same that the input. The methods we should implement are:\n","* *fit* method, in case we need to learn and train for extracting the feature\n","* *transform method*, that applies the defined transformation to unseen data"]},{"cell_type":"markdown","metadata":{"id":"y1CiKX7C7at0"},"source":["Now we show the general approach to develop transformers"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"SLWs8bw17at0","executionInfo":{"status":"ok","timestamp":1652017985754,"user_tz":-120,"elapsed":534,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}}},"outputs":[],"source":["# Generic Transformer \n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class GenericTransformer(BaseEstimator, TransformerMixin):\n","\n","    def transform(self, X, y=None):\n","        return do_something_to(X, self.vars)  # where the actual feature extraction happens\n","\n","    def fit(self, X, y=None):\n","        return self  # used if the feature requires training, for example, clustering"]},{"cell_type":"markdown","metadata":{"id":"ODW9zvou7at0"},"source":["Scikit-learn provides a class [FunctionTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) that makes easy to create new transformers. We have to provide a function that is executed in the method transform()."]},{"cell_type":"markdown","metadata":{"id":"w9hrux6L7at1"},"source":["# Lexical features"]},{"cell_type":"markdown","metadata":{"id":"btia7PMX7at1"},"source":["Here we include some examples of lexical features. We have omitted character features (for example, number of exclamation marks)."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ZVtfBLUy7at1","executionInfo":{"status":"ok","timestamp":1652017988850,"user_tz":-120,"elapsed":937,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}}},"outputs":[],"source":["# Sample of statistics using nltk\n","# Another option is defining a function and pass it as a parameter to FunctionTransformer\n","\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","class LexicalStats (BaseEstimator, TransformerMixin):\n","    \"\"\"Extract lexical features from each document\"\"\"\n","    \n","    def number_sentences(self, doc):\n","        sentences = sent_tokenize(doc, language='english')\n","        return len(sentences)\n","\n","    def fit(self, x, y=None):\n","        return self\n","\n","    def transform(self, docs):\n","        return [{'length': len(doc),\n","                 'num_sentences': self.number_sentences(doc)}\n","                for doc in docs]\n","\n","    "]},{"cell_type":"code","execution_count":12,"metadata":{"id":"UoNd4O2o7at2","executionInfo":{"status":"ok","timestamp":1652017990989,"user_tz":-120,"elapsed":289,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}}},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from nltk.stem import PorterStemmer\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","import string\n","\n","def custom_tokenizer(words):\n","    \"\"\"Preprocessing tokens as seen in the lexical notebook\"\"\"\n","    tokens = word_tokenize(words.lower())\n","    porter = PorterStemmer()\n","    lemmas = [porter.stem(t) for t in tokens]\n","    stoplist = stopwords.words('english')\n","    lemmas_clean = [w for w in lemmas if w not in stoplist]\n","    punctuation = set(string.punctuation)\n","    lemmas_punct = [w for w in lemmas_clean if  w not in punctuation]\n","    return lemmas_punct"]},{"cell_type":"markdown","metadata":{"id":"ECyu59CK7at2"},"source":["# Syntactic features"]},{"cell_type":"markdown","metadata":{"id":"GqbUBnfV7at2"},"source":["Here we include and example of syntactic feature extraction."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XmdrvCYF7at2","executionInfo":{"status":"ok","timestamp":1652017992941,"user_tz":-120,"elapsed":177,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}}},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from nltk import pos_tag\n","from collections import Counter \n","\n","class PosStats(BaseEstimator, TransformerMixin):\n","    \"\"\"Obtain number of tokens with POS categories\"\"\"\n","\n","    def stats(self, doc):\n","        tokens = custom_tokenizer(doc)\n","        tagged = pos_tag(tokens, tagset='universal')\n","        counts = Counter(tag for word,tag in tagged)\n","        total = sum(counts.values())\n","        #copy tags so that we return always the same number of features\n","        pos_features = {'NOUN': 0, 'ADJ': 0, 'VERB': 0, 'ADV': 0, 'CONJ': 0, \n","                        'ADP': 0, 'PRON':0, 'NUM': 0}\n","        \n","        pos_dic = dict((tag, float(count)/total) for tag,count in counts.items())\n","        for k in pos_dic:\n","            if k in pos_features:\n","                pos_features[k] = pos_dic[k]\n","        return pos_features\n","    \n","    def transform(self, docs, y=None):\n","        return [self.stats(doc) for doc in docs]\n","    \n","    def fit(self, docs, y=None):\n","        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n","        return self"]},{"cell_type":"markdown","metadata":{"id":"a4gM_RAo7at3"},"source":["# Feature Extraction Pipelines"]},{"cell_type":"markdown","metadata":{"id":"JvYIM1oO7at3"},"source":["We define Pipelines to extract the desired features.\n","\n","In case we want to apply different processing techniques to different part of the corpus (e.g. title or body or, ...), look [here](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html) for an example of how to extract and process the different parts into a Pipeline."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"DUfLljP77at3","executionInfo":{"status":"ok","timestamp":1652017996030,"user_tz":-120,"elapsed":206,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}}},"outputs":[],"source":["from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n","\n","\n","ngrams_featurizer = Pipeline([\n","  ('count_vectorizer',  CountVectorizer(ngram_range = (1, 3), encoding = 'ISO-8859-1', \n","                                        tokenizer=custom_tokenizer)),\n","  ('tfidf_transformer', TfidfTransformer())\n","])"]},{"cell_type":"markdown","metadata":{"id":"hsC_i2u57at3"},"source":["# Feature Union Pipeline"]},{"cell_type":"markdown","metadata":{"id":"4tMcFIMg7at4"},"source":["Now we can ensemble the different pipelines to define which features we want to extract, how to combine them, and apply later machine learning techniques to the resulting feature set.\n","\n","In Feature Union we can pass either a pipeline or a transformer.\n","\n","The basic idea is:\n","* **Pipelines** consist of sequential steps: one step works on the results of the previous step\n","* **FeatureUnions** consist of parallel tasks whose result is grouped when all have finished."]},{"cell_type":"code","source":["import nltk\n","nltk.download()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7uV2Bi6d8kNX","executionInfo":{"status":"ok","timestamp":1652018028381,"user_tz":-120,"elapsed":17343,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"3e686fd7-2fbe-42b2-fb85-7caf80c11d5e"},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> d\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> book\n","    Downloading collection 'book'\n","       | \n","       | Downloading package abc to /root/nltk_data...\n","       |   Unzipping corpora/abc.zip.\n","       | Downloading package brown to /root/nltk_data...\n","       |   Unzipping corpora/brown.zip.\n","       | Downloading package chat80 to /root/nltk_data...\n","       |   Unzipping corpora/chat80.zip.\n","       | Downloading package cmudict to /root/nltk_data...\n","       |   Unzipping corpora/cmudict.zip.\n","       | Downloading package conll2000 to /root/nltk_data...\n","       |   Unzipping corpora/conll2000.zip.\n","       | Downloading package conll2002 to /root/nltk_data...\n","       |   Unzipping corpora/conll2002.zip.\n","       | Downloading package dependency_treebank to /root/nltk_data...\n","       |   Unzipping corpora/dependency_treebank.zip.\n","       | Downloading package genesis to /root/nltk_data...\n","       |   Unzipping corpora/genesis.zip.\n","       | Downloading package gutenberg to /root/nltk_data...\n","       |   Unzipping corpora/gutenberg.zip.\n","       | Downloading package ieer to /root/nltk_data...\n","       |   Unzipping corpora/ieer.zip.\n","       | Downloading package inaugural to /root/nltk_data...\n","       |   Unzipping corpora/inaugural.zip.\n","       | Downloading package movie_reviews to /root/nltk_data...\n","       |   Unzipping corpora/movie_reviews.zip.\n","       | Downloading package nps_chat to /root/nltk_data...\n","       |   Unzipping corpora/nps_chat.zip.\n","       | Downloading package names to /root/nltk_data...\n","       |   Unzipping corpora/names.zip.\n","       | Downloading package ppattach to /root/nltk_data...\n","       |   Unzipping corpora/ppattach.zip.\n","       | Downloading package reuters to /root/nltk_data...\n","       | Downloading package senseval to /root/nltk_data...\n","       |   Unzipping corpora/senseval.zip.\n","       | Downloading package state_union to /root/nltk_data...\n","       |   Unzipping corpora/state_union.zip.\n","       | Downloading package stopwords to /root/nltk_data...\n","       |   Unzipping corpora/stopwords.zip.\n","       | Downloading package swadesh to /root/nltk_data...\n","       |   Unzipping corpora/swadesh.zip.\n","       | Downloading package timit to /root/nltk_data...\n","       |   Unzipping corpora/timit.zip.\n","       | Downloading package treebank to /root/nltk_data...\n","       |   Unzipping corpora/treebank.zip.\n","       | Downloading package toolbox to /root/nltk_data...\n","       |   Unzipping corpora/toolbox.zip.\n","       | Downloading package udhr to /root/nltk_data...\n","       |   Unzipping corpora/udhr.zip.\n","       | Downloading package udhr2 to /root/nltk_data...\n","       |   Unzipping corpora/udhr2.zip.\n","       | Downloading package unicode_samples to /root/nltk_data...\n","       |   Unzipping corpora/unicode_samples.zip.\n","       | Downloading package webtext to /root/nltk_data...\n","       |   Unzipping corpora/webtext.zip.\n","       | Downloading package wordnet to /root/nltk_data...\n","       |   Unzipping corpora/wordnet.zip.\n","       | Downloading package wordnet_ic to /root/nltk_data...\n","       |   Unzipping corpora/wordnet_ic.zip.\n","       | Downloading package words to /root/nltk_data...\n","       |   Unzipping corpora/words.zip.\n","       | Downloading package maxent_treebank_pos_tagger to\n","       |     /root/nltk_data...\n","       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","       | Downloading package maxent_ne_chunker to /root/nltk_data...\n","       |   Unzipping chunkers/maxent_ne_chunker.zip.\n","       | Downloading package universal_tagset to /root/nltk_data...\n","       |   Unzipping taggers/universal_tagset.zip.\n","       | Downloading package punkt to /root/nltk_data...\n","       |   Unzipping tokenizers/punkt.zip.\n","       | Downloading package book_grammars to /root/nltk_data...\n","       |   Unzipping grammars/book_grammars.zip.\n","       | Downloading package city_database to /root/nltk_data...\n","       |   Unzipping corpora/city_database.zip.\n","       | Downloading package tagsets to /root/nltk_data...\n","       |   Unzipping help/tagsets.zip.\n","       | Downloading package panlex_swadesh to /root/nltk_data...\n","       | Downloading package averaged_perceptron_tagger to\n","       |     /root/nltk_data...\n","       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","       | \n","     Done downloading collection book\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> q\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-m3TuwCS7at4","executionInfo":{"status":"ok","timestamp":1652018254353,"user_tz":-120,"elapsed":223873,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"0a9a07d1-326d-4ab4-8977-da58cd80c5d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Scores in every iteration [0.39125561 0.43097643]\n","Accuracy: 0.41 (+/- 0.04)\n"]}],"source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import cross_val_score, KFold\n","from sklearn.metrics import classification_report\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.decomposition import NMF, LatentDirichletAllocation\n","\n","\n","\n","## All the steps of the Pipeline should end with a sparse vector as the input data\n","\n","pipeline = Pipeline([\n","       ('features', FeatureUnion([\n","                    ('lexical_stats', Pipeline([\n","                                ('stats', LexicalStats()),\n","                                ('vectors', DictVectorizer())\n","                            ])),\n","                    ('words', TfidfVectorizer(tokenizer=custom_tokenizer)),\n","                    ('ngrams', ngrams_featurizer),\n","                    ('pos_stats', Pipeline([\n","                                ('pos_stats', PosStats()),\n","                                ('vectors', DictVectorizer())\n","                            ])),\n","                    ('lda', Pipeline([ \n","                                ('count', CountVectorizer(tokenizer=custom_tokenizer)),\n","                                ('lda',  LatentDirichletAllocation(n_components=4, max_iter=5,\n","                                                       learning_method='online', \n","                                                       learning_offset=50.,\n","                                                       random_state=0))\n","                            ])),\n","                ])),\n","       \n","        ('clf', MultinomialNB(alpha=.01))  # classifier\n","    ])\n","\n","# Using KFold validation\n","\n","cv = KFold(2, shuffle=True, random_state=33)\n","scores = cross_val_score(pipeline, X, y, cv=cv)\n","print(\"Scores in every iteration\", scores)\n","print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"]},{"cell_type":"markdown","metadata":{"id":"fWVxf7Zb7at4"},"source":["The result is not very good :(."]},{"cell_type":"markdown","metadata":{"id":"KKB5bEGW7at5"},"source":["# References"]},{"cell_type":"markdown","metadata":{"id":"u6T8SCzu7at5"},"source":["* [NLTK Book. Natural Language Processing with Python. Steven Bird, Ewan Klein, and Edward Loper. O'Reilly Media, 2009 ](http://www.nltk.org/book_1ed/)\n","* [NLTK Essentials, Nitin Hardeniya, Packt Publishing, 2015](http://proquest.safaribooksonline.com/search?q=NLTK%20Essentials)"]},{"cell_type":"markdown","metadata":{"id":"BsNUheQO7at5"},"source":["## Licence"]},{"cell_type":"markdown","metadata":{"id":"UEha8A9g7at5"},"source":["The notebook is freely licensed under under the [Creative Commons Attribution Share-Alike license](https://creativecommons.org/licenses/by/2.0/).  \n","\n","© Carlos A. Iglesias, Universidad Politécnica de Madrid."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"latex_envs":{"LaTeX_envs_menu_present":true,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"colab":{"name":"4_6_Combining_Features.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}