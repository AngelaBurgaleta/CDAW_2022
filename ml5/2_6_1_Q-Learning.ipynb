{"cells":[{"cell_type":"markdown","metadata":{"id":"EF-pI12yQEti"},"source":["![](images/EscUpmPolit_p.gif \"UPM\")"]},{"cell_type":"markdown","metadata":{"id":"Rogj8DcEQEtk"},"source":["# Course Notes for Learning Intelligent Systems"]},{"cell_type":"markdown","metadata":{"id":"neFStUJXQEtk"},"source":["Department of Telematic Engineering Systems, Universidad Politécnica de Madrid, © 2018 Carlos A. Iglesias"]},{"cell_type":"markdown","metadata":{"id":"FvWVP5uYQEtl"},"source":["## [Introduction to Machine Learning V](2_6_0_Intro_RL.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"c5QCpjqrQEtm"},"source":["# Table of Contents\n","\n","* [Introduction](#Introduction)\n","* [Getting started with OpenAI Gym](#Getting-started-with-OpenAI-Gym)\n","* [The Frozen Lake scenario](#The-Frozen-Lake-scenario)\n","* [Q-Learning with the Frozen Lake scenario](#Q-Learning-with-the-Frozen-Lake-scenario)\n","* [Exercises](#Exercises)\n","* [Optional exercises](#Optional-exercises)"]},{"cell_type":"markdown","metadata":{"id":"DitwyelWQEtm"},"source":["# Introduction\n","The purpose of this practice is to understand better Reinforcement Learning (RL) and, in particular, Q-Learning.\n","\n","We are going to use [OpenAI Gym](https://gym.openai.com/). OpenAI is a toolkit for developing and comparing RL algorithms.Take a loot at ther [website](https://gym.openai.com/).\n","\n","It implements [algorithm imitation](http://gym.openai.com/envs/#algorithmic), [classic control problems](http://gym.openai.com/envs/#classic_control), [Atari games](http://gym.openai.com/envs/#atari), [Box2D continuous control](http://gym.openai.com/envs/#box2d), [robotics with MuJoCo, Multi-Joint dynamics with Contact](http://gym.openai.com/envs/#mujoco),  and [simple text based environments](http://gym.openai.com/envs/#toy_text).\n","\n","This notebook is based on * [Diving deeper into Reinforcement Learning with Q-Learning](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe).\n","\n","First of all, install the OpenAI Gym  library:\n","\n","```console\n","foo@bar:~$ pip install gym\n","```\n","\n","\n","If you get the error message 'NotImplementedError: abstract', [execute](https://github.com/openai/gym/issues/775) \n","```console\n","foo@bar:~$ pip install pyglet==1.2.4\n","```\n","\n","If you want to try the Atari environment, it is better that you opt for the full installation from the source. Follow the instructions at [https://github.com/openai/gym#id15](OpenGym).\n"]},{"cell_type":"markdown","metadata":{"id":"Kxhg2i_vQEtp"},"source":["# Getting started with OpenAI Gym\n","\n","First of all, read the [introduction](http://gym.openai.com/docs/#getting-started-with-gym) of OpenAI Gym."]},{"cell_type":"markdown","metadata":{"id":"9F5FMH3_QEtq"},"source":["## Environments\n","OpenGym provides a number of problems called *environments*. \n","\n","Try the 'CartPole-v0' (or 'MountainCar)."]},{"cell_type":"code","source":["import numpy as np\n","import gym\n","import random \n","#Instalación de OpenAI Gymm Library para el entorno de colab\n","!pip install gym\n","!pip install pyglet==1.2.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"topuymYKQcKf","executionInfo":{"status":"ok","timestamp":1651050318797,"user_tz":-120,"elapsed":7621,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"c47c248a-a45c-4faf-cf62-781b0c211949"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n","Collecting pyglet==1.2.4\n","  Downloading pyglet-1.2.4-py3-none-any.whl (964 kB)\n","\u001b[K     |████████████████████████████████| 964 kB 4.7 MB/s \n","\u001b[?25hInstalling collected packages: pyglet\n","  Attempting uninstall: pyglet\n","    Found existing installation: pyglet 1.5.0\n","    Uninstalling pyglet-1.5.0:\n","      Successfully uninstalled pyglet-1.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gym 0.17.3 requires pyglet<=1.5.0,>=1.4.0, but you have pyglet 1.2.4 which is incompatible.\u001b[0m\n","Successfully installed pyglet-1.2.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pyglet"]}}},"metadata":{}}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"8oG8GrD7QEtr","executionInfo":{"status":"error","timestamp":1651050372081,"user_tz":-120,"elapsed":218,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"63b1cd44-33e2-476f-97e6-8cb114e9b6a2"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-8453ef6796e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# your agent here (this takes random actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"]}],"source":["\n","env = gym.make(\"CartPole-v1\")\n","#env = gym.make('MountainCar-v0')\n","#env = gym.make('Taxi-v2')\n","\n","observation = env.reset()\n","for _ in range(1000):\n","  env.render()\n","  action = env.action_space.sample() # your agent here (this takes random actions)\n","  observation, reward, done, info = env.step(action)\n","\n","  if done:\n","    observation = env.reset()\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"D9UlptbeQEtt"},"source":["This will launch an external window with the game. If you cannot close that window, just execute in a code cell:\n","\n","```python\n","env.close()\n","```\n","\n","The full list of available environments can be found printing the environment registry as follows."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BwFcucPCQEtu","executionInfo":{"status":"ok","timestamp":1651050359267,"user_tz":-120,"elapsed":460,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"caa56601-f467-4d21-9dd1-c946a2180e14"},"outputs":[{"output_type":"stream","name":"stdout","text":["dict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v3), EnvSpec(BipedalWalkerHardcore-v3), EnvSpec(CarRacing-v0), EnvSpec(Blackjack-v0), EnvSpec(KellyCoinflip-v0), EnvSpec(KellyCoinflipGeneralized-v0), EnvSpec(FrozenLake-v0), EnvSpec(FrozenLake8x8-v0), EnvSpec(CliffWalking-v0), EnvSpec(NChain-v0), EnvSpec(Roulette-v0), EnvSpec(Taxi-v3), EnvSpec(GuessingGame-v0), EnvSpec(HotterColder-v0), EnvSpec(Reacher-v2), EnvSpec(Pusher-v2), EnvSpec(Thrower-v2), EnvSpec(Striker-v2), EnvSpec(InvertedPendulum-v2), EnvSpec(InvertedDoublePendulum-v2), EnvSpec(HalfCheetah-v2), EnvSpec(HalfCheetah-v3), EnvSpec(Hopper-v2), EnvSpec(Hopper-v3), EnvSpec(Swimmer-v2), EnvSpec(Swimmer-v3), EnvSpec(Walker2d-v2), EnvSpec(Walker2d-v3), EnvSpec(Ant-v2), EnvSpec(Ant-v3), EnvSpec(Humanoid-v2), EnvSpec(Humanoid-v3), EnvSpec(HumanoidStandup-v2), EnvSpec(FetchSlide-v1), EnvSpec(FetchPickAndPlace-v1), EnvSpec(FetchReach-v1), EnvSpec(FetchPush-v1), EnvSpec(HandReach-v0), EnvSpec(HandManipulateBlockRotateZ-v0), EnvSpec(HandManipulateBlockRotateZTouchSensors-v0), EnvSpec(HandManipulateBlockRotateZTouchSensors-v1), EnvSpec(HandManipulateBlockRotateParallel-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensors-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensors-v1), EnvSpec(HandManipulateBlockRotateXYZ-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensors-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensors-v1), EnvSpec(HandManipulateBlockFull-v0), EnvSpec(HandManipulateBlock-v0), EnvSpec(HandManipulateBlockTouchSensors-v0), EnvSpec(HandManipulateBlockTouchSensors-v1), EnvSpec(HandManipulateEggRotate-v0), EnvSpec(HandManipulateEggRotateTouchSensors-v0), EnvSpec(HandManipulateEggRotateTouchSensors-v1), EnvSpec(HandManipulateEggFull-v0), EnvSpec(HandManipulateEgg-v0), EnvSpec(HandManipulateEggTouchSensors-v0), EnvSpec(HandManipulateEggTouchSensors-v1), EnvSpec(HandManipulatePenRotate-v0), EnvSpec(HandManipulatePenRotateTouchSensors-v0), EnvSpec(HandManipulatePenRotateTouchSensors-v1), EnvSpec(HandManipulatePenFull-v0), EnvSpec(HandManipulatePen-v0), EnvSpec(HandManipulatePenTouchSensors-v0), EnvSpec(HandManipulatePenTouchSensors-v1), EnvSpec(FetchSlideDense-v1), EnvSpec(FetchPickAndPlaceDense-v1), EnvSpec(FetchReachDense-v1), EnvSpec(FetchPushDense-v1), EnvSpec(HandReachDense-v0), EnvSpec(HandManipulateBlockRotateZDense-v0), EnvSpec(HandManipulateBlockRotateZTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateZTouchSensorsDense-v1), EnvSpec(HandManipulateBlockRotateParallelDense-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensorsDense-v1), EnvSpec(HandManipulateBlockRotateXYZDense-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensorsDense-v1), EnvSpec(HandManipulateBlockFullDense-v0), EnvSpec(HandManipulateBlockDense-v0), EnvSpec(HandManipulateBlockTouchSensorsDense-v0), EnvSpec(HandManipulateBlockTouchSensorsDense-v1), EnvSpec(HandManipulateEggRotateDense-v0), EnvSpec(HandManipulateEggRotateTouchSensorsDense-v0), EnvSpec(HandManipulateEggRotateTouchSensorsDense-v1), EnvSpec(HandManipulateEggFullDense-v0), EnvSpec(HandManipulateEggDense-v0), EnvSpec(HandManipulateEggTouchSensorsDense-v0), EnvSpec(HandManipulateEggTouchSensorsDense-v1), EnvSpec(HandManipulatePenRotateDense-v0), EnvSpec(HandManipulatePenRotateTouchSensorsDense-v0), EnvSpec(HandManipulatePenRotateTouchSensorsDense-v1), EnvSpec(HandManipulatePenFullDense-v0), EnvSpec(HandManipulatePenDense-v0), EnvSpec(HandManipulatePenTouchSensorsDense-v0), EnvSpec(HandManipulatePenTouchSensorsDense-v1), EnvSpec(Adventure-v0), EnvSpec(Adventure-v4), EnvSpec(AdventureDeterministic-v0), EnvSpec(AdventureDeterministic-v4), EnvSpec(AdventureNoFrameskip-v0), EnvSpec(AdventureNoFrameskip-v4), EnvSpec(Adventure-ram-v0), EnvSpec(Adventure-ram-v4), EnvSpec(Adventure-ramDeterministic-v0), EnvSpec(Adventure-ramDeterministic-v4), EnvSpec(Adventure-ramNoFrameskip-v0), EnvSpec(Adventure-ramNoFrameskip-v4), EnvSpec(AirRaid-v0), EnvSpec(AirRaid-v4), EnvSpec(AirRaidDeterministic-v0), EnvSpec(AirRaidDeterministic-v4), EnvSpec(AirRaidNoFrameskip-v0), EnvSpec(AirRaidNoFrameskip-v4), EnvSpec(AirRaid-ram-v0), EnvSpec(AirRaid-ram-v4), EnvSpec(AirRaid-ramDeterministic-v0), EnvSpec(AirRaid-ramDeterministic-v4), EnvSpec(AirRaid-ramNoFrameskip-v0), EnvSpec(AirRaid-ramNoFrameskip-v4), EnvSpec(Alien-v0), EnvSpec(Alien-v4), EnvSpec(AlienDeterministic-v0), EnvSpec(AlienDeterministic-v4), EnvSpec(AlienNoFrameskip-v0), EnvSpec(AlienNoFrameskip-v4), EnvSpec(Alien-ram-v0), EnvSpec(Alien-ram-v4), EnvSpec(Alien-ramDeterministic-v0), EnvSpec(Alien-ramDeterministic-v4), EnvSpec(Alien-ramNoFrameskip-v0), EnvSpec(Alien-ramNoFrameskip-v4), EnvSpec(Amidar-v0), EnvSpec(Amidar-v4), EnvSpec(AmidarDeterministic-v0), EnvSpec(AmidarDeterministic-v4), EnvSpec(AmidarNoFrameskip-v0), EnvSpec(AmidarNoFrameskip-v4), EnvSpec(Amidar-ram-v0), EnvSpec(Amidar-ram-v4), EnvSpec(Amidar-ramDeterministic-v0), EnvSpec(Amidar-ramDeterministic-v4), EnvSpec(Amidar-ramNoFrameskip-v0), EnvSpec(Amidar-ramNoFrameskip-v4), EnvSpec(Assault-v0), EnvSpec(Assault-v4), EnvSpec(AssaultDeterministic-v0), EnvSpec(AssaultDeterministic-v4), EnvSpec(AssaultNoFrameskip-v0), EnvSpec(AssaultNoFrameskip-v4), EnvSpec(Assault-ram-v0), EnvSpec(Assault-ram-v4), EnvSpec(Assault-ramDeterministic-v0), EnvSpec(Assault-ramDeterministic-v4), EnvSpec(Assault-ramNoFrameskip-v0), EnvSpec(Assault-ramNoFrameskip-v4), EnvSpec(Asterix-v0), EnvSpec(Asterix-v4), EnvSpec(AsterixDeterministic-v0), EnvSpec(AsterixDeterministic-v4), EnvSpec(AsterixNoFrameskip-v0), EnvSpec(AsterixNoFrameskip-v4), EnvSpec(Asterix-ram-v0), EnvSpec(Asterix-ram-v4), EnvSpec(Asterix-ramDeterministic-v0), EnvSpec(Asterix-ramDeterministic-v4), EnvSpec(Asterix-ramNoFrameskip-v0), EnvSpec(Asterix-ramNoFrameskip-v4), EnvSpec(Asteroids-v0), EnvSpec(Asteroids-v4), EnvSpec(AsteroidsDeterministic-v0), EnvSpec(AsteroidsDeterministic-v4), EnvSpec(AsteroidsNoFrameskip-v0), EnvSpec(AsteroidsNoFrameskip-v4), EnvSpec(Asteroids-ram-v0), EnvSpec(Asteroids-ram-v4), EnvSpec(Asteroids-ramDeterministic-v0), EnvSpec(Asteroids-ramDeterministic-v4), EnvSpec(Asteroids-ramNoFrameskip-v0), EnvSpec(Asteroids-ramNoFrameskip-v4), EnvSpec(Atlantis-v0), EnvSpec(Atlantis-v4), EnvSpec(AtlantisDeterministic-v0), EnvSpec(AtlantisDeterministic-v4), EnvSpec(AtlantisNoFrameskip-v0), EnvSpec(AtlantisNoFrameskip-v4), EnvSpec(Atlantis-ram-v0), EnvSpec(Atlantis-ram-v4), EnvSpec(Atlantis-ramDeterministic-v0), EnvSpec(Atlantis-ramDeterministic-v4), EnvSpec(Atlantis-ramNoFrameskip-v0), EnvSpec(Atlantis-ramNoFrameskip-v4), EnvSpec(BankHeist-v0), EnvSpec(BankHeist-v4), EnvSpec(BankHeistDeterministic-v0), EnvSpec(BankHeistDeterministic-v4), EnvSpec(BankHeistNoFrameskip-v0), EnvSpec(BankHeistNoFrameskip-v4), EnvSpec(BankHeist-ram-v0), EnvSpec(BankHeist-ram-v4), EnvSpec(BankHeist-ramDeterministic-v0), EnvSpec(BankHeist-ramDeterministic-v4), EnvSpec(BankHeist-ramNoFrameskip-v0), EnvSpec(BankHeist-ramNoFrameskip-v4), EnvSpec(BattleZone-v0), EnvSpec(BattleZone-v4), EnvSpec(BattleZoneDeterministic-v0), EnvSpec(BattleZoneDeterministic-v4), EnvSpec(BattleZoneNoFrameskip-v0), EnvSpec(BattleZoneNoFrameskip-v4), EnvSpec(BattleZone-ram-v0), EnvSpec(BattleZone-ram-v4), EnvSpec(BattleZone-ramDeterministic-v0), EnvSpec(BattleZone-ramDeterministic-v4), EnvSpec(BattleZone-ramNoFrameskip-v0), EnvSpec(BattleZone-ramNoFrameskip-v4), EnvSpec(BeamRider-v0), EnvSpec(BeamRider-v4), EnvSpec(BeamRiderDeterministic-v0), EnvSpec(BeamRiderDeterministic-v4), EnvSpec(BeamRiderNoFrameskip-v0), EnvSpec(BeamRiderNoFrameskip-v4), EnvSpec(BeamRider-ram-v0), EnvSpec(BeamRider-ram-v4), EnvSpec(BeamRider-ramDeterministic-v0), EnvSpec(BeamRider-ramDeterministic-v4), EnvSpec(BeamRider-ramNoFrameskip-v0), EnvSpec(BeamRider-ramNoFrameskip-v4), EnvSpec(Berzerk-v0), EnvSpec(Berzerk-v4), EnvSpec(BerzerkDeterministic-v0), EnvSpec(BerzerkDeterministic-v4), EnvSpec(BerzerkNoFrameskip-v0), EnvSpec(BerzerkNoFrameskip-v4), EnvSpec(Berzerk-ram-v0), EnvSpec(Berzerk-ram-v4), EnvSpec(Berzerk-ramDeterministic-v0), EnvSpec(Berzerk-ramDeterministic-v4), EnvSpec(Berzerk-ramNoFrameskip-v0), EnvSpec(Berzerk-ramNoFrameskip-v4), EnvSpec(Bowling-v0), EnvSpec(Bowling-v4), EnvSpec(BowlingDeterministic-v0), EnvSpec(BowlingDeterministic-v4), EnvSpec(BowlingNoFrameskip-v0), EnvSpec(BowlingNoFrameskip-v4), EnvSpec(Bowling-ram-v0), EnvSpec(Bowling-ram-v4), EnvSpec(Bowling-ramDeterministic-v0), EnvSpec(Bowling-ramDeterministic-v4), EnvSpec(Bowling-ramNoFrameskip-v0), EnvSpec(Bowling-ramNoFrameskip-v4), EnvSpec(Boxing-v0), EnvSpec(Boxing-v4), EnvSpec(BoxingDeterministic-v0), EnvSpec(BoxingDeterministic-v4), EnvSpec(BoxingNoFrameskip-v0), EnvSpec(BoxingNoFrameskip-v4), EnvSpec(Boxing-ram-v0), EnvSpec(Boxing-ram-v4), EnvSpec(Boxing-ramDeterministic-v0), EnvSpec(Boxing-ramDeterministic-v4), EnvSpec(Boxing-ramNoFrameskip-v0), EnvSpec(Boxing-ramNoFrameskip-v4), EnvSpec(Breakout-v0), EnvSpec(Breakout-v4), EnvSpec(BreakoutDeterministic-v0), EnvSpec(BreakoutDeterministic-v4), EnvSpec(BreakoutNoFrameskip-v0), EnvSpec(BreakoutNoFrameskip-v4), EnvSpec(Breakout-ram-v0), EnvSpec(Breakout-ram-v4), EnvSpec(Breakout-ramDeterministic-v0), EnvSpec(Breakout-ramDeterministic-v4), EnvSpec(Breakout-ramNoFrameskip-v0), EnvSpec(Breakout-ramNoFrameskip-v4), EnvSpec(Carnival-v0), EnvSpec(Carnival-v4), EnvSpec(CarnivalDeterministic-v0), EnvSpec(CarnivalDeterministic-v4), EnvSpec(CarnivalNoFrameskip-v0), EnvSpec(CarnivalNoFrameskip-v4), EnvSpec(Carnival-ram-v0), EnvSpec(Carnival-ram-v4), EnvSpec(Carnival-ramDeterministic-v0), EnvSpec(Carnival-ramDeterministic-v4), EnvSpec(Carnival-ramNoFrameskip-v0), EnvSpec(Carnival-ramNoFrameskip-v4), EnvSpec(Centipede-v0), EnvSpec(Centipede-v4), EnvSpec(CentipedeDeterministic-v0), EnvSpec(CentipedeDeterministic-v4), EnvSpec(CentipedeNoFrameskip-v0), EnvSpec(CentipedeNoFrameskip-v4), EnvSpec(Centipede-ram-v0), EnvSpec(Centipede-ram-v4), EnvSpec(Centipede-ramDeterministic-v0), EnvSpec(Centipede-ramDeterministic-v4), EnvSpec(Centipede-ramNoFrameskip-v0), EnvSpec(Centipede-ramNoFrameskip-v4), EnvSpec(ChopperCommand-v0), EnvSpec(ChopperCommand-v4), EnvSpec(ChopperCommandDeterministic-v0), EnvSpec(ChopperCommandDeterministic-v4), EnvSpec(ChopperCommandNoFrameskip-v0), EnvSpec(ChopperCommandNoFrameskip-v4), EnvSpec(ChopperCommand-ram-v0), EnvSpec(ChopperCommand-ram-v4), EnvSpec(ChopperCommand-ramDeterministic-v0), EnvSpec(ChopperCommand-ramDeterministic-v4), EnvSpec(ChopperCommand-ramNoFrameskip-v0), EnvSpec(ChopperCommand-ramNoFrameskip-v4), EnvSpec(CrazyClimber-v0), EnvSpec(CrazyClimber-v4), EnvSpec(CrazyClimberDeterministic-v0), EnvSpec(CrazyClimberDeterministic-v4), EnvSpec(CrazyClimberNoFrameskip-v0), EnvSpec(CrazyClimberNoFrameskip-v4), EnvSpec(CrazyClimber-ram-v0), EnvSpec(CrazyClimber-ram-v4), EnvSpec(CrazyClimber-ramDeterministic-v0), EnvSpec(CrazyClimber-ramDeterministic-v4), EnvSpec(CrazyClimber-ramNoFrameskip-v0), EnvSpec(CrazyClimber-ramNoFrameskip-v4), EnvSpec(Defender-v0), EnvSpec(Defender-v4), EnvSpec(DefenderDeterministic-v0), EnvSpec(DefenderDeterministic-v4), EnvSpec(DefenderNoFrameskip-v0), EnvSpec(DefenderNoFrameskip-v4), EnvSpec(Defender-ram-v0), EnvSpec(Defender-ram-v4), EnvSpec(Defender-ramDeterministic-v0), EnvSpec(Defender-ramDeterministic-v4), EnvSpec(Defender-ramNoFrameskip-v0), EnvSpec(Defender-ramNoFrameskip-v4), EnvSpec(DemonAttack-v0), EnvSpec(DemonAttack-v4), EnvSpec(DemonAttackDeterministic-v0), EnvSpec(DemonAttackDeterministic-v4), EnvSpec(DemonAttackNoFrameskip-v0), EnvSpec(DemonAttackNoFrameskip-v4), EnvSpec(DemonAttack-ram-v0), EnvSpec(DemonAttack-ram-v4), EnvSpec(DemonAttack-ramDeterministic-v0), EnvSpec(DemonAttack-ramDeterministic-v4), EnvSpec(DemonAttack-ramNoFrameskip-v0), EnvSpec(DemonAttack-ramNoFrameskip-v4), EnvSpec(DoubleDunk-v0), EnvSpec(DoubleDunk-v4), EnvSpec(DoubleDunkDeterministic-v0), EnvSpec(DoubleDunkDeterministic-v4), EnvSpec(DoubleDunkNoFrameskip-v0), EnvSpec(DoubleDunkNoFrameskip-v4), EnvSpec(DoubleDunk-ram-v0), EnvSpec(DoubleDunk-ram-v4), EnvSpec(DoubleDunk-ramDeterministic-v0), EnvSpec(DoubleDunk-ramDeterministic-v4), EnvSpec(DoubleDunk-ramNoFrameskip-v0), EnvSpec(DoubleDunk-ramNoFrameskip-v4), EnvSpec(ElevatorAction-v0), EnvSpec(ElevatorAction-v4), EnvSpec(ElevatorActionDeterministic-v0), EnvSpec(ElevatorActionDeterministic-v4), EnvSpec(ElevatorActionNoFrameskip-v0), EnvSpec(ElevatorActionNoFrameskip-v4), EnvSpec(ElevatorAction-ram-v0), EnvSpec(ElevatorAction-ram-v4), EnvSpec(ElevatorAction-ramDeterministic-v0), EnvSpec(ElevatorAction-ramDeterministic-v4), EnvSpec(ElevatorAction-ramNoFrameskip-v0), EnvSpec(ElevatorAction-ramNoFrameskip-v4), EnvSpec(Enduro-v0), EnvSpec(Enduro-v4), EnvSpec(EnduroDeterministic-v0), EnvSpec(EnduroDeterministic-v4), EnvSpec(EnduroNoFrameskip-v0), EnvSpec(EnduroNoFrameskip-v4), EnvSpec(Enduro-ram-v0), EnvSpec(Enduro-ram-v4), EnvSpec(Enduro-ramDeterministic-v0), EnvSpec(Enduro-ramDeterministic-v4), EnvSpec(Enduro-ramNoFrameskip-v0), EnvSpec(Enduro-ramNoFrameskip-v4), EnvSpec(FishingDerby-v0), EnvSpec(FishingDerby-v4), EnvSpec(FishingDerbyDeterministic-v0), EnvSpec(FishingDerbyDeterministic-v4), EnvSpec(FishingDerbyNoFrameskip-v0), EnvSpec(FishingDerbyNoFrameskip-v4), EnvSpec(FishingDerby-ram-v0), EnvSpec(FishingDerby-ram-v4), EnvSpec(FishingDerby-ramDeterministic-v0), EnvSpec(FishingDerby-ramDeterministic-v4), EnvSpec(FishingDerby-ramNoFrameskip-v0), EnvSpec(FishingDerby-ramNoFrameskip-v4), EnvSpec(Freeway-v0), EnvSpec(Freeway-v4), EnvSpec(FreewayDeterministic-v0), EnvSpec(FreewayDeterministic-v4), EnvSpec(FreewayNoFrameskip-v0), EnvSpec(FreewayNoFrameskip-v4), EnvSpec(Freeway-ram-v0), EnvSpec(Freeway-ram-v4), EnvSpec(Freeway-ramDeterministic-v0), EnvSpec(Freeway-ramDeterministic-v4), EnvSpec(Freeway-ramNoFrameskip-v0), EnvSpec(Freeway-ramNoFrameskip-v4), EnvSpec(Frostbite-v0), EnvSpec(Frostbite-v4), EnvSpec(FrostbiteDeterministic-v0), EnvSpec(FrostbiteDeterministic-v4), EnvSpec(FrostbiteNoFrameskip-v0), EnvSpec(FrostbiteNoFrameskip-v4), EnvSpec(Frostbite-ram-v0), EnvSpec(Frostbite-ram-v4), EnvSpec(Frostbite-ramDeterministic-v0), EnvSpec(Frostbite-ramDeterministic-v4), EnvSpec(Frostbite-ramNoFrameskip-v0), EnvSpec(Frostbite-ramNoFrameskip-v4), EnvSpec(Gopher-v0), EnvSpec(Gopher-v4), EnvSpec(GopherDeterministic-v0), EnvSpec(GopherDeterministic-v4), EnvSpec(GopherNoFrameskip-v0), EnvSpec(GopherNoFrameskip-v4), EnvSpec(Gopher-ram-v0), EnvSpec(Gopher-ram-v4), EnvSpec(Gopher-ramDeterministic-v0), EnvSpec(Gopher-ramDeterministic-v4), EnvSpec(Gopher-ramNoFrameskip-v0), EnvSpec(Gopher-ramNoFrameskip-v4), EnvSpec(Gravitar-v0), EnvSpec(Gravitar-v4), EnvSpec(GravitarDeterministic-v0), EnvSpec(GravitarDeterministic-v4), EnvSpec(GravitarNoFrameskip-v0), EnvSpec(GravitarNoFrameskip-v4), EnvSpec(Gravitar-ram-v0), EnvSpec(Gravitar-ram-v4), EnvSpec(Gravitar-ramDeterministic-v0), EnvSpec(Gravitar-ramDeterministic-v4), EnvSpec(Gravitar-ramNoFrameskip-v0), EnvSpec(Gravitar-ramNoFrameskip-v4), EnvSpec(Hero-v0), EnvSpec(Hero-v4), EnvSpec(HeroDeterministic-v0), EnvSpec(HeroDeterministic-v4), EnvSpec(HeroNoFrameskip-v0), EnvSpec(HeroNoFrameskip-v4), EnvSpec(Hero-ram-v0), EnvSpec(Hero-ram-v4), EnvSpec(Hero-ramDeterministic-v0), EnvSpec(Hero-ramDeterministic-v4), EnvSpec(Hero-ramNoFrameskip-v0), EnvSpec(Hero-ramNoFrameskip-v4), EnvSpec(IceHockey-v0), EnvSpec(IceHockey-v4), EnvSpec(IceHockeyDeterministic-v0), EnvSpec(IceHockeyDeterministic-v4), EnvSpec(IceHockeyNoFrameskip-v0), EnvSpec(IceHockeyNoFrameskip-v4), EnvSpec(IceHockey-ram-v0), EnvSpec(IceHockey-ram-v4), EnvSpec(IceHockey-ramDeterministic-v0), EnvSpec(IceHockey-ramDeterministic-v4), EnvSpec(IceHockey-ramNoFrameskip-v0), EnvSpec(IceHockey-ramNoFrameskip-v4), EnvSpec(Jamesbond-v0), EnvSpec(Jamesbond-v4), EnvSpec(JamesbondDeterministic-v0), EnvSpec(JamesbondDeterministic-v4), EnvSpec(JamesbondNoFrameskip-v0), EnvSpec(JamesbondNoFrameskip-v4), EnvSpec(Jamesbond-ram-v0), EnvSpec(Jamesbond-ram-v4), EnvSpec(Jamesbond-ramDeterministic-v0), EnvSpec(Jamesbond-ramDeterministic-v4), EnvSpec(Jamesbond-ramNoFrameskip-v0), EnvSpec(Jamesbond-ramNoFrameskip-v4), EnvSpec(JourneyEscape-v0), EnvSpec(JourneyEscape-v4), EnvSpec(JourneyEscapeDeterministic-v0), EnvSpec(JourneyEscapeDeterministic-v4), EnvSpec(JourneyEscapeNoFrameskip-v0), EnvSpec(JourneyEscapeNoFrameskip-v4), EnvSpec(JourneyEscape-ram-v0), EnvSpec(JourneyEscape-ram-v4), EnvSpec(JourneyEscape-ramDeterministic-v0), EnvSpec(JourneyEscape-ramDeterministic-v4), EnvSpec(JourneyEscape-ramNoFrameskip-v0), EnvSpec(JourneyEscape-ramNoFrameskip-v4), EnvSpec(Kangaroo-v0), EnvSpec(Kangaroo-v4), EnvSpec(KangarooDeterministic-v0), EnvSpec(KangarooDeterministic-v4), EnvSpec(KangarooNoFrameskip-v0), EnvSpec(KangarooNoFrameskip-v4), EnvSpec(Kangaroo-ram-v0), EnvSpec(Kangaroo-ram-v4), EnvSpec(Kangaroo-ramDeterministic-v0), EnvSpec(Kangaroo-ramDeterministic-v4), EnvSpec(Kangaroo-ramNoFrameskip-v0), EnvSpec(Kangaroo-ramNoFrameskip-v4), EnvSpec(Krull-v0), EnvSpec(Krull-v4), EnvSpec(KrullDeterministic-v0), EnvSpec(KrullDeterministic-v4), EnvSpec(KrullNoFrameskip-v0), EnvSpec(KrullNoFrameskip-v4), EnvSpec(Krull-ram-v0), EnvSpec(Krull-ram-v4), EnvSpec(Krull-ramDeterministic-v0), EnvSpec(Krull-ramDeterministic-v4), EnvSpec(Krull-ramNoFrameskip-v0), EnvSpec(Krull-ramNoFrameskip-v4), EnvSpec(KungFuMaster-v0), EnvSpec(KungFuMaster-v4), EnvSpec(KungFuMasterDeterministic-v0), EnvSpec(KungFuMasterDeterministic-v4), EnvSpec(KungFuMasterNoFrameskip-v0), EnvSpec(KungFuMasterNoFrameskip-v4), EnvSpec(KungFuMaster-ram-v0), EnvSpec(KungFuMaster-ram-v4), EnvSpec(KungFuMaster-ramDeterministic-v0), EnvSpec(KungFuMaster-ramDeterministic-v4), EnvSpec(KungFuMaster-ramNoFrameskip-v0), EnvSpec(KungFuMaster-ramNoFrameskip-v4), EnvSpec(MontezumaRevenge-v0), EnvSpec(MontezumaRevenge-v4), EnvSpec(MontezumaRevengeDeterministic-v0), EnvSpec(MontezumaRevengeDeterministic-v4), EnvSpec(MontezumaRevengeNoFrameskip-v0), EnvSpec(MontezumaRevengeNoFrameskip-v4), EnvSpec(MontezumaRevenge-ram-v0), EnvSpec(MontezumaRevenge-ram-v4), EnvSpec(MontezumaRevenge-ramDeterministic-v0), EnvSpec(MontezumaRevenge-ramDeterministic-v4), EnvSpec(MontezumaRevenge-ramNoFrameskip-v0), EnvSpec(MontezumaRevenge-ramNoFrameskip-v4), EnvSpec(MsPacman-v0), EnvSpec(MsPacman-v4), EnvSpec(MsPacmanDeterministic-v0), EnvSpec(MsPacmanDeterministic-v4), EnvSpec(MsPacmanNoFrameskip-v0), EnvSpec(MsPacmanNoFrameskip-v4), EnvSpec(MsPacman-ram-v0), EnvSpec(MsPacman-ram-v4), EnvSpec(MsPacman-ramDeterministic-v0), EnvSpec(MsPacman-ramDeterministic-v4), EnvSpec(MsPacman-ramNoFrameskip-v0), EnvSpec(MsPacman-ramNoFrameskip-v4), EnvSpec(NameThisGame-v0), EnvSpec(NameThisGame-v4), EnvSpec(NameThisGameDeterministic-v0), EnvSpec(NameThisGameDeterministic-v4), EnvSpec(NameThisGameNoFrameskip-v0), EnvSpec(NameThisGameNoFrameskip-v4), EnvSpec(NameThisGame-ram-v0), EnvSpec(NameThisGame-ram-v4), EnvSpec(NameThisGame-ramDeterministic-v0), EnvSpec(NameThisGame-ramDeterministic-v4), EnvSpec(NameThisGame-ramNoFrameskip-v0), EnvSpec(NameThisGame-ramNoFrameskip-v4), EnvSpec(Phoenix-v0), EnvSpec(Phoenix-v4), EnvSpec(PhoenixDeterministic-v0), EnvSpec(PhoenixDeterministic-v4), EnvSpec(PhoenixNoFrameskip-v0), EnvSpec(PhoenixNoFrameskip-v4), EnvSpec(Phoenix-ram-v0), EnvSpec(Phoenix-ram-v4), EnvSpec(Phoenix-ramDeterministic-v0), EnvSpec(Phoenix-ramDeterministic-v4), EnvSpec(Phoenix-ramNoFrameskip-v0), EnvSpec(Phoenix-ramNoFrameskip-v4), EnvSpec(Pitfall-v0), EnvSpec(Pitfall-v4), EnvSpec(PitfallDeterministic-v0), EnvSpec(PitfallDeterministic-v4), EnvSpec(PitfallNoFrameskip-v0), EnvSpec(PitfallNoFrameskip-v4), EnvSpec(Pitfall-ram-v0), EnvSpec(Pitfall-ram-v4), EnvSpec(Pitfall-ramDeterministic-v0), EnvSpec(Pitfall-ramDeterministic-v4), EnvSpec(Pitfall-ramNoFrameskip-v0), EnvSpec(Pitfall-ramNoFrameskip-v4), EnvSpec(Pong-v0), EnvSpec(Pong-v4), EnvSpec(PongDeterministic-v0), EnvSpec(PongDeterministic-v4), EnvSpec(PongNoFrameskip-v0), EnvSpec(PongNoFrameskip-v4), EnvSpec(Pong-ram-v0), EnvSpec(Pong-ram-v4), EnvSpec(Pong-ramDeterministic-v0), EnvSpec(Pong-ramDeterministic-v4), EnvSpec(Pong-ramNoFrameskip-v0), EnvSpec(Pong-ramNoFrameskip-v4), EnvSpec(Pooyan-v0), EnvSpec(Pooyan-v4), EnvSpec(PooyanDeterministic-v0), EnvSpec(PooyanDeterministic-v4), EnvSpec(PooyanNoFrameskip-v0), EnvSpec(PooyanNoFrameskip-v4), EnvSpec(Pooyan-ram-v0), EnvSpec(Pooyan-ram-v4), EnvSpec(Pooyan-ramDeterministic-v0), EnvSpec(Pooyan-ramDeterministic-v4), EnvSpec(Pooyan-ramNoFrameskip-v0), EnvSpec(Pooyan-ramNoFrameskip-v4), EnvSpec(PrivateEye-v0), EnvSpec(PrivateEye-v4), EnvSpec(PrivateEyeDeterministic-v0), EnvSpec(PrivateEyeDeterministic-v4), EnvSpec(PrivateEyeNoFrameskip-v0), EnvSpec(PrivateEyeNoFrameskip-v4), EnvSpec(PrivateEye-ram-v0), EnvSpec(PrivateEye-ram-v4), EnvSpec(PrivateEye-ramDeterministic-v0), EnvSpec(PrivateEye-ramDeterministic-v4), EnvSpec(PrivateEye-ramNoFrameskip-v0), EnvSpec(PrivateEye-ramNoFrameskip-v4), EnvSpec(Qbert-v0), EnvSpec(Qbert-v4), EnvSpec(QbertDeterministic-v0), EnvSpec(QbertDeterministic-v4), EnvSpec(QbertNoFrameskip-v0), EnvSpec(QbertNoFrameskip-v4), EnvSpec(Qbert-ram-v0), EnvSpec(Qbert-ram-v4), EnvSpec(Qbert-ramDeterministic-v0), EnvSpec(Qbert-ramDeterministic-v4), EnvSpec(Qbert-ramNoFrameskip-v0), EnvSpec(Qbert-ramNoFrameskip-v4), EnvSpec(Riverraid-v0), EnvSpec(Riverraid-v4), EnvSpec(RiverraidDeterministic-v0), EnvSpec(RiverraidDeterministic-v4), EnvSpec(RiverraidNoFrameskip-v0), EnvSpec(RiverraidNoFrameskip-v4), EnvSpec(Riverraid-ram-v0), EnvSpec(Riverraid-ram-v4), EnvSpec(Riverraid-ramDeterministic-v0), EnvSpec(Riverraid-ramDeterministic-v4), EnvSpec(Riverraid-ramNoFrameskip-v0), EnvSpec(Riverraid-ramNoFrameskip-v4), EnvSpec(RoadRunner-v0), EnvSpec(RoadRunner-v4), EnvSpec(RoadRunnerDeterministic-v0), EnvSpec(RoadRunnerDeterministic-v4), EnvSpec(RoadRunnerNoFrameskip-v0), EnvSpec(RoadRunnerNoFrameskip-v4), EnvSpec(RoadRunner-ram-v0), EnvSpec(RoadRunner-ram-v4), EnvSpec(RoadRunner-ramDeterministic-v0), EnvSpec(RoadRunner-ramDeterministic-v4), EnvSpec(RoadRunner-ramNoFrameskip-v0), EnvSpec(RoadRunner-ramNoFrameskip-v4), EnvSpec(Robotank-v0), EnvSpec(Robotank-v4), EnvSpec(RobotankDeterministic-v0), EnvSpec(RobotankDeterministic-v4), EnvSpec(RobotankNoFrameskip-v0), EnvSpec(RobotankNoFrameskip-v4), EnvSpec(Robotank-ram-v0), EnvSpec(Robotank-ram-v4), EnvSpec(Robotank-ramDeterministic-v0), EnvSpec(Robotank-ramDeterministic-v4), EnvSpec(Robotank-ramNoFrameskip-v0), EnvSpec(Robotank-ramNoFrameskip-v4), EnvSpec(Seaquest-v0), EnvSpec(Seaquest-v4), EnvSpec(SeaquestDeterministic-v0), EnvSpec(SeaquestDeterministic-v4), EnvSpec(SeaquestNoFrameskip-v0), EnvSpec(SeaquestNoFrameskip-v4), EnvSpec(Seaquest-ram-v0), EnvSpec(Seaquest-ram-v4), EnvSpec(Seaquest-ramDeterministic-v0), EnvSpec(Seaquest-ramDeterministic-v4), EnvSpec(Seaquest-ramNoFrameskip-v0), EnvSpec(Seaquest-ramNoFrameskip-v4), EnvSpec(Skiing-v0), EnvSpec(Skiing-v4), EnvSpec(SkiingDeterministic-v0), EnvSpec(SkiingDeterministic-v4), EnvSpec(SkiingNoFrameskip-v0), EnvSpec(SkiingNoFrameskip-v4), EnvSpec(Skiing-ram-v0), EnvSpec(Skiing-ram-v4), EnvSpec(Skiing-ramDeterministic-v0), EnvSpec(Skiing-ramDeterministic-v4), EnvSpec(Skiing-ramNoFrameskip-v0), EnvSpec(Skiing-ramNoFrameskip-v4), EnvSpec(Solaris-v0), EnvSpec(Solaris-v4), EnvSpec(SolarisDeterministic-v0), EnvSpec(SolarisDeterministic-v4), EnvSpec(SolarisNoFrameskip-v0), EnvSpec(SolarisNoFrameskip-v4), EnvSpec(Solaris-ram-v0), EnvSpec(Solaris-ram-v4), EnvSpec(Solaris-ramDeterministic-v0), EnvSpec(Solaris-ramDeterministic-v4), EnvSpec(Solaris-ramNoFrameskip-v0), EnvSpec(Solaris-ramNoFrameskip-v4), EnvSpec(SpaceInvaders-v0), EnvSpec(SpaceInvaders-v4), EnvSpec(SpaceInvadersDeterministic-v0), EnvSpec(SpaceInvadersDeterministic-v4), EnvSpec(SpaceInvadersNoFrameskip-v0), EnvSpec(SpaceInvadersNoFrameskip-v4), EnvSpec(SpaceInvaders-ram-v0), EnvSpec(SpaceInvaders-ram-v4), EnvSpec(SpaceInvaders-ramDeterministic-v0), EnvSpec(SpaceInvaders-ramDeterministic-v4), EnvSpec(SpaceInvaders-ramNoFrameskip-v0), EnvSpec(SpaceInvaders-ramNoFrameskip-v4), EnvSpec(StarGunner-v0), EnvSpec(StarGunner-v4), EnvSpec(StarGunnerDeterministic-v0), EnvSpec(StarGunnerDeterministic-v4), EnvSpec(StarGunnerNoFrameskip-v0), EnvSpec(StarGunnerNoFrameskip-v4), EnvSpec(StarGunner-ram-v0), EnvSpec(StarGunner-ram-v4), EnvSpec(StarGunner-ramDeterministic-v0), EnvSpec(StarGunner-ramDeterministic-v4), EnvSpec(StarGunner-ramNoFrameskip-v0), EnvSpec(StarGunner-ramNoFrameskip-v4), EnvSpec(Tennis-v0), EnvSpec(Tennis-v4), EnvSpec(TennisDeterministic-v0), EnvSpec(TennisDeterministic-v4), EnvSpec(TennisNoFrameskip-v0), EnvSpec(TennisNoFrameskip-v4), EnvSpec(Tennis-ram-v0), EnvSpec(Tennis-ram-v4), EnvSpec(Tennis-ramDeterministic-v0), EnvSpec(Tennis-ramDeterministic-v4), EnvSpec(Tennis-ramNoFrameskip-v0), EnvSpec(Tennis-ramNoFrameskip-v4), EnvSpec(TimePilot-v0), EnvSpec(TimePilot-v4), EnvSpec(TimePilotDeterministic-v0), EnvSpec(TimePilotDeterministic-v4), EnvSpec(TimePilotNoFrameskip-v0), EnvSpec(TimePilotNoFrameskip-v4), EnvSpec(TimePilot-ram-v0), EnvSpec(TimePilot-ram-v4), EnvSpec(TimePilot-ramDeterministic-v0), EnvSpec(TimePilot-ramDeterministic-v4), EnvSpec(TimePilot-ramNoFrameskip-v0), EnvSpec(TimePilot-ramNoFrameskip-v4), EnvSpec(Tutankham-v0), EnvSpec(Tutankham-v4), EnvSpec(TutankhamDeterministic-v0), EnvSpec(TutankhamDeterministic-v4), EnvSpec(TutankhamNoFrameskip-v0), EnvSpec(TutankhamNoFrameskip-v4), EnvSpec(Tutankham-ram-v0), EnvSpec(Tutankham-ram-v4), EnvSpec(Tutankham-ramDeterministic-v0), EnvSpec(Tutankham-ramDeterministic-v4), EnvSpec(Tutankham-ramNoFrameskip-v0), EnvSpec(Tutankham-ramNoFrameskip-v4), EnvSpec(UpNDown-v0), EnvSpec(UpNDown-v4), EnvSpec(UpNDownDeterministic-v0), EnvSpec(UpNDownDeterministic-v4), EnvSpec(UpNDownNoFrameskip-v0), EnvSpec(UpNDownNoFrameskip-v4), EnvSpec(UpNDown-ram-v0), EnvSpec(UpNDown-ram-v4), EnvSpec(UpNDown-ramDeterministic-v0), EnvSpec(UpNDown-ramDeterministic-v4), EnvSpec(UpNDown-ramNoFrameskip-v0), EnvSpec(UpNDown-ramNoFrameskip-v4), EnvSpec(Venture-v0), EnvSpec(Venture-v4), EnvSpec(VentureDeterministic-v0), EnvSpec(VentureDeterministic-v4), EnvSpec(VentureNoFrameskip-v0), EnvSpec(VentureNoFrameskip-v4), EnvSpec(Venture-ram-v0), EnvSpec(Venture-ram-v4), EnvSpec(Venture-ramDeterministic-v0), EnvSpec(Venture-ramDeterministic-v4), EnvSpec(Venture-ramNoFrameskip-v0), EnvSpec(Venture-ramNoFrameskip-v4), EnvSpec(VideoPinball-v0), EnvSpec(VideoPinball-v4), EnvSpec(VideoPinballDeterministic-v0), EnvSpec(VideoPinballDeterministic-v4), EnvSpec(VideoPinballNoFrameskip-v0), EnvSpec(VideoPinballNoFrameskip-v4), EnvSpec(VideoPinball-ram-v0), EnvSpec(VideoPinball-ram-v4), EnvSpec(VideoPinball-ramDeterministic-v0), EnvSpec(VideoPinball-ramDeterministic-v4), EnvSpec(VideoPinball-ramNoFrameskip-v0), EnvSpec(VideoPinball-ramNoFrameskip-v4), EnvSpec(WizardOfWor-v0), EnvSpec(WizardOfWor-v4), EnvSpec(WizardOfWorDeterministic-v0), EnvSpec(WizardOfWorDeterministic-v4), EnvSpec(WizardOfWorNoFrameskip-v0), EnvSpec(WizardOfWorNoFrameskip-v4), EnvSpec(WizardOfWor-ram-v0), EnvSpec(WizardOfWor-ram-v4), EnvSpec(WizardOfWor-ramDeterministic-v0), EnvSpec(WizardOfWor-ramDeterministic-v4), EnvSpec(WizardOfWor-ramNoFrameskip-v0), EnvSpec(WizardOfWor-ramNoFrameskip-v4), EnvSpec(YarsRevenge-v0), EnvSpec(YarsRevenge-v4), EnvSpec(YarsRevengeDeterministic-v0), EnvSpec(YarsRevengeDeterministic-v4), EnvSpec(YarsRevengeNoFrameskip-v0), EnvSpec(YarsRevengeNoFrameskip-v4), EnvSpec(YarsRevenge-ram-v0), EnvSpec(YarsRevenge-ram-v4), EnvSpec(YarsRevenge-ramDeterministic-v0), EnvSpec(YarsRevenge-ramDeterministic-v4), EnvSpec(YarsRevenge-ramNoFrameskip-v0), EnvSpec(YarsRevenge-ramNoFrameskip-v4), EnvSpec(Zaxxon-v0), EnvSpec(Zaxxon-v4), EnvSpec(ZaxxonDeterministic-v0), EnvSpec(ZaxxonDeterministic-v4), EnvSpec(ZaxxonNoFrameskip-v0), EnvSpec(ZaxxonNoFrameskip-v4), EnvSpec(Zaxxon-ram-v0), EnvSpec(Zaxxon-ram-v4), EnvSpec(Zaxxon-ramDeterministic-v0), EnvSpec(Zaxxon-ramDeterministic-v4), EnvSpec(Zaxxon-ramNoFrameskip-v0), EnvSpec(Zaxxon-ramNoFrameskip-v4), EnvSpec(CubeCrash-v0), EnvSpec(CubeCrashSparse-v0), EnvSpec(CubeCrashScreenBecomesBlack-v0), EnvSpec(MemorizeDigits-v0)])\n"]}],"source":["from gym import envs\n","print(envs.registry.all())"]},{"cell_type":"markdown","metadata":{"id":"AARGGGviQEtu"},"source":["The environment’s **step** function returns  four values. These are:\n","\n","* **observation (object):** an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n","* **reward (float):** amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n","* **done (boolean):** whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.).\n","* **info (dict):** diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n","\n","The typical agent loop consists in first calling the method *reset* which provides an initial observation. Then the agent executes an action, and receives the reward, the new observation, and if the episode has finished (done is true). \n","\n","For example, analyze this sample of agent loop for 100 ms. The details of the previous variables for this game as described [here](https://github.com/openai/gym/wiki/CartPole-v0) are:\n","* **observation**: Cart Position, Cart Velocity, Pole Angle, Pole Velocity.\n","* **action**: 0\t(Push cart to the left), 1\t(Push cart to the right).\n","* **reward**: 1  for every step taken, including the termination step."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQuqX6uqQEtv","executionInfo":{"status":"ok","timestamp":1651050446836,"user_tz":-120,"elapsed":1306,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"b51eb0fa-cfb7-4125-ae43-139cfb21ed82"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0.04345342  0.00643966 -0.02580859 -0.0492291 ]\n","Action  0\n","Observation  [ 0.04358222 -0.1883029  -0.02679317  0.23520057] , reward  1.0 , done  False , info  {}\n","[ 0.04358222 -0.1883029  -0.02679317  0.23520057]\n","Action  1\n","Observation  [ 0.03981616  0.00719141 -0.02208916 -0.0658119 ] , reward  1.0 , done  False , info  {}\n","[ 0.03981616  0.00719141 -0.02208916 -0.0658119 ]\n","Action  0\n","Observation  [ 0.03995999 -0.18760699 -0.0234054   0.21982074] , reward  1.0 , done  False , info  {}\n","[ 0.03995999 -0.18760699 -0.0234054   0.21982074]\n","Action  0\n","Observation  [ 0.03620785 -0.3823867  -0.01900898  0.50502971] , reward  1.0 , done  False , info  {}\n","[ 0.03620785 -0.3823867  -0.01900898  0.50502971]\n","Action  0\n","Observation  [ 0.02856011 -0.57723568 -0.00890839  0.79166207] , reward  1.0 , done  False , info  {}\n","[ 0.02856011 -0.57723568 -0.00890839  0.79166207]\n","Action  1\n","Observation  [ 0.0170154  -0.38199255  0.00692485  0.49618998] , reward  1.0 , done  False , info  {}\n","[ 0.0170154  -0.38199255  0.00692485  0.49618998]\n","Action  0\n","Observation  [ 0.00937555 -0.57721146  0.01684865  0.79104722] , reward  1.0 , done  False , info  {}\n","[ 0.00937555 -0.57721146  0.01684865  0.79104722]\n","Action  0\n","Observation  [-0.00216868 -0.77256066  0.0326696   1.08898269] , reward  1.0 , done  False , info  {}\n","[-0.00216868 -0.77256066  0.0326696   1.08898269]\n","Action  0\n","Observation  [-0.01761989 -0.96809775  0.05444925  1.39173497] , reward  1.0 , done  False , info  {}\n","[-0.01761989 -0.96809775  0.05444925  1.39173497]\n","Action  0\n","Observation  [-0.03698185 -1.16385378  0.08228395  1.70093402] , reward  1.0 , done  False , info  {}\n","[-0.03698185 -1.16385378  0.08228395  1.70093402]\n","Action  1\n","Observation  [-0.06025892 -0.96977052  0.11630263  1.43495832] , reward  1.0 , done  False , info  {}\n","[-0.06025892 -0.96977052  0.11630263  1.43495832]\n","Action  1\n","Observation  [-0.07965433 -0.77625907  0.1450018   1.180768  ] , reward  1.0 , done  False , info  {}\n","[-0.07965433 -0.77625907  0.1450018   1.180768  ]\n","Action  0\n","Observation  [-0.09517952 -0.97293453  0.16861716  1.51516651] , reward  1.0 , done  False , info  {}\n","[-0.09517952 -0.97293453  0.16861716  1.51516651]\n","Action  0\n","Observation  [-0.11463821 -1.16964788  0.19892049  1.85539065] , reward  1.0 , done  False , info  {}\n","[-0.11463821 -1.16964788  0.19892049  1.85539065]\n","Action  1\n","Observation  [-0.13803116 -0.97719031  0.2360283   1.63049476] , reward  1.0 , done  True , info  {}\n","Episode finished after 15 timesteps\n","[-0.02047457 -0.03174787 -0.00656653  0.00107221]\n","Action  1\n","Observation  [-0.02110953  0.16346764 -0.00654509 -0.29367528] , reward  1.0 , done  False , info  {}\n","[-0.02110953  0.16346764 -0.00654509 -0.29367528]\n","Action  0\n","Observation  [-0.01784018 -0.03156039 -0.0124186  -0.00306375] , reward  1.0 , done  False , info  {}\n","[-0.01784018 -0.03156039 -0.0124186  -0.00306375]\n","Action  1\n","Observation  [-0.01847138  0.16373744 -0.01247987 -0.29963888] , reward  1.0 , done  False , info  {}\n","[-0.01847138  0.16373744 -0.01247987 -0.29963888]\n","Action  0\n","Observation  [-0.01519663 -0.03120442 -0.01847265 -0.01091784] , reward  1.0 , done  False , info  {}\n","[-0.01519663 -0.03120442 -0.01847265 -0.01091784]\n","Action  0\n","Observation  [-0.01582072 -0.22605664 -0.018691    0.27587998] , reward  1.0 , done  False , info  {}\n","[-0.01582072 -0.22605664 -0.018691    0.27587998]\n","Action  1\n","Observation  [-0.02034186 -0.03067308 -0.01317341 -0.02263901] , reward  1.0 , done  False , info  {}\n","[-0.02034186 -0.03067308 -0.01317341 -0.02263901]\n","Action  0\n","Observation  [-0.02095532 -0.22560366 -0.01362619  0.26585862] , reward  1.0 , done  False , info  {}\n","[-0.02095532 -0.22560366 -0.01362619  0.26585862]\n","Action  1\n","Observation  [-0.02546739 -0.0302899  -0.00830901 -0.03109079] , reward  1.0 , done  False , info  {}\n","[-0.02546739 -0.0302899  -0.00830901 -0.03109079]\n","Action  0\n","Observation  [-0.02607319 -0.22529172 -0.00893083  0.25895901] , reward  1.0 , done  False , info  {}\n","[-0.02607319 -0.22529172 -0.00893083  0.25895901]\n","Action  1\n","Observation  [-0.03057902 -0.03004342 -0.00375165 -0.03652739] , reward  1.0 , done  False , info  {}\n","[-0.03057902 -0.03004342 -0.00375165 -0.03652739]\n","Action  1\n","Observation  [-0.03117989  0.16513213 -0.0044822  -0.33039163] , reward  1.0 , done  False , info  {}\n","[-0.03117989  0.16513213 -0.0044822  -0.33039163]\n","Action  0\n","Observation  [-0.02787725 -0.02992573 -0.01109003 -0.03912554] , reward  1.0 , done  False , info  {}\n","[-0.02787725 -0.02992573 -0.01109003 -0.03912554]\n","Action  1\n","Observation  [-0.02847576  0.16535348 -0.01187254 -0.33528675] , reward  1.0 , done  False , info  {}\n","[-0.02847576  0.16535348 -0.01187254 -0.33528675]\n","Action  0\n","Observation  [-0.02516869 -0.02959751 -0.01857828 -0.04637132] , reward  1.0 , done  False , info  {}\n","[-0.02516869 -0.02959751 -0.01857828 -0.04637132]\n","Action  1\n","Observation  [-0.02576064  0.16578585 -0.0195057  -0.34485748] , reward  1.0 , done  False , info  {}\n","[-0.02576064  0.16578585 -0.0195057  -0.34485748]\n","Action  1\n","Observation  [-0.02244493  0.36117977 -0.02640285 -0.64362691] , reward  1.0 , done  False , info  {}\n","[-0.02244493  0.36117977 -0.02640285 -0.64362691]\n","Action  0\n","Observation  [-0.01522133  0.16643556 -0.03927539 -0.35937395] , reward  1.0 , done  False , info  {}\n","[-0.01522133  0.16643556 -0.03927539 -0.35937395]\n","Action  1\n","Observation  [-0.01189262  0.36209318 -0.04646287 -0.66417804] , reward  1.0 , done  False , info  {}\n","[-0.01189262  0.36209318 -0.04646287 -0.66417804]\n","Action  0\n","Observation  [-0.00465076  0.16764731 -0.05974643 -0.38647919] , reward  1.0 , done  False , info  {}\n","[-0.00465076  0.16764731 -0.05974643 -0.38647919]\n","Action  1\n","Observation  [-0.00129781  0.36356427 -0.06747601 -0.69738526] , reward  1.0 , done  False , info  {}\n","[-0.00129781  0.36356427 -0.06747601 -0.69738526]\n","Action  0\n","Observation  [ 0.00597347  0.16943967 -0.08142372 -0.4266839 ] , reward  1.0 , done  False , info  {}\n","[ 0.00597347  0.16943967 -0.08142372 -0.4266839 ]\n","Action  0\n","Observation  [ 0.00936227 -0.0244403  -0.0899574  -0.16073958] , reward  1.0 , done  False , info  {}\n","[ 0.00936227 -0.0244403  -0.0899574  -0.16073958]\n","Action  0\n","Observation  [ 0.00887346 -0.21816695 -0.09317219  0.10226358] , reward  1.0 , done  False , info  {}\n","[ 0.00887346 -0.21816695 -0.09317219  0.10226358]\n","Action  0\n","Observation  [ 0.00451012 -0.41183868 -0.09112692  0.36415813] , reward  1.0 , done  False , info  {}\n","[ 0.00451012 -0.41183868 -0.09112692  0.36415813]\n","Action  0\n","Observation  [-0.00372665 -0.60555535 -0.08384375  0.62677324] , reward  1.0 , done  False , info  {}\n","[-0.00372665 -0.60555535 -0.08384375  0.62677324]\n","Action  1\n","Observation  [-0.01583776 -0.40936936 -0.07130829  0.30890681] , reward  1.0 , done  False , info  {}\n","[-0.01583776 -0.40936936 -0.07130829  0.30890681]\n","Action  1\n","Observation  [-0.02402514 -0.21330766 -0.06513015 -0.00538522] , reward  1.0 , done  False , info  {}\n","[-0.02402514 -0.21330766 -0.06513015 -0.00538522]\n","Action  1\n","Observation  [-0.0282913  -0.01731504 -0.06523786 -0.31788556] , reward  1.0 , done  False , info  {}\n","[-0.0282913  -0.01731504 -0.06523786 -0.31788556]\n","Action  0\n","Observation  [-0.0286376  -0.21145009 -0.07159557 -0.04646877] , reward  1.0 , done  False , info  {}\n","[-0.0286376  -0.21145009 -0.07159557 -0.04646877]\n","Action  1\n","Observation  [-0.0328666  -0.01537843 -0.07252494 -0.36085392] , reward  1.0 , done  False , info  {}\n","[-0.0328666  -0.01537843 -0.07252494 -0.36085392]\n","Action  1\n","Observation  [-0.03317417  0.18069546 -0.07974202 -0.67549524] , reward  1.0 , done  False , info  {}\n","[-0.03317417  0.18069546 -0.07974202 -0.67549524]\n","Action  1\n","Observation  [-0.02956026  0.37682966 -0.09325193 -0.99218097] , reward  1.0 , done  False , info  {}\n","[-0.02956026  0.37682966 -0.09325193 -0.99218097]\n","Action  1\n","Observation  [-0.02202367  0.57306731 -0.11309555 -1.31263487] , reward  1.0 , done  False , info  {}\n","[-0.02202367  0.57306731 -0.11309555 -1.31263487]\n","Action  1\n","Observation  [-0.01056232  0.76942473 -0.13934824 -1.63846861] , reward  1.0 , done  False , info  {}\n","[-0.01056232  0.76942473 -0.13934824 -1.63846861]\n","Action  1\n","Observation  [ 0.00482617  0.96587826 -0.17211762 -1.97112841] , reward  1.0 , done  False , info  {}\n","[ 0.00482617  0.96587826 -0.17211762 -1.97112841]\n","Action  1\n","Observation  [ 0.02414374  1.16234926 -0.21154018 -2.31183353] , reward  1.0 , done  True , info  {}\n","Episode finished after 36 timesteps\n","[ 0.04852461 -0.0454429   0.04317249  0.04183233]\n","Action  0\n","Observation  [ 0.04761575 -0.24115649  0.04400913  0.34781794] , reward  1.0 , done  False , info  {}\n","[ 0.04761575 -0.24115649  0.04400913  0.34781794]\n","Action  0\n","Observation  [ 0.04279262 -0.43687587  0.05096549  0.65404727] , reward  1.0 , done  False , info  {}\n","[ 0.04279262 -0.43687587  0.05096549  0.65404727]\n","Action  1\n","Observation  [ 0.0340551  -0.24249922  0.06404644  0.37783825] , reward  1.0 , done  False , info  {}\n","[ 0.0340551  -0.24249922  0.06404644  0.37783825]\n","Action  1\n","Observation  [ 0.02920512 -0.04834256  0.0716032   0.10601715] , reward  1.0 , done  False , info  {}\n","[ 0.02920512 -0.04834256  0.0716032   0.10601715]\n","Action  1\n","Observation  [ 0.02823827  0.14568412  0.07372355 -0.16324375] , reward  1.0 , done  False , info  {}\n","[ 0.02823827  0.14568412  0.07372355 -0.16324375]\n","Action  1\n","Observation  [ 0.03115195  0.3396775   0.07045867 -0.43178829] , reward  1.0 , done  False , info  {}\n","[ 0.03115195  0.3396775   0.07045867 -0.43178829]\n","Action  0\n","Observation  [ 0.0379455   0.14363231  0.06182291 -0.11775243] , reward  1.0 , done  False , info  {}\n","[ 0.0379455   0.14363231  0.06182291 -0.11775243]\n","Action  0\n","Observation  [ 0.04081815 -0.05231845  0.05946786  0.19377654] , reward  1.0 , done  False , info  {}\n","[ 0.04081815 -0.05231845  0.05946786  0.19377654]\n","Action  0\n","Observation  [ 0.03977178 -0.24823843  0.06334339  0.50461028] , reward  1.0 , done  False , info  {}\n","[ 0.03977178 -0.24823843  0.06334339  0.50461028]\n","Action  1\n","Observation  [ 0.03480701 -0.0540637   0.07343559  0.23254281] , reward  1.0 , done  False , info  {}\n","[ 0.03480701 -0.0540637   0.07343559  0.23254281]\n","Action  0\n","Observation  [ 0.03372573 -0.25015393  0.07808645  0.54745608] , reward  1.0 , done  False , info  {}\n","[ 0.03372573 -0.25015393  0.07808645  0.54745608]\n","Action  1\n","Observation  [ 0.02872266 -0.05621085  0.08903557  0.28036203] , reward  1.0 , done  False , info  {}\n","[ 0.02872266 -0.05621085  0.08903557  0.28036203]\n","Action  0\n","Observation  [ 0.02759844 -0.2524826   0.09464281  0.59974537] , reward  1.0 , done  False , info  {}\n","[ 0.02759844 -0.2524826   0.09464281  0.59974537]\n","Action  1\n","Observation  [ 0.02254879 -0.05880327  0.10663772  0.33831   ] , reward  1.0 , done  False , info  {}\n","[ 0.02254879 -0.05880327  0.10663772  0.33831   ]\n","Action  0\n","Observation  [ 0.02137272 -0.25526826  0.11340392  0.66262558] , reward  1.0 , done  False , info  {}\n","[ 0.02137272 -0.25526826  0.11340392  0.66262558]\n","Action  0\n","Observation  [ 0.01626736 -0.45177009  0.12665643  0.98875437] , reward  1.0 , done  False , info  {}\n","[ 0.01626736 -0.45177009  0.12665643  0.98875437]\n","Action  1\n","Observation  [ 0.00723195 -0.25855032  0.14643152  0.73838382] , reward  1.0 , done  False , info  {}\n","[ 0.00723195 -0.25855032  0.14643152  0.73838382]\n","Action  1\n","Observation  [ 0.00206095 -0.06572129  0.1611992   0.49513293] , reward  1.0 , done  False , info  {}\n","[ 0.00206095 -0.06572129  0.1611992   0.49513293]\n","Action  1\n","Observation  [0.00074652 0.12680416 0.17110185 0.25727632] , reward  1.0 , done  False , info  {}\n","[0.00074652 0.12680416 0.17110185 0.25727632]\n","Action  1\n","Observation  [0.00328261 0.3191229  0.17624738 0.0230695 ] , reward  1.0 , done  False , info  {}\n","[0.00328261 0.3191229  0.17624738 0.0230695 ]\n","Action  1\n","Observation  [ 0.00966506  0.51133694  0.17670877 -0.20923618] , reward  1.0 , done  False , info  {}\n","[ 0.00966506  0.51133694  0.17670877 -0.20923618]\n","Action  0\n","Observation  [0.0198918  0.31418644 0.17252405 0.13356682] , reward  1.0 , done  False , info  {}\n","[0.0198918  0.31418644 0.17252405 0.13356682]\n","Action  1\n","Observation  [ 0.02617553  0.5064713   0.17519538 -0.10010783] , reward  1.0 , done  False , info  {}\n","[ 0.02617553  0.5064713   0.17519538 -0.10010783]\n","Action  0\n","Observation  [0.03630496 0.30932784 0.17319323 0.24232506] , reward  1.0 , done  False , info  {}\n","[0.03630496 0.30932784 0.17319323 0.24232506]\n","Action  0\n","Observation  [0.04249151 0.11220962 0.17803973 0.58424354] , reward  1.0 , done  False , info  {}\n","[0.04249151 0.11220962 0.17803973 0.58424354]\n","Action  1\n","Observation  [0.04473571 0.30444963 0.1897246  0.35250929] , reward  1.0 , done  False , info  {}\n","[0.04473571 0.30444963 0.1897246  0.35250929]\n","Action  0\n","Observation  [0.0508247  0.10720774 0.19677478 0.69850824] , reward  1.0 , done  False , info  {}\n","[0.0508247  0.10720774 0.19677478 0.69850824]\n","Action  1\n","Observation  [0.05296885 0.29913621 0.21074495 0.47365039] , reward  1.0 , done  True , info  {}\n","Episode finished after 28 timesteps\n","[-0.0143328  -0.03180587  0.00156902 -0.02181111]\n","Action  1\n","Observation  [-0.01496892  0.16329354  0.0011328  -0.31399858] , reward  1.0 , done  False , info  {}\n","[-0.01496892  0.16329354  0.0011328  -0.31399858]\n","Action  0\n","Observation  [-0.01170304 -0.03184453 -0.00514717 -0.02095862] , reward  1.0 , done  False , info  {}\n","[-0.01170304 -0.03184453 -0.00514717 -0.02095862]\n","Action  0\n","Observation  [-0.01233994 -0.22689229 -0.00556634  0.27009588] , reward  1.0 , done  False , info  {}\n","[-0.01233994 -0.22689229 -0.00556634  0.27009588]\n","Action  0\n","Observation  [-1.68777811e-02 -4.21934364e-01 -1.64426630e-04  5.61017969e-01] , reward  1.0 , done  False , info  {}\n","[-1.68777811e-02 -4.21934364e-01 -1.64426630e-04  5.61017969e-01]\n","Action  0\n","Observation  [-0.02531647 -0.61705401  0.01105593  0.85364909] , reward  1.0 , done  False , info  {}\n","[-0.02531647 -0.61705401  0.01105593  0.85364909]\n","Action  1\n","Observation  [-0.03765755 -0.42208448  0.02812891  0.56446306] , reward  1.0 , done  False , info  {}\n","[-0.03765755 -0.42208448  0.02812891  0.56446306]\n","Action  0\n","Observation  [-0.04609924 -0.61758957  0.03941818  0.86587349] , reward  1.0 , done  False , info  {}\n","[-0.04609924 -0.61758957  0.03941818  0.86587349]\n","Action  1\n","Observation  [-0.05845103 -0.42302564  0.05673565  0.58584023] , reward  1.0 , done  False , info  {}\n","[-0.05845103 -0.42302564  0.05673565  0.58584023]\n","Action  1\n","Observation  [-0.06691154 -0.22874234  0.06845245  0.31155553] , reward  1.0 , done  False , info  {}\n","[-0.06691154 -0.22874234  0.06845245  0.31155553]\n","Action  1\n","Observation  [-0.07148639 -0.03465899  0.07468356  0.04122162] , reward  1.0 , done  False , info  {}\n","[-0.07148639 -0.03465899  0.07468356  0.04122162]\n","Action  1\n","Observation  [-0.07217957  0.15931697  0.07550799 -0.2269947 ] , reward  1.0 , done  False , info  {}\n","[-0.07217957  0.15931697  0.07550799 -0.2269947 ]\n","Action  0\n","Observation  [-0.06899323 -0.03679831  0.0709681   0.08851828] , reward  1.0 , done  False , info  {}\n","[-0.06899323 -0.03679831  0.0709681   0.08851828]\n","Action  1\n","Observation  [-0.0697292   0.15723842  0.07273846 -0.18095707] , reward  1.0 , done  False , info  {}\n","[-0.0697292   0.15723842  0.07273846 -0.18095707]\n","Action  0\n","Observation  [-0.06658443 -0.03884493  0.06911932  0.13375646] , reward  1.0 , done  False , info  {}\n","[-0.06658443 -0.03884493  0.06911932  0.13375646]\n","Action  0\n","Observation  [-0.06736133 -0.23488537  0.07179445  0.44741988] , reward  1.0 , done  False , info  {}\n","[-0.06736133 -0.23488537  0.07179445  0.44741988]\n","Action  1\n","Observation  [-0.07205903 -0.04084857  0.08074285  0.17820391] , reward  1.0 , done  False , info  {}\n","[-0.07205903 -0.04084857  0.08074285  0.17820391]\n","Action  1\n","Observation  [-0.072876    0.15303065  0.08430693 -0.08795484] , reward  1.0 , done  False , info  {}\n","[-0.072876    0.15303065  0.08430693 -0.08795484]\n","Action  0\n","Observation  [-0.06981539 -0.04319222  0.08254783  0.23009096] , reward  1.0 , done  False , info  {}\n","[-0.06981539 -0.04319222  0.08254783  0.23009096]\n","Action  0\n","Observation  [-0.07067924 -0.2393908   0.08714965  0.5476282 ] , reward  1.0 , done  False , info  {}\n","[-0.07067924 -0.2393908   0.08714965  0.5476282 ]\n","Action  0\n","Observation  [-0.07546705 -0.43562204  0.09810221  0.86644756] , reward  1.0 , done  False , info  {}\n","[-0.07546705 -0.43562204  0.09810221  0.86644756]\n","Action  1\n","Observation  [-0.08417949 -0.24196221  0.11543117  0.60615035] , reward  1.0 , done  False , info  {}\n","[-0.08417949 -0.24196221  0.11543117  0.60615035]\n","Action  1\n","Observation  [-0.08901874 -0.04862742  0.12755417  0.35193951] , reward  1.0 , done  False , info  {}\n","[-0.08901874 -0.04862742  0.12755417  0.35193951]\n","Action  0\n","Observation  [-0.08999129 -0.24531065  0.13459296  0.68196689] , reward  1.0 , done  False , info  {}\n","[-0.08999129 -0.24531065  0.13459296  0.68196689]\n","Action  1\n","Observation  [-0.0948975  -0.05228915  0.1482323   0.43450414] , reward  1.0 , done  False , info  {}\n","[-0.0948975  -0.05228915  0.1482323   0.43450414]\n","Action  1\n","Observation  [-0.09594328  0.14045751  0.15692238  0.1919756 ] , reward  1.0 , done  False , info  {}\n","[-0.09594328  0.14045751  0.15692238  0.1919756 ]\n","Action  1\n","Observation  [-0.09313413  0.33302721  0.1607619  -0.04738371] , reward  1.0 , done  False , info  {}\n","[-0.09313413  0.33302721  0.1607619  -0.04738371]\n","Action  0\n","Observation  [-0.08647359  0.13600894  0.15981422  0.29139372] , reward  1.0 , done  False , info  {}\n","[-0.08647359  0.13600894  0.15981422  0.29139372]\n","Action  1\n","Observation  [-0.08375341  0.32853419  0.1656421   0.05307155] , reward  1.0 , done  False , info  {}\n","[-0.08375341  0.32853419  0.1656421   0.05307155]\n","Action  0\n","Observation  [-0.07718272  0.13147237  0.16670353  0.39309479] , reward  1.0 , done  False , info  {}\n","[-0.07718272  0.13147237  0.16670353  0.39309479]\n","Action  0\n","Observation  [-0.07455328 -0.06557431  0.17456542  0.7333515 ] , reward  1.0 , done  False , info  {}\n","[-0.07455328 -0.06557431  0.17456542  0.7333515 ]\n","Action  0\n","Observation  [-0.07586476 -0.26262318  0.18923245  1.0754947 ] , reward  1.0 , done  False , info  {}\n","[-0.07586476 -0.26262318  0.18923245  1.0754947 ]\n","Action  1\n","Observation  [-0.08111723 -0.07043608  0.21074235  0.84766308] , reward  1.0 , done  True , info  {}\n","Episode finished after 32 timesteps\n","[0.00768244 0.03361127 0.03675214 0.04879453]\n","Action  1\n","Observation  [ 0.00835466  0.22818748  0.03772804 -0.23206999] , reward  1.0 , done  False , info  {}\n","[ 0.00835466  0.22818748  0.03772804 -0.23206999]\n","Action  1\n","Observation  [ 0.01291841  0.4227506   0.03308664 -0.51261757] , reward  1.0 , done  False , info  {}\n","[ 0.01291841  0.4227506   0.03308664 -0.51261757]\n","Action  0\n","Observation  [ 0.02137342  0.22717862  0.02283428 -0.20969447] , reward  1.0 , done  False , info  {}\n","[ 0.02137342  0.22717862  0.02283428 -0.20969447]\n","Action  1\n","Observation  [ 0.025917    0.42196676  0.01864039 -0.49508781] , reward  1.0 , done  False , info  {}\n","[ 0.025917    0.42196676  0.01864039 -0.49508781]\n","Action  0\n","Observation  [ 0.03435633  0.22658696  0.00873864 -0.19658908] , reward  1.0 , done  False , info  {}\n","[ 0.03435633  0.22658696  0.00873864 -0.19658908]\n","Action  1\n","Observation  [ 0.03888807  0.42158284  0.00480686 -0.48650259] , reward  1.0 , done  False , info  {}\n","[ 0.03888807  0.42158284  0.00480686 -0.48650259]\n","Action  1\n","Observation  [ 0.04731973  0.61663663 -0.00492319 -0.77766669] , reward  1.0 , done  False , info  {}\n","[ 0.04731973  0.61663663 -0.00492319 -0.77766669]\n","Action  0\n","Observation  [ 0.05965246  0.42158273 -0.02047653 -0.48653679] , reward  1.0 , done  False , info  {}\n","[ 0.05965246  0.42158273 -0.02047653 -0.48653679]\n","Action  0\n","Observation  [ 0.06808411  0.2267556  -0.03020726 -0.20037705] , reward  1.0 , done  False , info  {}\n","[ 0.06808411  0.2267556  -0.03020726 -0.20037705]\n","Action  1\n","Observation  [ 0.07261923  0.42229627 -0.03421481 -0.50243383] , reward  1.0 , done  False , info  {}\n","[ 0.07261923  0.42229627 -0.03421481 -0.50243383]\n","Action  0\n","Observation  [ 0.08106515  0.22767287 -0.04426348 -0.22072678] , reward  1.0 , done  False , info  {}\n","[ 0.08106515  0.22767287 -0.04426348 -0.22072678]\n","Action  0\n","Observation  [ 0.08561861  0.03321065 -0.04867802  0.05767163] , reward  1.0 , done  False , info  {}\n","[ 0.08561861  0.03321065 -0.04867802  0.05767163]\n","Action  0\n","Observation  [ 0.08628282 -0.16118077 -0.04752459  0.33460768] , reward  1.0 , done  False , info  {}\n","[ 0.08628282 -0.16118077 -0.04752459  0.33460768]\n","Action  0\n","Observation  [ 0.08305921 -0.35559526 -0.04083243  0.61193318] , reward  1.0 , done  False , info  {}\n","[ 0.08305921 -0.35559526 -0.04083243  0.61193318]\n","Action  0\n","Observation  [ 0.0759473  -0.55012346 -0.02859377  0.89148086] , reward  1.0 , done  False , info  {}\n","[ 0.0759473  -0.55012346 -0.02859377  0.89148086]\n","Action  1\n","Observation  [ 0.06494483 -0.35462551 -0.01076415  0.5899484 ] , reward  1.0 , done  False , info  {}\n","[ 0.06494483 -0.35462551 -0.01076415  0.5899484 ]\n","Action  0\n","Observation  [ 0.05785232 -0.54959511  0.00103482  0.87922124] , reward  1.0 , done  False , info  {}\n","[ 0.05785232 -0.54959511  0.00103482  0.87922124]\n","Action  1\n","Observation  [ 0.04686042 -0.35448723  0.01861924  0.58686382] , reward  1.0 , done  False , info  {}\n","[ 0.04686042 -0.35448723  0.01861924  0.58686382]\n","Action  1\n","Observation  [ 0.03977068 -0.15963093  0.03035652  0.30010378] , reward  1.0 , done  False , info  {}\n","[ 0.03977068 -0.15963093  0.03035652  0.30010378]\n","Action  0\n","Observation  [ 0.03657806 -0.35517212  0.03635859  0.60220388] , reward  1.0 , done  False , info  {}\n","[ 0.03657806 -0.35517212  0.03635859  0.60220388]\n","Action  1\n","Observation  [ 0.02947461 -0.1605771   0.04840267  0.32119133] , reward  1.0 , done  False , info  {}\n","[ 0.02947461 -0.1605771   0.04840267  0.32119133]\n","Action  0\n","Observation  [ 0.02626307 -0.35635373  0.0548265   0.62873716] , reward  1.0 , done  False , info  {}\n","[ 0.02626307 -0.35635373  0.0548265   0.62873716]\n","Action  1\n","Observation  [ 0.019136   -0.16203806  0.06740124  0.35381254] , reward  1.0 , done  False , info  {}\n","[ 0.019136   -0.16203806  0.06740124  0.35381254]\n","Action  0\n","Observation  [ 0.01589524 -0.35805043  0.07447749  0.66696447] , reward  1.0 , done  False , info  {}\n","[ 0.01589524 -0.35805043  0.07447749  0.66696447]\n","Action  1\n","Observation  [ 0.00873423 -0.16403896  0.08781678  0.39863015] , reward  1.0 , done  False , info  {}\n","[ 0.00873423 -0.16403896  0.08781678  0.39863015]\n","Action  0\n","Observation  [ 0.00545345 -0.36028981  0.09578938  0.71765704] , reward  1.0 , done  False , info  {}\n","[ 0.00545345 -0.36028981  0.09578938  0.71765704]\n","Action  0\n","Observation  [-0.00175235 -0.55659768  0.11014253  1.03888798] , reward  1.0 , done  False , info  {}\n","[-0.00175235 -0.55659768  0.11014253  1.03888798]\n","Action  0\n","Observation  [-0.0128843  -0.75299712  0.13092029  1.36401846] , reward  1.0 , done  False , info  {}\n","[-0.0128843  -0.75299712  0.13092029  1.36401846]\n","Action  0\n","Observation  [-0.02794424 -0.94949328  0.15820065  1.69462104] , reward  1.0 , done  False , info  {}\n","[-0.02794424 -0.94949328  0.15820065  1.69462104]\n","Action  0\n","Observation  [-0.04693411 -1.14604806  0.19209308  2.03208867] , reward  1.0 , done  False , info  {}\n","[-0.04693411 -1.14604806  0.19209308  2.03208867]\n","Action  1\n","Observation  [-0.06985507 -0.9533585   0.23273485  1.80449928] , reward  1.0 , done  True , info  {}\n","Episode finished after 31 timesteps\n","[-0.03036935  0.04734854  0.01599114 -0.02061408]\n","Action  1\n","Observation  [-0.02942238  0.24223756  0.01557886 -0.30820903] , reward  1.0 , done  False , info  {}\n","[-0.02942238  0.24223756  0.01557886 -0.30820903]\n","Action  1\n","Observation  [-0.02457763  0.4371341   0.00941468 -0.59593837] , reward  1.0 , done  False , info  {}\n","[-0.02457763  0.4371341   0.00941468 -0.59593837]\n","Action  1\n","Observation  [-0.01583495  0.63212304 -0.00250409 -0.88564094] , reward  1.0 , done  False , info  {}\n","[-0.01583495  0.63212304 -0.00250409 -0.88564094]\n","Action  0\n","Observation  [-0.00319249  0.43703517 -0.02021691 -0.59374626] , reward  1.0 , done  False , info  {}\n","[-0.00319249  0.43703517 -0.02021691 -0.59374626]\n","Action  1\n","Observation  [ 0.00554822  0.63243419 -0.03209183 -0.89272825] , reward  1.0 , done  False , info  {}\n","[ 0.00554822  0.63243419 -0.03209183 -0.89272825]\n","Action  0\n","Observation  [ 0.0181969   0.43776188 -0.0499464  -0.61030353] , reward  1.0 , done  False , info  {}\n","[ 0.0181969   0.43776188 -0.0499464  -0.61030353]\n","Action  0\n","Observation  [ 0.02695214  0.24337236 -0.06215247 -0.333761  ] , reward  1.0 , done  False , info  {}\n","[ 0.02695214  0.24337236 -0.06215247 -0.333761  ]\n","Action  1\n","Observation  [ 0.03181958  0.43932131 -0.06882769 -0.64537797] , reward  1.0 , done  False , info  {}\n","[ 0.03181958  0.43932131 -0.06882769 -0.64537797]\n","Action  0\n","Observation  [ 0.04060601  0.24522256 -0.08173525 -0.37513856] , reward  1.0 , done  False , info  {}\n","[ 0.04060601  0.24522256 -0.08173525 -0.37513856]\n","Action  1\n","Observation  [ 0.04551046  0.4414046  -0.08923802 -0.69243263] , reward  1.0 , done  False , info  {}\n","[ 0.04551046  0.4414046  -0.08923802 -0.69243263]\n","Action  1\n","Observation  [ 0.05433855  0.63764384 -0.10308667 -1.01182138] , reward  1.0 , done  False , info  {}\n","[ 0.05433855  0.63764384 -0.10308667 -1.01182138]\n","Action  0\n","Observation  [ 0.06709143  0.44403708 -0.1233231  -0.75320678] , reward  1.0 , done  False , info  {}\n","[ 0.06709143  0.44403708 -0.1233231  -0.75320678]\n","Action  0\n","Observation  [ 0.07597217  0.25081174 -0.13838724 -0.50173515] , reward  1.0 , done  False , info  {}\n","[ 0.07597217  0.25081174 -0.13838724 -0.50173515]\n","Action  0\n","Observation  [ 0.08098841  0.05788397 -0.14842194 -0.25566626] , reward  1.0 , done  False , info  {}\n","[ 0.08098841  0.05788397 -0.14842194 -0.25566626]\n","Action  0\n","Observation  [ 0.08214609 -0.13484157 -0.15353526 -0.01323229] , reward  1.0 , done  False , info  {}\n","[ 0.08214609 -0.13484157 -0.15353526 -0.01323229]\n","Action  1\n","Observation  [ 0.07944925  0.06211078 -0.15379991 -0.35014781] , reward  1.0 , done  False , info  {}\n","[ 0.07944925  0.06211078 -0.15379991 -0.35014781]\n","Action  1\n","Observation  [ 0.08069147  0.25904733 -0.16080287 -0.68710481] , reward  1.0 , done  False , info  {}\n","[ 0.08069147  0.25904733 -0.16080287 -0.68710481]\n","Action  1\n","Observation  [ 0.08587242  0.45599271 -0.17454496 -1.02578427] , reward  1.0 , done  False , info  {}\n","[ 0.08587242  0.45599271 -0.17454496 -1.02578427]\n","Action  0\n","Observation  [ 0.09499227  0.26356979 -0.19506065 -0.79259156] , reward  1.0 , done  False , info  {}\n","[ 0.09499227  0.26356979 -0.19506065 -0.79259156]\n","Action  1\n","Observation  [ 0.10026367  0.46075793 -0.21091248 -1.13974938] , reward  1.0 , done  True , info  {}\n","Episode finished after 20 timesteps\n","[ 0.01338438 -0.04594429 -0.02436977 -0.01797597]\n","Action  0\n","Observation  [ 0.01246549 -0.24070843 -0.02472929  0.26691949] , reward  1.0 , done  False , info  {}\n","[ 0.01246549 -0.24070843 -0.02472929  0.26691949]\n","Action  1\n","Observation  [ 0.00765132 -0.04524243 -0.0193909  -0.03345953] , reward  1.0 , done  False , info  {}\n","[ 0.00765132 -0.04524243 -0.0193909  -0.03345953]\n","Action  1\n","Observation  [ 0.00674647  0.15015215 -0.02006009 -0.33219687] , reward  1.0 , done  False , info  {}\n","[ 0.00674647  0.15015215 -0.02006009 -0.33219687]\n","Action  0\n","Observation  [ 0.00974952 -0.04467861 -0.02670403 -0.0459068 ] , reward  1.0 , done  False , info  {}\n","[ 0.00974952 -0.04467861 -0.02670403 -0.0459068 ]\n","Action  1\n","Observation  [ 0.00885594  0.15081588 -0.02762216 -0.34689404] , reward  1.0 , done  False , info  {}\n","[ 0.00885594  0.15081588 -0.02762216 -0.34689404]\n","Action  0\n","Observation  [ 0.01187226 -0.04390251 -0.03456004 -0.06304776] , reward  1.0 , done  False , info  {}\n","[ 0.01187226 -0.04390251 -0.03456004 -0.06304776]\n","Action  1\n","Observation  [ 0.01099421  0.15169747 -0.035821   -0.36643115] , reward  1.0 , done  False , info  {}\n","[ 0.01099421  0.15169747 -0.035821   -0.36643115]\n","Action  1\n","Observation  [ 0.01402816  0.34730966 -0.04314962 -0.67019033] , reward  1.0 , done  False , info  {}\n","[ 0.01402816  0.34730966 -0.04314962 -0.67019033]\n","Action  1\n","Observation  [ 0.02097435  0.54300412 -0.05655343 -0.97614085] , reward  1.0 , done  False , info  {}\n","[ 0.02097435  0.54300412 -0.05655343 -0.97614085]\n","Action  1\n","Observation  [ 0.03183444  0.73883708 -0.07607625 -1.28603851] , reward  1.0 , done  False , info  {}\n","[ 0.03183444  0.73883708 -0.07607625 -1.28603851]\n","Action  0\n","Observation  [ 0.04661118  0.54476136 -0.10179702 -1.0181118 ] , reward  1.0 , done  False , info  {}\n","[ 0.04661118  0.54476136 -0.10179702 -1.0181118 ]\n","Action  0\n","Observation  [ 0.0575064   0.35113281 -0.12215925 -0.75904921] , reward  1.0 , done  False , info  {}\n","[ 0.0575064   0.35113281 -0.12215925 -0.75904921]\n","Action  0\n","Observation  [ 0.06452906  0.15788689 -0.13734024 -0.50716604] , reward  1.0 , done  False , info  {}\n","[ 0.06452906  0.15788689 -0.13734024 -0.50716604]\n","Action  1\n","Observation  [ 0.0676868   0.35464961 -0.14748356 -0.83978215] , reward  1.0 , done  False , info  {}\n","[ 0.0676868   0.35464961 -0.14748356 -0.83978215]\n","Action  0\n","Observation  [ 0.07477979  0.16181583 -0.1642792  -0.59687473] , reward  1.0 , done  False , info  {}\n","[ 0.07477979  0.16181583 -0.1642792  -0.59687473]\n","Action  0\n","Observation  [ 0.07801611 -0.03067242 -0.17621669 -0.36011085] , reward  1.0 , done  False , info  {}\n","[ 0.07801611 -0.03067242 -0.17621669 -0.36011085]\n","Action  0\n","Observation  [ 0.07740266 -0.22290906 -0.18341891 -0.12776136] , reward  1.0 , done  False , info  {}\n","[ 0.07740266 -0.22290906 -0.18341891 -0.12776136]\n","Action  0\n","Observation  [ 0.07294448 -0.41499445 -0.18597414  0.10191032] , reward  1.0 , done  False , info  {}\n","[ 0.07294448 -0.41499445 -0.18597414  0.10191032]\n","Action  1\n","Observation  [ 0.06464459 -0.21776172 -0.18393593 -0.2431991 ] , reward  1.0 , done  False , info  {}\n","[ 0.06464459 -0.21776172 -0.18393593 -0.2431991 ]\n","Action  0\n","Observation  [ 0.06028935 -0.40984524 -0.18879991 -0.01370684] , reward  1.0 , done  False , info  {}\n","[ 0.06028935 -0.40984524 -0.18879991 -0.01370684]\n","Action  1\n","Observation  [ 0.05209245 -0.21258794 -0.18907405 -0.35951294] , reward  1.0 , done  False , info  {}\n","[ 0.05209245 -0.21258794 -0.18907405 -0.35951294]\n","Action  0\n","Observation  [ 0.04784069 -0.40458988 -0.19626431 -0.13189978] , reward  1.0 , done  False , info  {}\n","[ 0.04784069 -0.40458988 -0.19626431 -0.13189978]\n","Action  1\n","Observation  [ 0.03974889 -0.20727727 -0.19890231 -0.47951863] , reward  1.0 , done  False , info  {}\n","[ 0.03974889 -0.20727727 -0.19890231 -0.47951863]\n","Action  1\n","Observation  [ 0.03560335 -0.00998536 -0.20849268 -0.82771426] , reward  1.0 , done  False , info  {}\n","[ 0.03560335 -0.00998536 -0.20849268 -0.82771426]\n","Action  1\n","Observation  [ 0.03540364  0.18728402 -0.22504696 -1.17806396] , reward  1.0 , done  True , info  {}\n","Episode finished after 25 timesteps\n","[-0.04884394 -0.01339751  0.0209424  -0.04875304]\n","Action  1\n","Observation  [-0.04911189  0.18141798  0.01996734 -0.33475558] , reward  1.0 , done  False , info  {}\n","[-0.04911189  0.18141798  0.01996734 -0.33475558]\n","Action  0\n","Observation  [-0.04548353 -0.01398237  0.01327223 -0.03584347] , reward  1.0 , done  False , info  {}\n","[-0.04548353 -0.01398237  0.01327223 -0.03584347]\n","Action  0\n","Observation  [-0.04576318 -0.20929211  0.01255536  0.26099725] , reward  1.0 , done  False , info  {}\n","[-0.04576318 -0.20929211  0.01255536  0.26099725]\n","Action  1\n","Observation  [-0.04994902 -0.01435162  0.0177753  -0.02769926] , reward  1.0 , done  False , info  {}\n","[-0.04994902 -0.01435162  0.0177753  -0.02769926]\n","Action  0\n","Observation  [-0.05023605 -0.20972391  0.01722132  0.27053854] , reward  1.0 , done  False , info  {}\n","[-0.05023605 -0.20972391  0.01722132  0.27053854]\n","Action  1\n","Observation  [-0.05443053 -0.01485189  0.02263209 -0.01666333] , reward  1.0 , done  False , info  {}\n","[-0.05443053 -0.01485189  0.02263209 -0.01666333]\n","Action  0\n","Observation  [-0.05472757 -0.21029097  0.02229882  0.28307349] , reward  1.0 , done  False , info  {}\n","[-0.05472757 -0.21029097  0.02229882  0.28307349]\n","Action  0\n","Observation  [-0.05893339 -0.40572376  0.02796029  0.58270511] , reward  1.0 , done  False , info  {}\n","[-0.05893339 -0.40572376  0.02796029  0.58270511]\n","Action  1\n","Observation  [-0.06704787 -0.21100447  0.03961439  0.29895958] , reward  1.0 , done  False , info  {}\n","[-0.06704787 -0.21100447  0.03961439  0.29895958]\n","Action  0\n","Observation  [-0.07126795 -0.40666805  0.04559358  0.60386827] , reward  1.0 , done  False , info  {}\n","[-0.07126795 -0.40666805  0.04559358  0.60386827]\n","Action  1\n","Observation  [-0.07940132 -0.21221241  0.05767095  0.32588781] , reward  1.0 , done  False , info  {}\n","[-0.07940132 -0.21221241  0.05767095  0.32588781]\n","Action  0\n","Observation  [-0.08364556 -0.40810602  0.06418871  0.63618557] , reward  1.0 , done  False , info  {}\n","[-0.08364556 -0.40810602  0.06418871  0.63618557]\n","Action  0\n","Observation  [-0.09180768 -0.60406168  0.07691242  0.94837227] , reward  1.0 , done  False , info  {}\n","[-0.09180768 -0.60406168  0.07691242  0.94837227]\n","Action  1\n","Observation  [-0.10388892 -0.41005481  0.09587986  0.68081224] , reward  1.0 , done  False , info  {}\n","[-0.10388892 -0.41005481  0.09587986  0.68081224]\n","Action  1\n","Observation  [-0.11209001 -0.21638609  0.10949611  0.41978893] , reward  1.0 , done  False , info  {}\n","[-0.11209001 -0.21638609  0.10949611  0.41978893]\n","Action  1\n","Observation  [-0.11641774 -0.02297216  0.11789189  0.16353306] , reward  1.0 , done  False , info  {}\n","[-0.11641774 -0.02297216  0.11789189  0.16353306]\n","Action  1\n","Observation  [-0.11687718  0.17028211  0.12116255 -0.08975625] , reward  1.0 , done  False , info  {}\n","[-0.11687718  0.17028211  0.12116255 -0.08975625]\n","Action  0\n","Observation  [-0.11347154 -0.02634939  0.11936742  0.23856339] , reward  1.0 , done  False , info  {}\n","[-0.11347154 -0.02634939  0.11936742  0.23856339]\n","Action  0\n","Observation  [-0.11399852 -0.22295642  0.12413869  0.56638613] , reward  1.0 , done  False , info  {}\n","[-0.11399852 -0.22295642  0.12413869  0.56638613]\n","Action  1\n","Observation  [-0.11845765 -0.02977452  0.13546641  0.31524629] , reward  1.0 , done  False , info  {}\n","[-0.11845765 -0.02977452  0.13546641  0.31524629]\n","Action  1\n","Observation  [-0.11905314  0.1631839   0.14177134  0.06816578] , reward  1.0 , done  False , info  {}\n","[-0.11905314  0.1631839   0.14177134  0.06816578]\n","Action  1\n","Observation  [-0.11578947  0.35601872  0.14313465 -0.17664317] , reward  1.0 , done  False , info  {}\n","[-0.11578947  0.35601872  0.14313465 -0.17664317]\n","Action  0\n","Observation  [-0.10866909  0.15916936  0.13960179  0.15754936] , reward  1.0 , done  False , info  {}\n","[-0.10866909  0.15916936  0.13960179  0.15754936]\n","Action  0\n","Observation  [-0.1054857  -0.03764657  0.14275278  0.4908109 ] , reward  1.0 , done  False , info  {}\n","[-0.1054857  -0.03764657  0.14275278  0.4908109 ]\n","Action  0\n","Observation  [-0.10623864 -0.23446318  0.152569    0.82485975] , reward  1.0 , done  False , info  {}\n","[-0.10623864 -0.23446318  0.152569    0.82485975]\n","Action  1\n","Observation  [-0.1109279  -0.04172036  0.16906619  0.58378535] , reward  1.0 , done  False , info  {}\n","[-0.1109279  -0.04172036  0.16906619  0.58378535]\n","Action  0\n","Observation  [-0.11176231 -0.2387569   0.1807419   0.92459524] , reward  1.0 , done  False , info  {}\n","[-0.11176231 -0.2387569   0.1807419   0.92459524]\n","Action  1\n","Observation  [-0.11653744 -0.04647567  0.1992338   0.6937209 ] , reward  1.0 , done  False , info  {}\n","[-0.11653744 -0.04647567  0.1992338   0.6937209 ]\n","Action  1\n","Observation  [-0.11746696  0.14540694  0.21310822  0.46977856] , reward  1.0 , done  True , info  {}\n","Episode finished after 29 timesteps\n","[-0.03259577 -0.03165672  0.03214118 -0.0124179 ]\n","Action  1\n","Observation  [-0.03322891  0.1629899   0.03189283 -0.29478914] , reward  1.0 , done  False , info  {}\n","[-0.03322891  0.1629899   0.03189283 -0.29478914]\n","Action  1\n","Observation  [-0.02996911  0.35764299  0.02599704 -0.5772454 ] , reward  1.0 , done  False , info  {}\n","[-0.02996911  0.35764299  0.02599704 -0.5772454 ]\n","Action  0\n","Observation  [-0.02281625  0.16216649  0.01445214 -0.27648745] , reward  1.0 , done  False , info  {}\n","[-0.02281625  0.16216649  0.01445214 -0.27648745]\n","Action  0\n","Observation  [-0.01957292 -0.03315864  0.00892239  0.02071842] , reward  1.0 , done  False , info  {}\n","[-0.01957292 -0.03315864  0.00892239  0.02071842]\n","Action  0\n","Observation  [-0.02023609 -0.2284074   0.00933676  0.31620306] , reward  1.0 , done  False , info  {}\n","[-0.02023609 -0.2284074   0.00933676  0.31620306]\n","Action  1\n","Observation  [-0.02480424 -0.03341968  0.01566082  0.02647919] , reward  1.0 , done  False , info  {}\n","[-0.02480424 -0.03341968  0.01566082  0.02647919]\n","Action  0\n","Observation  [-0.02547263 -0.22876268  0.0161904   0.32406184] , reward  1.0 , done  False , info  {}\n","[-0.02547263 -0.22876268  0.0161904   0.32406184]\n","Action  0\n","Observation  [-0.03004789 -0.42411138  0.02267164  0.62180626] , reward  1.0 , done  False , info  {}\n","[-0.03004789 -0.42411138  0.02267164  0.62180626]\n","Action  1\n","Observation  [-0.03853011 -0.22931323  0.03510776  0.33634902] , reward  1.0 , done  False , info  {}\n","[-0.03853011 -0.22931323  0.03510776  0.33634902]\n","Action  0\n","Observation  [-0.04311638 -0.42491676  0.04183474  0.63989307] , reward  1.0 , done  False , info  {}\n","[-0.04311638 -0.42491676  0.04183474  0.63989307]\n","Action  0\n","Observation  [-0.05161471 -0.62059623  0.0546326   0.9454513 ] , reward  1.0 , done  False , info  {}\n","[-0.05161471 -0.62059623  0.0546326   0.9454513 ]\n","Action  0\n","Observation  [-0.06402664 -0.81640979  0.07354163  1.25478741] , reward  1.0 , done  False , info  {}\n","[-0.06402664 -0.81640979  0.07354163  1.25478741]\n","Action  1\n","Observation  [-0.08035483 -0.62230255  0.09863738  0.9860153 ] , reward  1.0 , done  False , info  {}\n","[-0.08035483 -0.62230255  0.09863738  0.9860153 ]\n","Action  0\n","Observation  [-0.09280088 -0.81859725  0.11835768  1.30797854] , reward  1.0 , done  False , info  {}\n","[-0.09280088 -0.81859725  0.11835768  1.30797854]\n","Action  1\n","Observation  [-0.10917283 -0.62515718  0.14451726  1.05456439] , reward  1.0 , done  False , info  {}\n","[-0.10917283 -0.62515718  0.14451726  1.05456439]\n","Action  1\n","Observation  [-0.12167597 -0.43221563  0.16560854  0.81050937] , reward  1.0 , done  False , info  {}\n","[-0.12167597 -0.43221563  0.16560854  0.81050937]\n","Action  1\n","Observation  [-0.13032029 -0.23970272  0.18181873  0.57415754] , reward  1.0 , done  False , info  {}\n","[-0.13032029 -0.23970272  0.18181873  0.57415754]\n","Action  0\n","Observation  [-0.13511434 -0.43684534  0.19330188  0.91815774] , reward  1.0 , done  False , info  {}\n","[-0.13511434 -0.43684534  0.19330188  0.91815774]\n","Action  1\n","Observation  [-0.14385125 -0.24478768  0.21166504  0.69191426] , reward  1.0 , done  True , info  {}\n","Episode finished after 19 timesteps\n","[-0.02756468  0.02373868 -0.04217389 -0.01144648]\n","Action  0\n","Observation  [-0.02708991 -0.17075386 -0.04240282  0.26763747] , reward  1.0 , done  False , info  {}\n","[-0.02708991 -0.17075386 -0.04240282  0.26763747]\n","Action  1\n","Observation  [-0.03050498  0.02494678 -0.03705007 -0.03811233] , reward  1.0 , done  False , info  {}\n","[-0.03050498  0.02494678 -0.03705007 -0.03811233]\n","Action  1\n","Observation  [-0.03000605  0.22057991 -0.03781232 -0.34225086] , reward  1.0 , done  False , info  {}\n","[-0.03000605  0.22057991 -0.03781232 -0.34225086]\n","Action  0\n","Observation  [-0.02559445  0.02601575 -0.04465734 -0.06172741] , reward  1.0 , done  False , info  {}\n","[-0.02559445  0.02601575 -0.04465734 -0.06172741]\n","Action  1\n","Observation  [-0.02507414  0.22174859 -0.04589188 -0.36815885] , reward  1.0 , done  False , info  {}\n","[-0.02507414  0.22174859 -0.04589188 -0.36815885]\n","Action  0\n","Observation  [-0.02063916  0.02730775 -0.05325506 -0.09029214] , reward  1.0 , done  False , info  {}\n","[-0.02063916  0.02730775 -0.05325506 -0.09029214]\n","Action  1\n","Observation  [-0.02009301  0.22315099 -0.0550609  -0.39929011] , reward  1.0 , done  False , info  {}\n","[-0.02009301  0.22315099 -0.0550609  -0.39929011]\n","Action  0\n","Observation  [-0.01562999  0.02885159 -0.06304671 -0.12446243] , reward  1.0 , done  False , info  {}\n","[-0.01562999  0.02885159 -0.06304671 -0.12446243]\n","Action  1\n","Observation  [-0.01505296  0.22481746 -0.06553595 -0.43635067] , reward  1.0 , done  False , info  {}\n","[-0.01505296  0.22481746 -0.06553595 -0.43635067]\n","Action  1\n","Observation  [-0.01055661  0.42080294 -0.07426297 -0.74895158] , reward  1.0 , done  False , info  {}\n","[-0.01055661  0.42080294 -0.07426297 -0.74895158]\n","Action  0\n","Observation  [-0.00214055  0.22677966 -0.089242   -0.48053207] , reward  1.0 , done  False , info  {}\n","[-0.00214055  0.22677966 -0.089242   -0.48053207]\n","Action  1\n","Observation  [ 0.00239504  0.42304055 -0.09885264 -0.79995423] , reward  1.0 , done  False , info  {}\n","[ 0.00239504  0.42304055 -0.09885264 -0.79995423]\n","Action  1\n","Observation  [ 0.01085586  0.61936943 -0.11485172 -1.12202523] , reward  1.0 , done  False , info  {}\n","[ 0.01085586  0.61936943 -0.11485172 -1.12202523]\n","Action  0\n","Observation  [ 0.02324324  0.42592531 -0.13729223 -0.86746294] , reward  1.0 , done  False , info  {}\n","[ 0.02324324  0.42592531 -0.13729223 -0.86746294]\n","Action  0\n","Observation  [ 0.03176175  0.23291175 -0.15464149 -0.62090415] , reward  1.0 , done  False , info  {}\n","[ 0.03176175  0.23291175 -0.15464149 -0.62090415]\n","Action  0\n","Observation  [ 0.03641998  0.0402489  -0.16705957 -0.38064211] , reward  1.0 , done  False , info  {}\n","[ 0.03641998  0.0402489  -0.16705957 -0.38064211]\n","Action  0\n","Observation  [ 0.03722496 -0.15215573 -0.17467241 -0.14494052] , reward  1.0 , done  False , info  {}\n","[ 0.03722496 -0.15215573 -0.17467241 -0.14494052]\n","Action  0\n","Observation  [ 0.03418185 -0.34440199 -0.17757122  0.08794795] , reward  1.0 , done  False , info  {}\n","[ 0.03418185 -0.34440199 -0.17757122  0.08794795]\n","Action  1\n","Observation  [ 0.02729381 -0.14723807 -0.17581226 -0.25507952] , reward  1.0 , done  False , info  {}\n","[ 0.02729381 -0.14723807 -0.17581226 -0.25507952]\n","Action  0\n","Observation  [ 0.02434905 -0.33947105 -0.18091386 -0.02259796] , reward  1.0 , done  False , info  {}\n","[ 0.02434905 -0.33947105 -0.18091386 -0.02259796]\n","Action  0\n","Observation  [ 0.01755963 -0.53159964 -0.18136581  0.20799252] , reward  1.0 , done  False , info  {}\n","[ 0.01755963 -0.53159964 -0.18136581  0.20799252]\n","Action  1\n","Observation  [ 0.00692763 -0.33441034 -0.17720596 -0.13596977] , reward  1.0 , done  False , info  {}\n","[ 0.00692763 -0.33441034 -0.17720596 -0.13596977]\n","Action  0\n","Observation  [ 2.39426552e-04 -5.26609930e-01 -1.79925360e-01  9.59885538e-02] , reward  1.0 , done  False , info  {}\n","[ 2.39426552e-04 -5.26609930e-01 -1.79925360e-01  9.59885538e-02]\n","Action  1\n","Observation  [-0.01029277 -0.32942659 -0.17800559 -0.24762488] , reward  1.0 , done  False , info  {}\n","[-0.01029277 -0.32942659 -0.17800559 -0.24762488]\n","Action  1\n","Observation  [-0.0168813  -0.13226826 -0.18295809 -0.59074708] , reward  1.0 , done  False , info  {}\n","[-0.0168813  -0.13226826 -0.18295809 -0.59074708]\n","Action  1\n","Observation  [-0.01952667  0.06487996 -0.19477303 -0.93502384] , reward  1.0 , done  False , info  {}\n","[-0.01952667  0.06487996 -0.19477303 -0.93502384]\n","Action  0\n","Observation  [-0.01822907 -0.12715749 -0.2134735  -0.70931622] , reward  1.0 , done  True , info  {}\n","Episode finished after 27 timesteps\n","[ 0.03466109 -0.02603866  0.01619667  0.00445727]\n","Action  1\n","Observation  [ 0.03414032  0.1688473   0.01628581 -0.28307173] , reward  1.0 , done  False , info  {}\n","[ 0.03414032  0.1688473   0.01628581 -0.28307173]\n","Action  0\n","Observation  [ 0.03751727 -0.0265031   0.01062438  0.01470284] , reward  1.0 , done  False , info  {}\n","[ 0.03751727 -0.0265031   0.01062438  0.01470284]\n","Action  1\n","Observation  [ 0.0369872   0.16846488  0.01091843 -0.27460912] , reward  1.0 , done  False , info  {}\n","[ 0.0369872   0.16846488  0.01091843 -0.27460912]\n","Action  0\n","Observation  [ 0.0403565  -0.02681114  0.00542625  0.0214974 ] , reward  1.0 , done  False , info  {}\n","[ 0.0403565  -0.02681114  0.00542625  0.0214974 ]\n","Action  0\n","Observation  [ 0.03982028 -0.22201048  0.0058562   0.31588742] , reward  1.0 , done  False , info  {}\n","[ 0.03982028 -0.22201048  0.0058562   0.31588742]\n","Action  1\n","Observation  [ 0.03538007 -0.02697244  0.01217395  0.02505708] , reward  1.0 , done  False , info  {}\n","[ 0.03538007 -0.02697244  0.01217395  0.02505708]\n","Action  0\n","Observation  [ 0.03484062 -0.22226684  0.01267509  0.32155602] , reward  1.0 , done  False , info  {}\n","[ 0.03484062 -0.22226684  0.01267509  0.32155602]\n","Action  1\n","Observation  [ 0.03039528 -0.02732766  0.01910621  0.03289712] , reward  1.0 , done  False , info  {}\n","[ 0.03039528 -0.02732766  0.01910621  0.03289712]\n","Action  1\n","Observation  [ 0.02984873  0.16751516  0.01976415 -0.25369689] , reward  1.0 , done  False , info  {}\n","[ 0.02984873  0.16751516  0.01976415 -0.25369689]\n","Action  0\n","Observation  [ 0.03319903 -0.02788333  0.01469021  0.0451539 ] , reward  1.0 , done  False , info  {}\n","[ 0.03319903 -0.02788333  0.01469021  0.0451539 ]\n","Action  0\n","Observation  [ 0.03264137 -0.22321282  0.01559329  0.34243529] , reward  1.0 , done  False , info  {}\n","[ 0.03264137 -0.22321282  0.01559329  0.34243529]\n","Action  0\n","Observation  [ 0.02817711 -0.41855311  0.022442    0.63999434] , reward  1.0 , done  False , info  {}\n","[ 0.02817711 -0.41855311  0.022442    0.63999434]\n","Action  0\n","Observation  [ 0.01980605 -0.61398063  0.03524188  0.9396592 ] , reward  1.0 , done  False , info  {}\n","[ 0.01980605 -0.61398063  0.03524188  0.9396592 ]\n","Action  1\n","Observation  [ 0.00752644 -0.41935101  0.05403507  0.65825502] , reward  1.0 , done  False , info  {}\n","[ 0.00752644 -0.41935101  0.05403507  0.65825502]\n","Action  0\n","Observation  [-8.60584172e-04 -6.15181771e-01  6.72001692e-02  9.67451001e-01] , reward  1.0 , done  False , info  {}\n","[-8.60584172e-04 -6.15181771e-01  6.72001692e-02  9.67451001e-01]\n","Action  0\n","Observation  [-0.01316422 -0.8111386   0.08654919  1.2804648 ] , reward  1.0 , done  False , info  {}\n","[-0.01316422 -0.8111386   0.08654919  1.2804648 ]\n","Action  1\n","Observation  [-0.02938699 -0.61721946  0.11215849  1.01608855] , reward  1.0 , done  False , info  {}\n","[-0.02938699 -0.61721946  0.11215849  1.01608855]\n","Action  1\n","Observation  [-0.04173138 -0.42375716  0.13248026  0.76062395] , reward  1.0 , done  False , info  {}\n","[-0.04173138 -0.42375716  0.13248026  0.76062395]\n","Action  1\n","Observation  [-0.05020652 -0.23068496  0.14769274  0.51238875] , reward  1.0 , done  False , info  {}\n","[-0.05020652 -0.23068496  0.14769274  0.51238875]\n","Action  1\n","Observation  [-0.05482022 -0.03791822  0.15794051  0.26965052] , reward  1.0 , done  False , info  {}\n","[-0.05482022 -0.03791822  0.15794051  0.26965052]\n","Action  1\n","Observation  [-0.05557859  0.15463872  0.16333352  0.03065186] , reward  1.0 , done  False , info  {}\n","[-0.05557859  0.15463872  0.16333352  0.03065186]\n","Action  0\n","Observation  [-0.05248581 -0.04240286  0.16394656  0.37008733] , reward  1.0 , done  False , info  {}\n","[-0.05248581 -0.04240286  0.16394656  0.37008733]\n","Action  1\n","Observation  [-0.05333387  0.15005644  0.1713483   0.13325412] , reward  1.0 , done  False , info  {}\n","[-0.05333387  0.15005644  0.1713483   0.13325412]\n","Action  1\n","Observation  [-0.05033274  0.34236267  0.17401339 -0.10085071] , reward  1.0 , done  False , info  {}\n","[-0.05033274  0.34236267  0.17401339 -0.10085071]\n","Action  0\n","Observation  [-0.04348549  0.1452293   0.17199637  0.24128577] , reward  1.0 , done  False , info  {}\n","[-0.04348549  0.1452293   0.17199637  0.24128577]\n","Action  1\n","Observation  [-0.0405809   0.33753048  0.17682209  0.00740807] , reward  1.0 , done  False , info  {}\n","[-0.0405809   0.33753048  0.17682209  0.00740807]\n","Action  0\n","Observation  [-0.03383029  0.14037155  0.17697025  0.35025043] , reward  1.0 , done  False , info  {}\n","[-0.03383029  0.14037155  0.17697025  0.35025043]\n","Action  0\n","Observation  [-0.03102286 -0.05676761  0.18397526  0.69309878] , reward  1.0 , done  False , info  {}\n","[-0.03102286 -0.05676761  0.18397526  0.69309878]\n","Action  0\n","Observation  [-0.03215821 -0.25390043  0.19783723  1.03759197] , reward  1.0 , done  False , info  {}\n","[-0.03215821 -0.25390043  0.19783723  1.03759197]\n","Action  1\n","Observation  [-0.03723622 -0.06187864  0.21858907  0.81296314] , reward  1.0 , done  True , info  {}\n","Episode finished after 30 timesteps\n","[0.01580933 0.00071766 0.04928415 0.02922572]\n","Action  0\n","Observation  [ 0.01582368 -0.19507514  0.04986867  0.337042  ] , reward  1.0 , done  False , info  {}\n","[ 0.01582368 -0.19507514  0.04986867  0.337042  ]\n","Action  0\n","Observation  [ 0.01192218 -0.39086998  0.05660951  0.64502444] , reward  1.0 , done  False , info  {}\n","[ 0.01192218 -0.39086998  0.05660951  0.64502444]\n","Action  1\n","Observation  [ 0.00410478 -0.1965807   0.06950999  0.37069168] , reward  1.0 , done  False , info  {}\n","[ 0.00410478 -0.1965807   0.06950999  0.37069168]\n","Action  1\n","Observation  [ 0.00017316 -0.0025116   0.07692383  0.10071049] , reward  1.0 , done  False , info  {}\n","[ 0.00017316 -0.0025116   0.07692383  0.10071049]\n","Action  1\n","Observation  [ 1.22929880e-04  1.91428450e-01  7.89380373e-02 -1.66746008e-01] , reward  1.0 , done  False , info  {}\n","[ 1.22929880e-04  1.91428450e-01  7.89380373e-02 -1.66746008e-01]\n","Action  1\n","Observation  [ 0.0039515   0.38533692  0.07560312 -0.43351928] , reward  1.0 , done  False , info  {}\n","[ 0.0039515   0.38533692  0.07560312 -0.43351928]\n","Action  0\n","Observation  [ 0.01165824  0.18923055  0.06693273 -0.11799386] , reward  1.0 , done  False , info  {}\n","[ 0.01165824  0.18923055  0.06693273 -0.11799386]\n","Action  0\n","Observation  [ 0.01544285 -0.00678338  0.06457285  0.19503221] , reward  1.0 , done  False , info  {}\n","[ 0.01544285 -0.00678338  0.06457285  0.19503221]\n","Action  1\n","Observation  [ 0.01530718  0.18735832  0.0684735  -0.07660219] , reward  1.0 , done  False , info  {}\n","[ 0.01530718  0.18735832  0.0684735  -0.07660219]\n","Action  0\n","Observation  [ 0.01905435 -0.00867504  0.06694145  0.23687425] , reward  1.0 , done  False , info  {}\n","[ 0.01905435 -0.00867504  0.06694145  0.23687425]\n","Action  0\n","Observation  [ 0.01888085 -0.20468633  0.07167894  0.54989875] , reward  1.0 , done  False , info  {}\n","[ 0.01888085 -0.20468633  0.07167894  0.54989875]\n","Action  0\n","Observation  [ 0.01478712 -0.40073804  0.08267691  0.86427674] , reward  1.0 , done  False , info  {}\n","[ 0.01478712 -0.40073804  0.08267691  0.86427674]\n","Action  1\n","Observation  [ 0.00677236 -0.20683298  0.09996245  0.59869199] , reward  1.0 , done  False , info  {}\n","[ 0.00677236 -0.20683298  0.09996245  0.59869199]\n","Action  0\n","Observation  [ 0.0026357  -0.40320101  0.11193629  0.92111365] , reward  1.0 , done  False , info  {}\n","[ 0.0026357  -0.40320101  0.11193629  0.92111365]\n","Action  0\n","Observation  [-0.00542832 -0.5996431   0.13035856  1.24677328] , reward  1.0 , done  False , info  {}\n","[-0.00542832 -0.5996431   0.13035856  1.24677328]\n","Action  1\n","Observation  [-0.01742118 -0.40641135  0.15529403  0.99760187] , reward  1.0 , done  False , info  {}\n","[-0.01742118 -0.40641135  0.15529403  0.99760187]\n","Action  0\n","Observation  [-0.02554941 -0.60323011  0.17524607  1.33475042] , reward  1.0 , done  False , info  {}\n","[-0.02554941 -0.60323011  0.17524607  1.33475042]\n","Action  0\n","Observation  [-0.03761401 -0.80007334  0.20194107  1.67675191] , reward  1.0 , done  False , info  {}\n","[-0.03761401 -0.80007334  0.20194107  1.67675191]\n","Action  1\n","Observation  [-0.05361548 -0.60778528  0.23547611  1.453149  ] , reward  1.0 , done  True , info  {}\n","Episode finished after 19 timesteps\n","[-0.00535831  0.02782952 -0.00783477 -0.04372068]\n","Action  1\n","Observation  [-0.00480172  0.22306294 -0.00870919 -0.33886521] , reward  1.0 , done  False , info  {}\n","[-0.00480172  0.22306294 -0.00870919 -0.33886521]\n","Action  1\n","Observation  [-3.40462816e-04  4.18307724e-01 -1.54864924e-02 -6.34281758e-01] , reward  1.0 , done  False , info  {}\n","[-3.40462816e-04  4.18307724e-01 -1.54864924e-02 -6.34281758e-01]\n","Action  1\n","Observation  [ 0.00802569  0.61364223 -0.02817213 -0.93180123] , reward  1.0 , done  False , info  {}\n","[ 0.00802569  0.61364223 -0.02817213 -0.93180123]\n","Action  1\n","Observation  [ 0.02029854  0.80913279 -0.04680815 -1.23320222] , reward  1.0 , done  False , info  {}\n","[ 0.02029854  0.80913279 -0.04680815 -1.23320222]\n","Action  1\n","Observation  [ 0.03648119  1.00482428 -0.0714722  -1.54017452] , reward  1.0 , done  False , info  {}\n","[ 0.03648119  1.00482428 -0.0714722  -1.54017452]\n","Action  1\n","Observation  [ 0.05657768  1.2007294  -0.10227569 -1.8542769 ] , reward  1.0 , done  False , info  {}\n","[ 0.05657768  1.2007294  -0.10227569 -1.8542769 ]\n","Action  0\n","Observation  [ 0.08059227  1.0068694  -0.13936123 -1.59502311] , reward  1.0 , done  False , info  {}\n","[ 0.08059227  1.0068694  -0.13936123 -1.59502311]\n","Action  0\n","Observation  [ 0.10072965  0.81364834 -0.17126169 -1.34884117] , reward  1.0 , done  False , info  {}\n","[ 0.10072965  0.81364834 -0.17126169 -1.34884117]\n","Action  0\n","Observation  [ 0.11700262  0.62104162 -0.19823851 -1.11426285] , reward  1.0 , done  False , info  {}\n","[ 0.11700262  0.62104162 -0.19823851 -1.11426285]\n","Action  1\n","Observation  [ 0.12942345  0.81813479 -0.22052377 -1.46201364] , reward  1.0 , done  True , info  {}\n","Episode finished after 10 timesteps\n","[-0.0270254   0.02239975 -0.02201017 -0.03407842]\n","Action  1\n","Observation  [-0.0265774   0.21783031 -0.02269174 -0.33362372] , reward  1.0 , done  False , info  {}\n","[-0.0265774   0.21783031 -0.02269174 -0.33362372]\n","Action  1\n","Observation  [-0.0222208   0.41326775 -0.02936422 -0.63337522] , reward  1.0 , done  False , info  {}\n","[-0.0222208   0.41326775 -0.02936422 -0.63337522]\n","Action  1\n","Observation  [-0.01395544  0.60878676 -0.04203172 -0.93515914] , reward  1.0 , done  False , info  {}\n","[-0.01395544  0.60878676 -0.04203172 -0.93515914]\n","Action  0\n","Observation  [-0.00177971  0.41425618 -0.0607349  -0.65597467] , reward  1.0 , done  False , info  {}\n","[-0.00177971  0.41425618 -0.0607349  -0.65597467]\n","Action  1\n","Observation  [ 0.00650542  0.61016872 -0.0738544  -0.96714674] , reward  1.0 , done  False , info  {}\n","[ 0.00650542  0.61016872 -0.0738544  -0.96714674]\n","Action  1\n","Observation  [ 0.01870879  0.80620058 -0.09319733 -1.2820864 ] , reward  1.0 , done  False , info  {}\n","[ 0.01870879  0.80620058 -0.09319733 -1.2820864 ]\n","Action  1\n","Observation  [ 0.0348328   1.00237783 -0.11883906 -1.60243561] , reward  1.0 , done  False , info  {}\n","[ 0.0348328   1.00237783 -0.11883906 -1.60243561]\n","Action  1\n","Observation  [ 0.05488036  1.19868919 -0.15088777 -1.92968226] , reward  1.0 , done  False , info  {}\n","[ 0.05488036  1.19868919 -0.15088777 -1.92968226]\n","Action  1\n","Observation  [ 0.07885414  1.39507164 -0.18948142 -2.26510187] , reward  1.0 , done  False , info  {}\n","[ 0.07885414  1.39507164 -0.18948142 -2.26510187]\n","Action  0\n","Observation  [ 0.10675558  1.20216051 -0.23478345 -2.03628904] , reward  1.0 , done  True , info  {}\n","Episode finished after 10 timesteps\n","[ 0.02437141  0.01710556 -0.0025311  -0.04826306]\n","Action  0\n","Observation  [ 0.02471352 -0.17798001 -0.00349636  0.24362021] , reward  1.0 , done  False , info  {}\n","[ 0.02471352 -0.17798001 -0.00349636  0.24362021]\n","Action  0\n","Observation  [ 0.02115392 -0.37305184  0.00137604  0.53519825] , reward  1.0 , done  False , info  {}\n","[ 0.02115392 -0.37305184  0.00137604  0.53519825]\n","Action  1\n","Observation  [ 0.01369288 -0.17794927  0.01208001  0.24294922] , reward  1.0 , done  False , info  {}\n","[ 0.01369288 -0.17794927  0.01208001  0.24294922]\n","Action  0\n","Observation  [ 0.0101339  -0.37324167  0.01693899  0.53941788] , reward  1.0 , done  False , info  {}\n","[ 0.0101339  -0.37324167  0.01693899  0.53941788]\n","Action  0\n","Observation  [ 0.00266906 -0.56859759  0.02772735  0.83738956] , reward  1.0 , done  False , info  {}\n","[ 0.00266906 -0.56859759  0.02772735  0.83738956]\n","Action  0\n","Observation  [-0.00870289 -0.76408703  0.04447514  1.1386618 ] , reward  1.0 , done  False , info  {}\n","[-0.00870289 -0.76408703  0.04447514  1.1386618 ]\n","Action  1\n","Observation  [-0.02398463 -0.56957397  0.06724838  0.86025211] , reward  1.0 , done  False , info  {}\n","[-0.02398463 -0.56957397  0.06724838  0.86025211]\n","Action  1\n","Observation  [-0.03537611 -0.37542919  0.08445342  0.5894493 ] , reward  1.0 , done  False , info  {}\n","[-0.03537611 -0.37542919  0.08445342  0.5894493 ]\n","Action  0\n","Observation  [-0.04288469 -0.57162583  0.09624241  0.90749518] , reward  1.0 , done  False , info  {}\n","[-0.04288469 -0.57162583  0.09624241  0.90749518]\n","Action  1\n","Observation  [-0.05431721 -0.3779293   0.11439231  0.64654654] , reward  1.0 , done  False , info  {}\n","[-0.05431721 -0.3779293   0.11439231  0.64654654]\n","Action  1\n","Observation  [-0.0618758  -0.1845714   0.12732324  0.39196333] , reward  1.0 , done  False , info  {}\n","[-0.0618758  -0.1845714   0.12732324  0.39196333]\n","Action  0\n","Observation  [-0.06556722 -0.38124862  0.13516251  0.72192307] , reward  1.0 , done  False , info  {}\n","[-0.06556722 -0.38124862  0.13516251  0.72192307]\n","Action  0\n","Observation  [-0.0731922  -0.57795561  0.14960097  1.05390933] , reward  1.0 , done  False , info  {}\n","[-0.0731922  -0.57795561  0.14960097  1.05390933]\n","Action  1\n","Observation  [-0.08475131 -0.38509925  0.17067916  0.81167473] , reward  1.0 , done  False , info  {}\n","[-0.08475131 -0.38509925  0.17067916  0.81167473]\n","Action  0\n","Observation  [-0.09245329 -0.5820967   0.18691265  1.15281364] , reward  1.0 , done  False , info  {}\n","[-0.09245329 -0.5820967   0.18691265  1.15281364]\n","Action  0\n","Observation  [-0.10409523 -0.77909856  0.20996892  1.49780244] , reward  1.0 , done  True , info  {}\n","Episode finished after 16 timesteps\n","[ 0.03019378 -0.03967636 -0.01388725  0.0254547 ]\n","Action  0\n","Observation  [ 0.02940025 -0.23459644 -0.01337815  0.31372389] , reward  1.0 , done  False , info  {}\n","[ 0.02940025 -0.23459644 -0.01337815  0.31372389]\n","Action  1\n","Observation  [ 0.02470832 -0.03928649 -0.00710368  0.01685213] , reward  1.0 , done  False , info  {}\n","[ 0.02470832 -0.03928649 -0.00710368  0.01685213]\n","Action  1\n","Observation  [ 0.02392259  0.15593661 -0.00676663 -0.2780636 ] , reward  1.0 , done  False , info  {}\n","[ 0.02392259  0.15593661 -0.00676663 -0.2780636 ]\n","Action  0\n","Observation  [ 0.02704133 -0.03908815 -0.0123279   0.01247748] , reward  1.0 , done  False , info  {}\n","[ 0.02704133 -0.03908815 -0.0123279   0.01247748]\n","Action  0\n","Observation  [ 0.02625956 -0.23403116 -0.01207836  0.30124545] , reward  1.0 , done  False , info  {}\n","[ 0.02625956 -0.23403116 -0.01207836  0.30124545]\n","Action  0\n","Observation  [ 0.02157894 -0.42897889 -0.00605345  0.59009477] , reward  1.0 , done  False , info  {}\n","[ 0.02157894 -0.42897889 -0.00605345  0.59009477]\n","Action  1\n","Observation  [ 0.01299936 -0.23377271  0.00574845  0.29551116] , reward  1.0 , done  False , info  {}\n","[ 0.01299936 -0.23377271  0.00574845  0.29551116]\n","Action  0\n","Observation  [ 0.00832391 -0.42897614  0.01165867  0.5900015 ] , reward  1.0 , done  False , info  {}\n","[ 0.00832391 -0.42897614  0.01165867  0.5900015 ]\n","Action  0\n","Observation  [-2.55614252e-04 -6.24259373e-01  2.34587025e-02  8.86334020e-01] , reward  1.0 , done  False , info  {}\n","[-2.55614252e-04 -6.24259373e-01  2.34587025e-02  8.86334020e-01]\n","Action  1\n","Observation  [-0.0127408  -0.4294636   0.04118538  0.60111697] , reward  1.0 , done  False , info  {}\n","[-0.0127408  -0.4294636   0.04118538  0.60111697]\n","Action  1\n","Observation  [-0.02133007 -0.23494125  0.05320772  0.32168597] , reward  1.0 , done  False , info  {}\n","[-0.02133007 -0.23494125  0.05320772  0.32168597]\n","Action  1\n","Observation  [-0.0260289  -0.04061578  0.05964144  0.04624597] , reward  1.0 , done  False , info  {}\n","[-0.0260289  -0.04061578  0.05964144  0.04624597]\n","Action  0\n","Observation  [-0.02684121 -0.23653999  0.06056636  0.35713394] , reward  1.0 , done  False , info  {}\n","[-0.02684121 -0.23653999  0.06056636  0.35713394]\n","Action  0\n","Observation  [-0.03157201 -0.43246838  0.06770904  0.66828326] , reward  1.0 , done  False , info  {}\n","[-0.03157201 -0.43246838  0.06770904  0.66828326]\n","Action  1\n","Observation  [-0.04022138 -0.23835005  0.08107471  0.39766422] , reward  1.0 , done  False , info  {}\n","[-0.04022138 -0.23835005  0.08107471  0.39766422]\n","Action  1\n","Observation  [-0.04498838 -0.04446629  0.08902799  0.13160373] , reward  1.0 , done  False , info  {}\n","[-0.04498838 -0.04446629  0.08902799  0.13160373]\n","Action  0\n","Observation  [-0.04587771 -0.24074326  0.09166006  0.45099286] , reward  1.0 , done  False , info  {}\n","[-0.04587771 -0.24074326  0.09166006  0.45099286]\n","Action  1\n","Observation  [-0.05069257 -0.04702909  0.10067992  0.18855172] , reward  1.0 , done  False , info  {}\n","[-0.05069257 -0.04702909  0.10067992  0.18855172]\n","Action  1\n","Observation  [-0.05163316  0.14651912  0.10445096 -0.07075051] , reward  1.0 , done  False , info  {}\n","[-0.05163316  0.14651912  0.10445096 -0.07075051]\n","Action  1\n","Observation  [-0.04870277  0.34000058  0.10303595 -0.3287382 ] , reward  1.0 , done  False , info  {}\n","[-0.04870277  0.34000058  0.10303595 -0.3287382 ]\n","Action  1\n","Observation  [-0.04190276  0.53351633  0.09646118 -0.58723336] , reward  1.0 , done  False , info  {}\n","[-0.04190276  0.53351633  0.09646118 -0.58723336]\n","Action  0\n","Observation  [-0.03123243  0.33718521  0.08471651 -0.26579009] , reward  1.0 , done  False , info  {}\n","[-0.03123243  0.33718521  0.08471651 -0.26579009]\n","Action  0\n","Observation  [-0.02448873  0.14096273  0.07940071  0.05236492] , reward  1.0 , done  False , info  {}\n","[-0.02448873  0.14096273  0.07940071  0.05236492]\n","Action  0\n","Observation  [-0.02166948 -0.05520265  0.08044801  0.36900523] , reward  1.0 , done  False , info  {}\n","[-0.02166948 -0.05520265  0.08044801  0.36900523]\n","Action  0\n","Observation  [-0.02277353 -0.25137     0.08782812  0.6859308 ] , reward  1.0 , done  False , info  {}\n","[-0.02277353 -0.25137     0.08782812  0.6859308 ]\n","Action  1\n","Observation  [-0.02780093 -0.05757001  0.10154673  0.42213957] , reward  1.0 , done  False , info  {}\n","[-0.02780093 -0.05757001  0.10154673  0.42213957]\n","Action  0\n","Observation  [-0.02895233 -0.25397299  0.10998952  0.74502987] , reward  1.0 , done  False , info  {}\n","[-0.02895233 -0.25397299  0.10998952  0.74502987]\n","Action  1\n","Observation  [-0.03403179 -0.06052686  0.12489012  0.48888586] , reward  1.0 , done  False , info  {}\n","[-0.03403179 -0.06052686  0.12489012  0.48888586]\n","Action  0\n","Observation  [-0.03524233 -0.25716905  0.13466784  0.81817409] , reward  1.0 , done  False , info  {}\n","[-0.03524233 -0.25716905  0.13466784  0.81817409]\n","Action  1\n","Observation  [-0.04038571 -0.06412211  0.15103132  0.57069824] , reward  1.0 , done  False , info  {}\n","[-0.04038571 -0.06412211  0.15103132  0.57069824]\n","Action  0\n","Observation  [-0.04166815 -0.26100336  0.16244528  0.90689289] , reward  1.0 , done  False , info  {}\n","[-0.04166815 -0.26100336  0.16244528  0.90689289]\n","Action  1\n","Observation  [-0.04688822 -0.06840918  0.18058314  0.66935409] , reward  1.0 , done  False , info  {}\n","[-0.04688822 -0.06840918  0.18058314  0.66935409]\n","Action  0\n","Observation  [-0.0482564  -0.26552127  0.19397022  1.01301776] , reward  1.0 , done  False , info  {}\n","[-0.0482564  -0.26552127  0.19397022  1.01301776]\n","Action  0\n","Observation  [-0.05356683 -0.46262694  0.21423058  1.35980201] , reward  1.0 , done  True , info  {}\n","Episode finished after 34 timesteps\n","[-0.00528396 -0.02272662 -0.00338653  0.04205582]\n","Action  0\n","Observation  [-0.00573849 -0.21779984 -0.00254541  0.33366834] , reward  1.0 , done  False , info  {}\n","[-0.00573849 -0.21779984 -0.00254541  0.33366834]\n","Action  0\n","Observation  [-0.01009448 -0.41288547  0.00412795  0.62554749] , reward  1.0 , done  False , info  {}\n","[-0.01009448 -0.41288547  0.00412795  0.62554749]\n","Action  1\n","Observation  [-0.01835219 -0.21782139  0.0166389   0.33416747] , reward  1.0 , done  False , info  {}\n","[-0.01835219 -0.21782139  0.0166389   0.33416747]\n","Action  0\n","Observation  [-0.02270862 -0.41317615  0.02332225  0.63205067] , reward  1.0 , done  False , info  {}\n","[-0.02270862 -0.41317615  0.02332225  0.63205067]\n","Action  0\n","Observation  [-0.03097214 -0.60861559  0.03596327  0.93198622] , reward  1.0 , done  False , info  {}\n","[-0.03097214 -0.60861559  0.03596327  0.93198622]\n","Action  1\n","Observation  [-0.04314446 -0.4139969   0.05460299  0.65081787] , reward  1.0 , done  False , info  {}\n","[-0.04314446 -0.4139969   0.05460299  0.65081787]\n","Action  0\n","Observation  [-0.05142439 -0.60983515  0.06761935  0.96018274] , reward  1.0 , done  False , info  {}\n","[-0.05142439 -0.60983515  0.06761935  0.96018274]\n","Action  0\n","Observation  [-0.0636211  -0.80579767  0.086823    1.27331971] , reward  1.0 , done  False , info  {}\n","[-0.0636211  -0.80579767  0.086823    1.27331971]\n","Action  1\n","Observation  [-0.07973705 -0.61188417  0.1122894   1.00903899] , reward  1.0 , done  False , info  {}\n","[-0.07973705 -0.61188417  0.1122894   1.00903899]\n","Action  1\n","Observation  [-0.09197473 -0.41842554  0.13247018  0.75362236] , reward  1.0 , done  False , info  {}\n","[-0.09197473 -0.41842554  0.13247018  0.75362236]\n","Action  0\n","Observation  [-0.10034325 -0.61510091  0.14754263  1.08488313] , reward  1.0 , done  False , info  {}\n","[-0.10034325 -0.61510091  0.14754263  1.08488313]\n","Action  1\n","Observation  [-0.11264526 -0.4222006   0.16924029  0.84189667] , reward  1.0 , done  False , info  {}\n","[-0.11264526 -0.4222006   0.16924029  0.84189667]\n","Action  1\n","Observation  [-0.12108928 -0.22974276  0.18607822  0.60685384] , reward  1.0 , done  False , info  {}\n","[-0.12108928 -0.22974276  0.18607822  0.60685384]\n","Action  1\n","Observation  [-0.12568413 -0.03764289  0.1982153   0.37807012] , reward  1.0 , done  False , info  {}\n","[-0.12568413 -0.03764289  0.1982153   0.37807012]\n","Action  0\n","Observation  [-0.12643699 -0.23494638  0.2057767   0.72612487] , reward  1.0 , done  False , info  {}\n","[-0.12643699 -0.23494638  0.2057767   0.72612487]\n","Action  0\n","Observation  [-0.13113592 -0.43222909  0.2202992   1.07587799] , reward  1.0 , done  True , info  {}\n","Episode finished after 16 timesteps\n","[ 0.00858766 -0.01828283  0.03163258  0.02780016]\n","Action  0\n","Observation  [ 0.00822201 -0.2138438   0.03218858  0.33029329] , reward  1.0 , done  False , info  {}\n","[ 0.00822201 -0.2138438   0.03218858  0.33029329]\n","Action  0\n","Observation  [ 0.00394513 -0.40940881  0.03879444  0.63295067] , reward  1.0 , done  False , info  {}\n","[ 0.00394513 -0.40940881  0.03879444  0.63295067]\n","Action  1\n","Observation  [-0.00424305 -0.21484893  0.05145346  0.35273313] , reward  1.0 , done  False , info  {}\n","[-0.00424305 -0.21484893  0.05145346  0.35273313]\n","Action  0\n","Observation  [-0.00854003 -0.41066336  0.05850812  0.66118669] , reward  1.0 , done  False , info  {}\n","[-0.00854003 -0.41066336  0.05850812  0.66118669]\n","Action  1\n","Observation  [-0.01675329 -0.21640225  0.07173185  0.38748521] , reward  1.0 , done  False , info  {}\n","[-0.01675329 -0.21640225  0.07173185  0.38748521]\n","Action  1\n","Observation  [-0.02108134 -0.02236794  0.07948156  0.11825331] , reward  1.0 , done  False , info  {}\n","[-0.02108134 -0.02236794  0.07948156  0.11825331]\n","Action  0\n","Observation  [-0.0215287  -0.21853341  0.08184662  0.43491555] , reward  1.0 , done  False , info  {}\n","[-0.0215287  -0.21853341  0.08184662  0.43491555]\n","Action  0\n","Observation  [-0.02589936 -0.41471289  0.09054494  0.75223574] , reward  1.0 , done  False , info  {}\n","[-0.02589936 -0.41471289  0.09054494  0.75223574]\n","Action  0\n","Observation  [-0.03419362 -0.61095898  0.10558965  1.07198288] , reward  1.0 , done  False , info  {}\n","[-0.03419362 -0.61095898  0.10558965  1.07198288]\n","Action  0\n","Observation  [-0.0464128  -0.80730627  0.12702931  1.39584921] , reward  1.0 , done  False , info  {}\n","[-0.0464128  -0.80730627  0.12702931  1.39584921]\n","Action  0\n","Observation  [-0.06255893 -1.00375895  0.15494629  1.72540014] , reward  1.0 , done  False , info  {}\n","[-0.06255893 -1.00375895  0.15494629  1.72540014]\n","Action  1\n","Observation  [-0.08263411 -0.81071195  0.1894543   1.4846709 ] , reward  1.0 , done  False , info  {}\n","[-0.08263411 -0.81071195  0.1894543   1.4846709 ]\n","Action  0\n","Observation  [-0.09884835 -1.00757056  0.21914771  1.83004223] , reward  1.0 , done  True , info  {}\n","Episode finished after 13 timesteps\n","[-0.00074344  0.00404033 -0.02436613  0.04326253]\n","Action  0\n","Observation  [-0.00066263 -0.1907239  -0.02350088  0.32815922] , reward  1.0 , done  False , info  {}\n","[-0.00066263 -0.1907239  -0.02350088  0.32815922]\n","Action  1\n","Observation  [-0.00447711  0.0047246  -0.0169377   0.0281588 ] , reward  1.0 , done  False , info  {}\n","[-0.00447711  0.0047246  -0.0169377   0.0281588 ]\n","Action  0\n","Observation  [-0.00438262 -0.19015041 -0.01637452  0.31544994] , reward  1.0 , done  False , info  {}\n","[-0.00438262 -0.19015041 -0.01637452  0.31544994]\n","Action  1\n","Observation  [-0.00818563  0.00520091 -0.01006553  0.01764834] , reward  1.0 , done  False , info  {}\n","[-0.00818563  0.00520091 -0.01006553  0.01764834]\n","Action  0\n","Observation  [-0.00808161 -0.18977526 -0.00971256  0.30713856] , reward  1.0 , done  False , info  {}\n","[-0.00808161 -0.18977526 -0.00971256  0.30713856]\n","Action  1\n","Observation  [-0.01187711  0.00548374 -0.00356979  0.01140844] , reward  1.0 , done  False , info  {}\n","[-0.01187711  0.00548374 -0.00356979  0.01140844]\n","Action  0\n","Observation  [-0.01176744 -0.18958684 -0.00334162  0.30296292] , reward  1.0 , done  False , info  {}\n","[-0.01176744 -0.18958684 -0.00334162  0.30296292]\n","Action  0\n","Observation  [-0.01555918 -0.384661    0.00271764  0.5945901 ] , reward  1.0 , done  False , info  {}\n","[-0.01555918 -0.384661    0.00271764  0.5945901 ]\n","Action  0\n","Observation  [-0.0232524  -0.57982089  0.01460944  0.88812783] , reward  1.0 , done  False , info  {}\n","[-0.0232524  -0.57982089  0.01460944  0.88812783]\n","Action  0\n","Observation  [-0.03484881 -0.77513804  0.032372    1.18536732] , reward  1.0 , done  False , info  {}\n","[-0.03484881 -0.77513804  0.032372    1.18536732]\n","Action  0\n","Observation  [-0.05035157 -0.97066457  0.05607934  1.48801916] , reward  1.0 , done  False , info  {}\n","[-0.05035157 -0.97066457  0.05607934  1.48801916]\n","Action  1\n","Observation  [-0.06976487 -0.7762688   0.08583973  1.21336259] , reward  1.0 , done  False , info  {}\n","[-0.06976487 -0.7762688   0.08583973  1.21336259]\n","Action  0\n","Observation  [-0.08529024 -0.97238716  0.11010698  1.53166287] , reward  1.0 , done  False , info  {}\n","[-0.08529024 -0.97238716  0.11010698  1.53166287]\n","Action  0\n","Observation  [-0.10473799 -1.16865057  0.14074024  1.85658132] , reward  1.0 , done  False , info  {}\n","[-0.10473799 -1.16865057  0.14074024  1.85658132]\n","Action  1\n","Observation  [-0.128111   -0.97532712  0.17787186  1.61070455] , reward  1.0 , done  False , info  {}\n","[-0.128111   -0.97532712  0.17787186  1.61070455]\n","Action  1\n","Observation  [-0.14761754 -0.78269606  0.21008595  1.37833584] , reward  1.0 , done  True , info  {}\n","Episode finished after 16 timesteps\n","[-0.02481933  0.04533485  0.01256723  0.02088052]\n","Action  1\n","Observation  [-0.02391264  0.24027434  0.01298484 -0.26781096] , reward  1.0 , done  False , info  {}\n","[-0.02391264  0.24027434  0.01298484 -0.26781096]\n","Action  0\n","Observation  [-0.01910715  0.0449695   0.00762862  0.02893903] , reward  1.0 , done  False , info  {}\n","[-0.01910715  0.0449695   0.00762862  0.02893903]\n","Action  1\n","Observation  [-0.01820776  0.23998123  0.0082074  -0.26132725] , reward  1.0 , done  False , info  {}\n","[-0.01820776  0.23998123  0.0082074  -0.26132725]\n","Action  0\n","Observation  [-0.01340814  0.04474309  0.00298086  0.03393305] , reward  1.0 , done  False , info  {}\n","[-0.01340814  0.04474309  0.00298086  0.03393305]\n","Action  1\n","Observation  [-0.01251327  0.23982216  0.00365952 -0.2578079 ] , reward  1.0 , done  False , info  {}\n","[-0.01251327  0.23982216  0.00365952 -0.2578079 ]\n","Action  0\n","Observation  [-0.00771683  0.04464816 -0.00149664  0.03602704] , reward  1.0 , done  False , info  {}\n","[-0.00771683  0.04464816 -0.00149664  0.03602704]\n","Action  0\n","Observation  [-0.00682387 -0.1504523  -0.0007761   0.32823739] , reward  1.0 , done  False , info  {}\n","[-0.00682387 -0.1504523  -0.0007761   0.32823739]\n","Action  0\n","Observation  [-0.00983291 -0.34556319  0.00578865  0.62067547] , reward  1.0 , done  False , info  {}\n","[-0.00983291 -0.34556319  0.00578865  0.62067547]\n","Action  0\n","Observation  [-0.01674418 -0.54076551  0.01820216  0.91517588] , reward  1.0 , done  False , info  {}\n","[-0.01674418 -0.54076551  0.01820216  0.91517588]\n","Action  0\n","Observation  [-0.02755949 -0.73612884  0.03650568  1.21352348] , reward  1.0 , done  False , info  {}\n","[-0.02755949 -0.73612884  0.03650568  1.21352348]\n","Action  0\n","Observation  [-0.04228206 -0.93170237  0.06077615  1.5174186 ] , reward  1.0 , done  False , info  {}\n","[-0.04228206 -0.93170237  0.06077615  1.5174186 ]\n","Action  0\n","Observation  [-0.06091611 -1.12750449  0.09112452  1.82843672] , reward  1.0 , done  False , info  {}\n","[-0.06091611 -1.12750449  0.09112452  1.82843672]\n","Action  0\n","Observation  [-0.0834662  -1.32351052  0.12769325  2.14797946] , reward  1.0 , done  False , info  {}\n","[-0.0834662  -1.32351052  0.12769325  2.14797946]\n","Action  0\n","Observation  [-0.10993641 -1.51963798  0.17065284  2.47721532] , reward  1.0 , done  False , info  {}\n","[-0.10993641 -1.51963798  0.17065284  2.47721532]\n","Action  0\n","Observation  [-0.14032917 -1.71572956  0.22019715  2.81700884] , reward  1.0 , done  True , info  {}\n","Episode finished after 15 timesteps\n"]}],"source":["import gym\n","env = gym.make('CartPole-v0')\n","for i_episode in range(20):\n","    observation = env.reset()\n","    for t in range(100):\n","       # env.render()\n","        print(observation)\n","        action = env.action_space.sample()\n","        print(\"Action \", action)\n","        observation, reward, done, info = env.step(action)\n","        print(\"Observation \", observation, \", reward \", reward, \", done \", done, \", info \" , info)\n","        if done:\n","            print(\"Episode finished after {} timesteps\".format(t+1))\n","            break"]},{"cell_type":"markdown","metadata":{"id":"2vw12EsTQEtw"},"source":["# The Frozen Lake scenario\n","We are going to play to the [Frozen Lake](http://gym.openai.com/envs/FrozenLake-v0/) game.\n","\n","The problem is a grid where you should go from the 'start' (S) position to the 'goal position (G) (the pizza!). You can only walk through the 'frozen tiles' (F). Unfortunately, you can fall in a  'hole' (H).\n","![](images/frozenlake-problem.png \"Frozen lake problem\")\n","\n","The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise. The possible actions are going left, right, up or down. However, the ice is slippery, so you won't always move in the direction you intend.\n","\n","![](images/frozenlake-world.png \"Frozen lake world\")\n","\n","\n","Here you can see several episodes. A full recording is available at  [Frozen World](http://gym.openai.com/envs/FrozenLake-v0/).\n","\n","![](images/recording.gif \"Example running\")\n"]},{"cell_type":"markdown","metadata":{"id":"Et01_AI0QEtw"},"source":["# Q-Learning with the Frozen Lake scenario\n","We are now going to apply Q-Learning for the Frozen Lake scenario. This part of the notebook is taken from [here](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/Q%20Learning%20with%20FrozenLake.ipynb).\n","\n","First we create the environment and a Q-table inizializated with zeros to store the value of each action in a given state. "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bpxe_rjnQEtx","executionInfo":{"status":"ok","timestamp":1651050460064,"user_tz":-120,"elapsed":595,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"65b6d4c5-8267-41fd-a23e-70c8384626db"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"source":["import numpy as np\n","import gym\n","import random\n","\n","env = gym.make(\"FrozenLake-v0\")\n","\n","\n","action_size = env.action_space.n\n","state_size = env.observation_space.n\n","\n","\n","qtable = np.zeros((state_size, action_size))\n","print(qtable)"]},{"cell_type":"markdown","metadata":{"id":"c9mZAm-yQEtx"},"source":["Now we define the hyperparameters."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"l2X7W5iiQEtx","executionInfo":{"status":"ok","timestamp":1651050464791,"user_tz":-120,"elapsed":245,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}}},"outputs":[],"source":["# Q-Learning hyperparameters\n","total_episodes = 10000        # Total episodes\n","learning_rate = 0.8           # Learning rate\n","max_steps = 99                # Max steps per episode\n","gamma = 0.95                  # Discounting rate\n","\n","# Exploration hyperparameters\n","epsilon = 1.0                 # Exploration rate\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability \n","decay_rate = 0.01             # Exponential decay rate for exploration prob"]},{"cell_type":"markdown","metadata":{"id":"kW5jrdrpQEty"},"source":["And now we implement the Q-Learning algorithm.\n","\n","![](images/qlearning-algo.png \"Q-Learning algorithm\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-e7mXZ3QEty","executionInfo":{"status":"ok","timestamp":1651050486100,"user_tz":-120,"elapsed":10829,"user":{"displayName":"Ángela Burgaleta Ledesma","userId":"01200761640961433987"}},"outputId":"c4dee0c8-9548-4094-a999-473c35dacc8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Score over time: 0.4846\n","[[2.91403879e-01 1.05376926e-01 1.04067982e-01 1.19798597e-01]\n"," [1.51153826e-01 8.92598375e-04 1.57710718e-03 3.94541133e-02]\n"," [3.46919495e-03 1.89593141e-03 1.99356432e-03 3.51112589e-02]\n"," [2.37804690e-04 6.10555145e-03 8.78743692e-04 2.71730922e-02]\n"," [5.37026200e-01 3.83507109e-02 6.53455796e-04 1.22072303e-01]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [2.57051897e-07 1.66374016e-06 3.74099023e-02 7.75961781e-07]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [6.08332753e-02 1.91325825e-02 4.53882411e-03 7.16736364e-01]\n"," [7.57867610e-02 8.46688861e-01 1.83053672e-01 9.23256676e-03]\n"," [2.49763738e-01 2.30818763e-02 5.10765717e-03 4.31481704e-03]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [1.00073401e-01 3.14877497e-02 9.24472905e-01 3.34524384e-02]\n"," [2.74194590e-01 9.97522556e-01 2.63211304e-01 1.39474704e-01]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"]}],"source":["# List of rewards\n","rewards = []\n","\n","# 2 For life or until learning is stopped\n","for episode in range(total_episodes):\n","    # Reset the environment\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards = 0\n","    \n","    for step in range(max_steps):\n","        # 3. Choose an action a in the current world state (s)\n","        ## First we randomize a number\n","        exp_exp_tradeoff = random.uniform(0, 1)\n","        \n","        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n","        if exp_exp_tradeoff > epsilon:\n","            action = np.argmax(qtable[state,:])\n","\n","        # Else doing a random choice --> exploration\n","        else:\n","            action = env.action_space.sample()\n","\n","        # Take the action (a) and observe the outcome state(s') and reward (r)\n","        new_state, reward, done, info = env.step(action)\n","\n","        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","        # qtable[new_state,:] : all the actions we can take from new state\n","        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n","        \n","        total_rewards += reward\n","        \n","        # Our new state is state\n","        state = new_state\n","        \n","        # If done (if we're dead) : finish episode\n","        if done == True: \n","            break\n","        \n","    episode += 1\n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n","    rewards.append(total_rewards)\n","\n","print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n","print(qtable)"]},{"cell_type":"markdown","metadata":{"id":"4Pm7MmLLQEtz"},"source":["Finally, we use the learnt Q-table for playing the Frozen World game."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3UUUjCAQEtz"},"outputs":[],"source":["\n","env.reset()\n","\n","for episode in range(5):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    print(\"****************************************************\")\n","    print(\"EPISODE \", episode)\n","\n","    for step in range(max_steps):\n","        env.render()\n","        # Take the action (index) that have the maximum expected future reward given that state\n","        action = np.argmax(qtable[state,:])\n","        \n","        new_state, reward, done, info = env.step(action)\n","        \n","        if done:\n","            break\n","        state = new_state\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"XKpWOm8mQEt0"},"source":["# Exercises\n","\n","## Taxi\n","Analyze the [Taxi problem](http://gym.openai.com/envs/Taxi-v2/) and solve it applying Q-Learning. You can find a solution as the one previously presented  [here](https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym).\n","\n","Analyze the impact of not changing the learning rate (alfa or epsilon, depending on the book) or changing it in a different way."]},{"cell_type":"markdown","metadata":{"id":"z6C5TecIQEt0"},"source":["# Optional exercises\n","\n","## Doom\n","Read this [article](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8) and execute the companion [notebook](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Doom/Deep%20Q%20learning%20with%20Doom.ipynb). Analyze the results and provide conclusions about DQN."]},{"cell_type":"markdown","metadata":{"id":"Q_OLpDQlQEt0"},"source":["## References\n","* [Diving deeper into Reinforcement Learning with Q-Learning, Thomas Simonini](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe).\n","* Illustrations by [Thomas Simonini](https://github.com/simoninithomas/Deep_reinforcement_learning_Course) and [Sung Kim](https://www.youtube.com/watch?v=xgoO54qN4lY).\n","* [Frozen Lake solution with TensorFlow](https://analyticsindiamag.com/openai-gym-frozen-lake-beginners-guide-reinforcement-learning/)\n","* [Deep Q-Learning for Doom](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n","* [Intro OpenAI Gym with Random Search and the Cart Pole scenario](http://www.pinchofintelligence.com/getting-started-openai-gym/)\n","* [Q-Learning for the Taxi scenario](https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym)"]},{"cell_type":"markdown","metadata":{"id":"S7EMbhwrQEt1"},"source":["## Licence"]},{"cell_type":"markdown","metadata":{"id":"ixanQ8_iQEt1"},"source":["The notebook is freely licensed under under the [Creative Commons Attribution Share-Alike license](https://creativecommons.org/licenses/by/2.0/).  \n","\n","© 2018 Carlos A. Iglesias, Universidad Politécnica de Madrid."]}],"metadata":{"datacleaner":{"position":{"top":"50px"},"python":{"varRefreshCmd":"try:\n    print(_datacleaner.dataframe_metadata())\nexcept:\n    print([])"},"window_display":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"latex_envs":{"LaTeX_envs_menu_present":true,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"colab":{"name":"2_6_1_Q-Learning.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}